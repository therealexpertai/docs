{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"expert.ai Natural Language API expert.ai Natural Language API is a cloud-based software service providing a comprehensive set of natural language understanding capabilities based on expert.ai technology. Quick start Start using the API Guide API capabilities Developer how-to Indications for programmatic use Reference REST interface reference See the list of supported languages for document analysis , document classification and information detection .","title":"Introduction"},{"location":"#expertai-natural-language-api","text":"expert.ai Natural Language API is a cloud-based software service providing a comprehensive set of natural language understanding capabilities based on expert.ai technology. Quick start Start using the API Guide API capabilities Developer how-to Indications for programmatic use Reference REST interface reference See the list of supported languages for document analysis , document classification and information detection .","title":"expert.ai Natural Language API"},{"location":"guide/","text":"API Guide This section is a guide to expert.ai Natural Language API capabilities and concepts. Capabilities are divided into three groups: Document analysis , comprising: Deep linguistic analysis , which, in turn, comprises: Text subdivision Part-of-speech tagging Morphological analysis Lemmatization Syntactic analysis Semantic analysis Keyphrase extraction Named entity recognition Relation extraction Sentiment analysis Document classification Information detection This guide reflects the above subdivision.","title":"Overview"},{"location":"guide/#api-guide","text":"This section is a guide to expert.ai Natural Language API capabilities and concepts. Capabilities are divided into three groups: Document analysis , comprising: Deep linguistic analysis , which, in turn, comprises: Text subdivision Part-of-speech tagging Morphological analysis Lemmatization Syntactic analysis Semantic analysis Keyphrase extraction Named entity recognition Relation extraction Sentiment analysis Document classification Information detection This guide reflects the above subdivision.","title":"API Guide"},{"location":"guide/classification/","text":"Document classification Classification and taxonomies Document classification determines what a text is about in terms of categories of a taxonomy . Available taxonomies are: Taxonomy English Spanish French German Italian iptc \u2714 \u2714 \u2714 \u2714 \u2714 geotax \u2714 \u2714 \u2714 \u2714 \u2714 emotional-traits \u2714 \u2714 behavioral-traits \u2714 \u2714 In the Natural Language API terminology, taxonomy \"x\" is both a specific set of categories and the name of the API resources capable of classifying a text according to that set. Taxonomies' resources have paths like this: categorize / taxonomy name / language code Boxed parts are placeholders, so for example: https://nlapi.expert.ai/v2/categorize/iptc/en is the URL of the iptc resource capable of performing the IPTC Media Topics classification of English texts. These resources must be requested with the POST method, submitting the text to classify. In the reference section of this manual you will find all the information you need to perform document classification using the API's RESTful interface, specifically: The format of the request to be submitted to the resources. How to build resources' paths and full endpoints . The output format . Note Even if you use the API through a client that hides the REST interface, whether it is made by you or is one of those provided by expert.ai , knowing the output format helps you understand and navigate the results. Here is an example of performing IPTC Media Topics classification of a short English text: Python This example is based on the Python client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the list of categories. from expertai.nlapi.cloud.client import ExpertAiClient client = ExpertAiClient () text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" taxonomy = 'iptc' language = 'en' output = client . classification ( body = { \"document\" : { \"text\" : text }}, params = { 'taxonomy' : taxonomy , 'language' : language }) print ( \"Tab separated list of categories:\" ) for category in output . categories : print ( category . id_ , category . hierarchy , sep = \" \\t \" ) Java This example is based on the Java client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the JSON response . import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.cloud.Categorizer ; import ai.expert.nlapi.v2.cloud.CategorizerConfig ; import ai.expert.nlapi.v2.message.CategorizeResponse ; import ai.expert.nlapi.v2.model.CategorizeDocument ; public class Main { public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static Categorizer createCategorizer () throws Exception { return new Categorizer ( CategorizerConfig . builder () . withVersion ( API . Versions . V2 ) . withTaxonomy ( \"iptc\" ) . withLanguage ( API . Languages . en ) . withAuthentication ( createAuthentication ()) . build ()); } public static void main ( String [] args ) { try { String text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" ; Categorizer categorizer = createCategorizer (); CategorizeResponse categorization = categorizer . categorize ( text ); // Output JSON representation System . out . println ( \"JSON representation:\" ); categorization . prettyPrint (); // Tab separated list of categories. System . out . println ( \"Tab separated list of categories:\" ); CategorizeDocument data = categorization . getData (); data . getCategories (). stream (). forEach ( c -> System . out . println ( c . getId () + \"\\t\" + c . getHierarchy ())); } catch ( Exception ex ) { ex . printStackTrace (); } } } curl (Linux) The following curl command posts a document to the document classification resource of the API's REST interface. Run the command from a shell after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/categorize/iptc/en \\ -H 'Authorization: Bearer token' \\ -H 'Content-Type: application/json; charset=utf-8' \\ -d '{ \"document\": { \"text\": \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan' \\' 's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" } }' The server returns a JSON object . curl (Windows) The following curl command posts a document to the document classification resource of the API's REST interface. Open a command prompt in the folder where you installed curl and run the command after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/categorize/iptc/en -H \"Authorization: Bearer token\" -H \"Content-Type: application/json; charset=utf-8\" -d \"{\\\" document\\ \": {\\\" text\\ \": \\\" Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\\ \"}}\" The server returns a JSON object . The following articles describe the capabilities of the available taxonomies. Self-documentation resources The API provides self-documentation resource to programmatically discover available taxonomies and their features. Learn more about this resource in the dedicated article .","title":"Overview"},{"location":"guide/classification/#document-classification","text":"","title":"Document classification"},{"location":"guide/classification/#classification-and-taxonomies","text":"Document classification determines what a text is about in terms of categories of a taxonomy . Available taxonomies are: Taxonomy English Spanish French German Italian iptc \u2714 \u2714 \u2714 \u2714 \u2714 geotax \u2714 \u2714 \u2714 \u2714 \u2714 emotional-traits \u2714 \u2714 behavioral-traits \u2714 \u2714 In the Natural Language API terminology, taxonomy \"x\" is both a specific set of categories and the name of the API resources capable of classifying a text according to that set. Taxonomies' resources have paths like this: categorize / taxonomy name / language code Boxed parts are placeholders, so for example: https://nlapi.expert.ai/v2/categorize/iptc/en is the URL of the iptc resource capable of performing the IPTC Media Topics classification of English texts. These resources must be requested with the POST method, submitting the text to classify. In the reference section of this manual you will find all the information you need to perform document classification using the API's RESTful interface, specifically: The format of the request to be submitted to the resources. How to build resources' paths and full endpoints . The output format . Note Even if you use the API through a client that hides the REST interface, whether it is made by you or is one of those provided by expert.ai , knowing the output format helps you understand and navigate the results. Here is an example of performing IPTC Media Topics classification of a short English text: Python This example is based on the Python client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the list of categories. from expertai.nlapi.cloud.client import ExpertAiClient client = ExpertAiClient () text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" taxonomy = 'iptc' language = 'en' output = client . classification ( body = { \"document\" : { \"text\" : text }}, params = { 'taxonomy' : taxonomy , 'language' : language }) print ( \"Tab separated list of categories:\" ) for category in output . categories : print ( category . id_ , category . hierarchy , sep = \" \\t \" ) Java This example is based on the Java client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the JSON response . import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.cloud.Categorizer ; import ai.expert.nlapi.v2.cloud.CategorizerConfig ; import ai.expert.nlapi.v2.message.CategorizeResponse ; import ai.expert.nlapi.v2.model.CategorizeDocument ; public class Main { public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static Categorizer createCategorizer () throws Exception { return new Categorizer ( CategorizerConfig . builder () . withVersion ( API . Versions . V2 ) . withTaxonomy ( \"iptc\" ) . withLanguage ( API . Languages . en ) . withAuthentication ( createAuthentication ()) . build ()); } public static void main ( String [] args ) { try { String text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" ; Categorizer categorizer = createCategorizer (); CategorizeResponse categorization = categorizer . categorize ( text ); // Output JSON representation System . out . println ( \"JSON representation:\" ); categorization . prettyPrint (); // Tab separated list of categories. System . out . println ( \"Tab separated list of categories:\" ); CategorizeDocument data = categorization . getData (); data . getCategories (). stream (). forEach ( c -> System . out . println ( c . getId () + \"\\t\" + c . getHierarchy ())); } catch ( Exception ex ) { ex . printStackTrace (); } } } curl (Linux) The following curl command posts a document to the document classification resource of the API's REST interface. Run the command from a shell after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/categorize/iptc/en \\ -H 'Authorization: Bearer token' \\ -H 'Content-Type: application/json; charset=utf-8' \\ -d '{ \"document\": { \"text\": \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan' \\' 's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" } }' The server returns a JSON object . curl (Windows) The following curl command posts a document to the document classification resource of the API's REST interface. Open a command prompt in the folder where you installed curl and run the command after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/categorize/iptc/en -H \"Authorization: Bearer token\" -H \"Content-Type: application/json; charset=utf-8\" -d \"{\\\" document\\ \": {\\\" text\\ \": \\\" Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\\ \"}}\" The server returns a JSON object . The following articles describe the capabilities of the available taxonomies.","title":"Classification and taxonomies"},{"location":"guide/classification/#self-documentation-resources","text":"The API provides self-documentation resource to programmatically discover available taxonomies and their features. Learn more about this resource in the dedicated article .","title":"Self-documentation resources"},{"location":"guide/classification/behavioral-traits/","text":"Behavioral traits classification The document classification API resources corresponding to the behavioral-traits taxonomy recognize references to personality traits\u2014like curiosity, honesty, negativity, etc.\u2014of the people mentioned in the text. They can recognize 72 different traits divided into seven super-groups with three sub-groups each. In the design phase, the behavioral traits to be identified were chosen based on the Big Five and by considering real world applications and recent papers on the subject 1 . You can find the category tree for this taxonomy in the reference section . Here is an excerpt: ... Action Action low Sedentariness Passivity Action fair Calmness Action high Initiative Dynamism Openness Openness low Rejection Apathy Apprehension Traditionalism Conformism Negativity Bias Openness fair Cautiousness Openness high Progressiveness Acceptance Courage Positivity Curiosity ... Categories at the first and second level of the hierarchy function as super-groups and sub-groups. Only the categories at the third level of the hierarchy\u2014the leaf categories\u2014are returned in the output, but the full path starting from the super-group is returned as an additional property for each category. Warning Grouping is purely ontological. The names of the groups have no positive or negative connotations: in particular, the high , fair and low qualifications in sub-groups are not to be interpreted as a moral judgment, but as the intensity with which encompassed behavioral traits represent the super-group. N. Najm, \"Big Five Traits: A Critical Review\", Gadjah Mada International Journal of Business, September 2019. R. M. S. Ramos, G. B. S. Neto, B. B. C. Silva, D. S. Monteiro, I. Paraboni, R. F. S. Dias, \"Building a Corpus for Personality-dependent Natural Language Understanding and Generation\", Lrec conference proceedings, 2018. K. Luyckx, W. Daelemans, \"Personae: a corpus for author and personality prediction from text\", Lrec conference proceedings, 2008. F. Celli, \"Adaptive Personality Recognition from Text\", PhD Thesis, University of Trento, 2012. \u21a9","title":"Behavioral traits"},{"location":"guide/classification/behavioral-traits/#behavioral-traits-classification","text":"The document classification API resources corresponding to the behavioral-traits taxonomy recognize references to personality traits\u2014like curiosity, honesty, negativity, etc.\u2014of the people mentioned in the text. They can recognize 72 different traits divided into seven super-groups with three sub-groups each. In the design phase, the behavioral traits to be identified were chosen based on the Big Five and by considering real world applications and recent papers on the subject 1 . You can find the category tree for this taxonomy in the reference section . Here is an excerpt: ... Action Action low Sedentariness Passivity Action fair Calmness Action high Initiative Dynamism Openness Openness low Rejection Apathy Apprehension Traditionalism Conformism Negativity Bias Openness fair Cautiousness Openness high Progressiveness Acceptance Courage Positivity Curiosity ... Categories at the first and second level of the hierarchy function as super-groups and sub-groups. Only the categories at the third level of the hierarchy\u2014the leaf categories\u2014are returned in the output, but the full path starting from the super-group is returned as an additional property for each category. Warning Grouping is purely ontological. The names of the groups have no positive or negative connotations: in particular, the high , fair and low qualifications in sub-groups are not to be interpreted as a moral judgment, but as the intensity with which encompassed behavioral traits represent the super-group. N. Najm, \"Big Five Traits: A Critical Review\", Gadjah Mada International Journal of Business, September 2019. R. M. S. Ramos, G. B. S. Neto, B. B. C. Silva, D. S. Monteiro, I. Paraboni, R. F. S. Dias, \"Building a Corpus for Personality-dependent Natural Language Understanding and Generation\", Lrec conference proceedings, 2018. K. Luyckx, W. Daelemans, \"Personae: a corpus for author and personality prediction from text\", Lrec conference proceedings, 2008. F. Celli, \"Adaptive Personality Recognition from Text\", PhD Thesis, University of Trento, 2012. \u21a9","title":"Behavioral traits classification"},{"location":"guide/classification/emotional-traits/","text":"Emotional traits classification The classification resources corresponding to the emotional-traits taxonomy classify documents in terms of the feelings\u2014joy, surprise, irritation, etc.\u2014expressed in the text. They can recognize 39 different emotional traits divided into eight groups. During the design phase, the choice of which emotional traits to identify was guided\u2014in addition to the developer community needs for this API extension\u2014by the literature available on the subject, including some recent research publications 1 . You can find the category tree for this taxonomy in the reference section . Here is an abstract of the category tree for English: ... Group Dejection Sadness Torment Suffering Sorrow Disappointment Disillusion Resignation Group Surprise Surprise Group Delight Happiness Excitement Joy Amusement Well-Being Satisfaction Relief ... Classification resources return leaf categories, that is 2nd level categories like Excitement and Disillusion , but if requested with a special parameter they also return the main groups of emotional traits. Main groups are the taxonomy groups corresponding to the most relevant emotional traits expressed in the text. They provide an easy-to-read indication of the clusters of emotional traits the text is more about, similarly to an abstract. A. Dabrowski, \"Emotions in philosophy. A short introduction\", Studia Humana, 2016, 5:3, 8-20. E. Kim, R. Klinger, \"A Survey on Sentiment and Emotion Analysis for Computational Literary Studies\", Institut f\u00fcr Maschinelle Sprachverarbeitung, University of Stuttgart, 2018. A. Yadollahi, A. G. Shahraki, O. S. Zaiane, \"Current State of Text Sentiment Analysis from Opinion to Emotion Mining\", University of Alberta, 2017. R. Donovan, A. Johnson, A. deRoiste, R. O'Reilly, \"Quantifying the Links between Personality Sub-Traits and the Basic Emotions\", Computer Science and its Applications, 2020. \u21a9","title":"Emotional traits"},{"location":"guide/classification/emotional-traits/#emotional-traits-classification","text":"The classification resources corresponding to the emotional-traits taxonomy classify documents in terms of the feelings\u2014joy, surprise, irritation, etc.\u2014expressed in the text. They can recognize 39 different emotional traits divided into eight groups. During the design phase, the choice of which emotional traits to identify was guided\u2014in addition to the developer community needs for this API extension\u2014by the literature available on the subject, including some recent research publications 1 . You can find the category tree for this taxonomy in the reference section . Here is an abstract of the category tree for English: ... Group Dejection Sadness Torment Suffering Sorrow Disappointment Disillusion Resignation Group Surprise Surprise Group Delight Happiness Excitement Joy Amusement Well-Being Satisfaction Relief ... Classification resources return leaf categories, that is 2nd level categories like Excitement and Disillusion , but if requested with a special parameter they also return the main groups of emotional traits. Main groups are the taxonomy groups corresponding to the most relevant emotional traits expressed in the text. They provide an easy-to-read indication of the clusters of emotional traits the text is more about, similarly to an abstract. A. Dabrowski, \"Emotions in philosophy. A short introduction\", Studia Humana, 2016, 5:3, 8-20. E. Kim, R. Klinger, \"A Survey on Sentiment and Emotion Analysis for Computational Literary Studies\", Institut f\u00fcr Maschinelle Sprachverarbeitung, University of Stuttgart, 2018. A. Yadollahi, A. G. Shahraki, O. S. Zaiane, \"Current State of Text Sentiment Analysis from Opinion to Emotion Mining\", University of Alberta, 2017. R. Donovan, A. Johnson, A. deRoiste, R. O'Reilly, \"Quantifying the Links between Personality Sub-Traits and the Basic Emotions\", Computer Science and its Applications, 2020. \u21a9","title":"Emotional traits classification"},{"location":"guide/classification/geotax/","text":"GeoTax classification The classification resources corresponding to the geotax taxonomy classify texts according to the names of the countries mentioned directly or indirectly. An example of indirect mention is the name of a city or that of a geographinal feature. For example: England \u2192 direct mention London \u2192 city, indirect mention of England Thames \u2192 river, indirect mention of England Use the self-documentation resources to get a list of the recognized countries. In the case of the US and UK, both the state/country and the federation/kingdom are returned. For example, in the case of this text: Chicago is the birthplace of many celebrities such as Walt Disney. Both Illinois and United States of America are returned. The same resources, when requested with a specific query-string parameter , also return countries' information as GeoJSON data.","title":"GeoTax"},{"location":"guide/classification/geotax/#geotax-classification","text":"The classification resources corresponding to the geotax taxonomy classify texts according to the names of the countries mentioned directly or indirectly. An example of indirect mention is the name of a city or that of a geographinal feature. For example: England \u2192 direct mention London \u2192 city, indirect mention of England Thames \u2192 river, indirect mention of England Use the self-documentation resources to get a list of the recognized countries. In the case of the US and UK, both the state/country and the federation/kingdom are returned. For example, in the case of this text: Chicago is the birthplace of many celebrities such as Walt Disney. Both Illinois and United States of America are returned. The same resources, when requested with a specific query-string parameter , also return countries' information as GeoJSON data.","title":"GeoTax classification"},{"location":"guide/classification/iptc-media-topics/","text":"IPTC Media Topics classification The document classification resources corresponding to the iptc taxonomy classify texts in terms of IPTC Media Topics subject codes. IPTC is the global standards body of the news media and as such, this type of classification is geared towards news media. Use the self-documentation resources to get the complete and updated list of recognized categories.","title":"IPTC Media Topics"},{"location":"guide/classification/iptc-media-topics/#iptc-media-topics-classification","text":"The document classification resources corresponding to the iptc taxonomy classify texts in terms of IPTC Media Topics subject codes. IPTC is the global standards body of the news media and as such, this type of classification is geared towards news media. Use the self-documentation resources to get the complete and updated list of recognized categories.","title":"IPTC Media Topics classification"},{"location":"guide/classification/taxonomies-info/","text":"Get information about taxonomies List of available taxonomies The API provides a self-documentation resource to programmatically discover available taxonomies and their features. It has this path: taxonomies Therefore, the complete URL is: https://nlapi.expert.ai/v2/taxonomies It must be requested with the GET method. It returns the list of available taxonomies along with the supported languages. In the reference section of this manual you will find all the information you need to get taxonomies information using the API's RESTful interface, specifically: How to build the resource path and the full endpoint The output format Note Even if you use the API through a client that hides the REST interface, whether it is made by you or is one of those provided by expert.ai , knowing the output format helps you understand and navigate the results. Here is an example of getting taxonomies information: Python This example is based on the Python client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the list of taxonomies with the language they support. from expertai.nlapi.cloud.client import ExpertAiClient client = ExpertAiClient () output = client . taxonomies () print ( \"Taxonomies:\" ) for taxonomy in output . taxonomies : print ( taxonomy . name ) print ( \" \\t Languages:\" ) for language in taxonomy . languages : print ( \" \\t \" , language . code ) Java This example is based on the Java client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the JSON response . import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.cloud.InfoAPI ; import ai.expert.nlapi.v2.cloud.InfoAPIConfig ; import ai.expert.nlapi.v2.message.TaxonomiesResponse ; public class Main { public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static void main ( String [] args ) { try { InfoAPI infoAPI = new InfoAPI ( InfoAPIConfig . builder () . withAuthentication ( createAuthentication ()) . withVersion ( API . Versions . V2 ) . build ()); TaxonomiesResponse taxonomies = infoAPI . getTaxonomies (); taxonomies . prettyPrint (); } catch ( Exception ex ) { ex . printStackTrace (); } } } curl (Linux) The following curl command gets the taxonomies documentation resource of the API's REST interface. Run the command from a shell after replacing token with the actual authorization token . curl -X GET https://nlapi.expert.ai/v2/taxonomies \\ -H 'Authorization: Bearer token' The server returns a JSON object . curl (Windows) The following curl command gets the taxonomies documentation resource of the API's REST interface. Open a command prompt in the folder where you installed curl and run the command after replacing token with the actual authorization token . curl -X GET https://nlapi.expert.ai/v2/taxonomies -H \"Authorization: Bearer token\" The server returns a JSON object . Category tree The API also provides self-documentation resources that return the category tree for a given taxonomy-language combination. These resources have paths like this: taxonomies / taxonomy name / language code Boxed parts are placeholders, so for example: https://nlapi.expert.ai/v2/taxonomies/iptc/en is the URL of the resource returning the category tree for the iptc taxonomy for the English language. These resources must be requested with the GET method. In the reference section of this manual you will find all the information you need to get these resources using the API's RESTful interface, specifically: How to build resources' paths and full endpoints The output format Note Even if you use the API through a client that hides the REST interface, whether it is made by you or is one of those provided by expert.ai , knowing the output format helps you understand and navigate the results. For example, here is how to get the category tree of the geotax taxonomy for English: Python This example is based on the Python client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the category tree. from expertai.nlapi.cloud.client import ExpertAiClient def printCategory ( level , category ): tabs = \" \\t \" * level print ( tabs , category . id , \"(\" , category . label , \")\" ) for nestedCategory in category . categories : printCategory ( level + 1 , nestedCategory ) client = ExpertAiClient () taxonomy = 'geotax' language = 'en' output = client . taxonomy ( params = { 'taxonomy' : taxonomy , 'language' : language }) print ( \"geotax categories' tree:\" ) for category in output . taxonomy [ 0 ] . categories : printCategory ( 1 , category ) Java This example is based on the Java client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the JSON response . import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.cloud.InfoAPI ; import ai.expert.nlapi.v2.cloud.InfoAPIConfig ; import ai.expert.nlapi.v2.message.TaxonomyResponse ; public class Main { public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static void main ( String [] args ) { try { InfoAPI infoAPI = new InfoAPI ( InfoAPIConfig . builder () . withAuthentication ( createAuthentication ()) . withVersion ( API . Versions . V2 ) . build ()); TaxonomyResponse taxonomy = infoAPI . getTaxonomy ( \"geotax\" , API . Languages . en ); taxonomy . prettyPrint (); } catch ( Exception ex ) { ex . printStackTrace (); } } } curl (Linux) The following curl command gets the resource of the API's REST interface that returns the categories' tree of the English geotax taxonomy. Run the command from a shell after replacing token with the actual authorization token . curl -X GET https://nlapi.expert.ai/v2/taxonomies/geotax/en \\ -H 'Authorization: Bearer token' The server returns a JSON object . curl (Windows) The following curl command gets the resource of the API's REST interface that returns the categories' tree of the English geotax taxonomy. Open a command prompt in the folder where you installed curl and run the command after replacing token with the actual authorization token . curl -X GET https://nlapi.expert.ai/v2/geotax/en -H \"Authorization: Bearer token\" The server returns a JSON object .","title":"Taxonomies info"},{"location":"guide/classification/taxonomies-info/#get-information-about-taxonomies","text":"","title":"Get information about taxonomies"},{"location":"guide/classification/taxonomies-info/#list-of-available-taxonomies","text":"The API provides a self-documentation resource to programmatically discover available taxonomies and their features. It has this path: taxonomies Therefore, the complete URL is: https://nlapi.expert.ai/v2/taxonomies It must be requested with the GET method. It returns the list of available taxonomies along with the supported languages. In the reference section of this manual you will find all the information you need to get taxonomies information using the API's RESTful interface, specifically: How to build the resource path and the full endpoint The output format Note Even if you use the API through a client that hides the REST interface, whether it is made by you or is one of those provided by expert.ai , knowing the output format helps you understand and navigate the results. Here is an example of getting taxonomies information: Python This example is based on the Python client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the list of taxonomies with the language they support. from expertai.nlapi.cloud.client import ExpertAiClient client = ExpertAiClient () output = client . taxonomies () print ( \"Taxonomies:\" ) for taxonomy in output . taxonomies : print ( taxonomy . name ) print ( \" \\t Languages:\" ) for language in taxonomy . languages : print ( \" \\t \" , language . code ) Java This example is based on the Java client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the JSON response . import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.cloud.InfoAPI ; import ai.expert.nlapi.v2.cloud.InfoAPIConfig ; import ai.expert.nlapi.v2.message.TaxonomiesResponse ; public class Main { public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static void main ( String [] args ) { try { InfoAPI infoAPI = new InfoAPI ( InfoAPIConfig . builder () . withAuthentication ( createAuthentication ()) . withVersion ( API . Versions . V2 ) . build ()); TaxonomiesResponse taxonomies = infoAPI . getTaxonomies (); taxonomies . prettyPrint (); } catch ( Exception ex ) { ex . printStackTrace (); } } } curl (Linux) The following curl command gets the taxonomies documentation resource of the API's REST interface. Run the command from a shell after replacing token with the actual authorization token . curl -X GET https://nlapi.expert.ai/v2/taxonomies \\ -H 'Authorization: Bearer token' The server returns a JSON object . curl (Windows) The following curl command gets the taxonomies documentation resource of the API's REST interface. Open a command prompt in the folder where you installed curl and run the command after replacing token with the actual authorization token . curl -X GET https://nlapi.expert.ai/v2/taxonomies -H \"Authorization: Bearer token\" The server returns a JSON object .","title":"List of available taxonomies"},{"location":"guide/classification/taxonomies-info/#category-tree","text":"The API also provides self-documentation resources that return the category tree for a given taxonomy-language combination. These resources have paths like this: taxonomies / taxonomy name / language code Boxed parts are placeholders, so for example: https://nlapi.expert.ai/v2/taxonomies/iptc/en is the URL of the resource returning the category tree for the iptc taxonomy for the English language. These resources must be requested with the GET method. In the reference section of this manual you will find all the information you need to get these resources using the API's RESTful interface, specifically: How to build resources' paths and full endpoints The output format Note Even if you use the API through a client that hides the REST interface, whether it is made by you or is one of those provided by expert.ai , knowing the output format helps you understand and navigate the results. For example, here is how to get the category tree of the geotax taxonomy for English: Python This example is based on the Python client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the category tree. from expertai.nlapi.cloud.client import ExpertAiClient def printCategory ( level , category ): tabs = \" \\t \" * level print ( tabs , category . id , \"(\" , category . label , \")\" ) for nestedCategory in category . categories : printCategory ( level + 1 , nestedCategory ) client = ExpertAiClient () taxonomy = 'geotax' language = 'en' output = client . taxonomy ( params = { 'taxonomy' : taxonomy , 'language' : language }) print ( \"geotax categories' tree:\" ) for category in output . taxonomy [ 0 ] . categories : printCategory ( 1 , category ) Java This example is based on the Java client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the JSON response . import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.cloud.InfoAPI ; import ai.expert.nlapi.v2.cloud.InfoAPIConfig ; import ai.expert.nlapi.v2.message.TaxonomyResponse ; public class Main { public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static void main ( String [] args ) { try { InfoAPI infoAPI = new InfoAPI ( InfoAPIConfig . builder () . withAuthentication ( createAuthentication ()) . withVersion ( API . Versions . V2 ) . build ()); TaxonomyResponse taxonomy = infoAPI . getTaxonomy ( \"geotax\" , API . Languages . en ); taxonomy . prettyPrint (); } catch ( Exception ex ) { ex . printStackTrace (); } } } curl (Linux) The following curl command gets the resource of the API's REST interface that returns the categories' tree of the English geotax taxonomy. Run the command from a shell after replacing token with the actual authorization token . curl -X GET https://nlapi.expert.ai/v2/taxonomies/geotax/en \\ -H 'Authorization: Bearer token' The server returns a JSON object . curl (Windows) The following curl command gets the resource of the API's REST interface that returns the categories' tree of the English geotax taxonomy. Open a command prompt in the folder where you installed curl and run the command after replacing token with the actual authorization token . curl -X GET https://nlapi.expert.ai/v2/geotax/en -H \"Authorization: Bearer token\" The server returns a JSON object .","title":"Category tree"},{"location":"guide/contexts-and-kg/","text":"Contexts Contexts for document analysis Document analysis resources operate within a context . The context determines the type of Knowledge Graph to use. A context can have more Knowledge Graphs of the same type to support as many languages. standard context To date, the API has only one context (named standard ) using universal, all-purpose Knowledge Graphs. Other contexts may be added in the future, equipped with domain-specific Knowledge Graphs. This is the overview of the document analysis capabilities and languages available for the standard context: Capability English Spanish French German Italian Deep linguistic analysis \u2714 \u2714 \u2714 \u2714 \u2714 Keyphrase extraction \u2714 \u2714 \u2714 \u2714 \u2714 Named entity recognition \u2714 \u2714 \u2714 \u2714 \u2714 Relation extraction \u2714 \u2714 \u2714 \u2714 \u2714 Sentiment analysis \u2714 Full analysis \u2714 \u2714* \u2714* \u2714* \u2714* * Doesn't include sentiment analysis. Self-documentation resource The API provides a self-documentation resource to discover available contexts and their features. It has this path: contexts Therefore, the complete URL is: https://nlapi.expert.ai/v2/contexts It must be requested with the GET method. It returns the list of available contexts along with the supported languages and analyses as shown in the above table. In the reference section of this manual you will find all the information you need to get contexts information using the API's RESTful interface, specifically: how to build the resource path and the full endpoint the format of the output Note Even if you use the API through a client that hides the REST interface, whether it is made by you or is one of those provided by expert.ai , knowing the output format helps you understand and navigate the results. Here is an example of getting contexts information: Python This example is based on the Python client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the list of contexts with the language they support. from expertai.nlapi.cloud.client import ExpertAiClient client = ExpertAiClient () output = client . contexts () # Contexts print ( \"Contexts: \\n \" ) for context in output . contexts : print ( context . name ) print ( \" \\t Languages:\" ) for language in context . languages : print ( \" \\t\\t {} \" . format ( language . code )) Java This example is based on the Java client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the JSON response . import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.cloud.InfoAPI ; import ai.expert.nlapi.v2.cloud.InfoAPIConfig ; import ai.expert.nlapi.v2.message.ContextsResponse ; public class Main { public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static void main ( String [] args ) { try { InfoAPI infoAPI = new InfoAPI ( InfoAPIConfig . builder () . withAuthentication ( createAuthentication ()) . withVersion ( API . Versions . V2 ) . build ()); ContextsResponse contexts = infoAPI . getContexts (); contexts . prettyPrint (); } catch ( Exception ex ) { ex . printStackTrace (); } } } curl (Linux) The following curl command gets the contexts documentation resource of the API's REST interface. Run the command from a shell after replacing token with the actual authorization token . curl -X GET https://nlapi.expert.ai/v2/contexts \\ -H 'Authorization: Bearer token' The server returns a JSON object . curl (Windows) The following curl command gets the contexts documentation resource of the API's REST interface. Open a command prompt in the folder where you installed curl and run the command after replacing token with the actual authorization token . curl -X GET https://nlapi.expert.ai/v2/contexts -H \"Authorization: Bearer token\" The server returns a JSON object . The Knowledge Graph The expert.ai Knowledge Graph is a concept-based representation of universal or domain-specific knowledge for a given language. Each entry in the Knowledge Graph corresponds to a concept. There are entries for common nouns, proper nouns, verbs, adjectives and adverbs. Each entry contains information, for example: The terms that can be used to express the concept in a text, for example hand, pass, pass on, hand off, turn over, reach . the corresponding part-of-speech\u2014for example (to) climb \u2192 verb\u2014and other grammatical information on the terms. The topics to which the concept corresponds, for example soprano \u2192 opera, singing. References to external knowledge bases such as Wikidata, DBpedia, GeoNames, etc. Extended proprieties, for example the coordinates of places. Modeling concepts that can be expressed in a language are not sufficient to enable the text analysis software to interpret ambiguous terms alone. For example, consider that in the expert.ai universal Knowledge Graph for the English language there are more than 20 entries for the verb (to) put . The single entry has statistical information indicating the frequency with which the concept is used in a reference corpus compared to other concepts that can be expressed with the same word. This information is useful for disambiguation, but insufficient. Using statistics alone can lead to incorrect interpretations and to a textual analysis of low quality and even lower usefulness. What really improves the results are the relationships between concepts, hence the term Knowledge Graph . A single entry is linked to one or more other entries and, as such, relationships can be numerous. For example, a concept can be connected to other concepts in the hierarchical relationship called \"IS-A\". So: sodium IS A alkaline metal IS A metal IS A element Or there can be a \"part-whole\" relationship: wheel IS A PART OF car clutch IS A PART OF car dashboard IS A PART OF car Relationships are designed to be navigated in both directions, so from the concept of car it is possible to discover the parts that make it up (wheels, clutch, dashboard, etc.) by navigating the \"IS PART OF\" relationships downstream. In the same way, for the \"IS-A\" relationship, starting from the concept of alkaline metal it is possible to discover which elements are \"types of\" the parent concept (sodium, cesium, lithium, etc.). Relationships can be one-to-many. If this is obvious for the \"part-whole\" relations if read from the \"whole\" to the parts and for the \"IS A\" relationship if read from the more generic concept to the more specific ones, it is not obvious in the opposite direction. However it can be, for example: cat IS A feline but also: cat IS A pet So it is possible that in a hierarchical relationship a concept can have multiple \"parents\". The relationships between Knowledge Graph entries are the foundations of solid disambiguation. Suppose the text contains a form of the verb (to) put . As stated, the standard English Knowledge Graph contains more than 20 different concepts that can be expressed with (to) put , but which is the right one? Relationships can help. The text analysis software can explore the relationships of each concept to find out if the concept itself is linked to other concepts expressed in the same text. The concept with more links to other concepts is a good candidate for the \"right concept\". The disambiguation of one word helps to disambiguate the others, but the text analysis software is always free to \"go back\" and correct its previous clarification choices as it proceeds with the analysis of the other words of the text, with a chain effect on other disambiguations. The name used by expert.ai to designate an entry in a Knowledge Graph is syncon .","title":"Contexts"},{"location":"guide/contexts-and-kg/#contexts","text":"","title":"Contexts"},{"location":"guide/contexts-and-kg/#contexts-for-document-analysis","text":"Document analysis resources operate within a context . The context determines the type of Knowledge Graph to use. A context can have more Knowledge Graphs of the same type to support as many languages.","title":"Contexts for document analysis"},{"location":"guide/contexts-and-kg/#standard-context","text":"To date, the API has only one context (named standard ) using universal, all-purpose Knowledge Graphs. Other contexts may be added in the future, equipped with domain-specific Knowledge Graphs. This is the overview of the document analysis capabilities and languages available for the standard context: Capability English Spanish French German Italian Deep linguistic analysis \u2714 \u2714 \u2714 \u2714 \u2714 Keyphrase extraction \u2714 \u2714 \u2714 \u2714 \u2714 Named entity recognition \u2714 \u2714 \u2714 \u2714 \u2714 Relation extraction \u2714 \u2714 \u2714 \u2714 \u2714 Sentiment analysis \u2714 Full analysis \u2714 \u2714* \u2714* \u2714* \u2714* * Doesn't include sentiment analysis.","title":"standard context"},{"location":"guide/contexts-and-kg/#self-documentation-resource","text":"The API provides a self-documentation resource to discover available contexts and their features. It has this path: contexts Therefore, the complete URL is: https://nlapi.expert.ai/v2/contexts It must be requested with the GET method. It returns the list of available contexts along with the supported languages and analyses as shown in the above table. In the reference section of this manual you will find all the information you need to get contexts information using the API's RESTful interface, specifically: how to build the resource path and the full endpoint the format of the output Note Even if you use the API through a client that hides the REST interface, whether it is made by you or is one of those provided by expert.ai , knowing the output format helps you understand and navigate the results. Here is an example of getting contexts information: Python This example is based on the Python client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the list of contexts with the language they support. from expertai.nlapi.cloud.client import ExpertAiClient client = ExpertAiClient () output = client . contexts () # Contexts print ( \"Contexts: \\n \" ) for context in output . contexts : print ( context . name ) print ( \" \\t Languages:\" ) for language in context . languages : print ( \" \\t\\t {} \" . format ( language . code )) Java This example is based on the Java client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the JSON response . import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.cloud.InfoAPI ; import ai.expert.nlapi.v2.cloud.InfoAPIConfig ; import ai.expert.nlapi.v2.message.ContextsResponse ; public class Main { public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static void main ( String [] args ) { try { InfoAPI infoAPI = new InfoAPI ( InfoAPIConfig . builder () . withAuthentication ( createAuthentication ()) . withVersion ( API . Versions . V2 ) . build ()); ContextsResponse contexts = infoAPI . getContexts (); contexts . prettyPrint (); } catch ( Exception ex ) { ex . printStackTrace (); } } } curl (Linux) The following curl command gets the contexts documentation resource of the API's REST interface. Run the command from a shell after replacing token with the actual authorization token . curl -X GET https://nlapi.expert.ai/v2/contexts \\ -H 'Authorization: Bearer token' The server returns a JSON object . curl (Windows) The following curl command gets the contexts documentation resource of the API's REST interface. Open a command prompt in the folder where you installed curl and run the command after replacing token with the actual authorization token . curl -X GET https://nlapi.expert.ai/v2/contexts -H \"Authorization: Bearer token\" The server returns a JSON object .","title":"Self-documentation resource"},{"location":"guide/contexts-and-kg/#the-knowledge-graph","text":"The expert.ai Knowledge Graph is a concept-based representation of universal or domain-specific knowledge for a given language. Each entry in the Knowledge Graph corresponds to a concept. There are entries for common nouns, proper nouns, verbs, adjectives and adverbs. Each entry contains information, for example: The terms that can be used to express the concept in a text, for example hand, pass, pass on, hand off, turn over, reach . the corresponding part-of-speech\u2014for example (to) climb \u2192 verb\u2014and other grammatical information on the terms. The topics to which the concept corresponds, for example soprano \u2192 opera, singing. References to external knowledge bases such as Wikidata, DBpedia, GeoNames, etc. Extended proprieties, for example the coordinates of places. Modeling concepts that can be expressed in a language are not sufficient to enable the text analysis software to interpret ambiguous terms alone. For example, consider that in the expert.ai universal Knowledge Graph for the English language there are more than 20 entries for the verb (to) put . The single entry has statistical information indicating the frequency with which the concept is used in a reference corpus compared to other concepts that can be expressed with the same word. This information is useful for disambiguation, but insufficient. Using statistics alone can lead to incorrect interpretations and to a textual analysis of low quality and even lower usefulness. What really improves the results are the relationships between concepts, hence the term Knowledge Graph . A single entry is linked to one or more other entries and, as such, relationships can be numerous. For example, a concept can be connected to other concepts in the hierarchical relationship called \"IS-A\". So: sodium IS A alkaline metal IS A metal IS A element Or there can be a \"part-whole\" relationship: wheel IS A PART OF car clutch IS A PART OF car dashboard IS A PART OF car Relationships are designed to be navigated in both directions, so from the concept of car it is possible to discover the parts that make it up (wheels, clutch, dashboard, etc.) by navigating the \"IS PART OF\" relationships downstream. In the same way, for the \"IS-A\" relationship, starting from the concept of alkaline metal it is possible to discover which elements are \"types of\" the parent concept (sodium, cesium, lithium, etc.). Relationships can be one-to-many. If this is obvious for the \"part-whole\" relations if read from the \"whole\" to the parts and for the \"IS A\" relationship if read from the more generic concept to the more specific ones, it is not obvious in the opposite direction. However it can be, for example: cat IS A feline but also: cat IS A pet So it is possible that in a hierarchical relationship a concept can have multiple \"parents\". The relationships between Knowledge Graph entries are the foundations of solid disambiguation. Suppose the text contains a form of the verb (to) put . As stated, the standard English Knowledge Graph contains more than 20 different concepts that can be expressed with (to) put , but which is the right one? Relationships can help. The text analysis software can explore the relationships of each concept to find out if the concept itself is linked to other concepts expressed in the same text. The concept with more links to other concepts is a good candidate for the \"right concept\". The disambiguation of one word helps to disambiguate the others, but the text analysis software is always free to \"go back\" and correct its previous clarification choices as it proceeds with the analysis of the other words of the text, with a chain effect on other disambiguations. The name used by expert.ai to designate an entry in a Knowledge Graph is syncon .","title":"The Knowledge Graph"},{"location":"guide/detection/","text":"Information detection Detection and detectors Information detection identifies and extracts information from a text. The software modules performing information detection are called detectors . Available detectors are: Detector name English Spanish French German Italian pii \u2714 \u2714 writeprint \u2714 \u2714 \u2714 \u2714 \u2714 temporal-information \u2714 \u2714 \u2714 \u2714 \u2714 Detectors' API resources have paths like: detect / detector name / language code Boxed parts are placeholders, so for example: https://nlapi.expert.ai/v2/detect/pii/en is the URL of the pii detector resource performing PII detection on an English text. These resources must be requested with the POST method, submitting the text in which to detect information. In the reference section of this manual you will find all the information you need to perform information detection using the API's RESTful interface, specifically: The format of the request to be submitted to the resources. How to build resources' paths and full endpoints . The output format . Note Even if you use the API through a client that hides the REST interface, whether it is made by you or is one of those provided by expert.ai , knowing the output format helps you understand and navigate the results. Here is an example of PII detection on a short English text: Python This example is based on the Python client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program detects Personally Identifiable Information (PII) in a short English text and prints the portion of the output containing results in JSON-LD format. import json from expertai.nlapi.cloud.client import ExpertAiClient client = ExpertAiClient () text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" detector = 'pii' language = 'en' output = client . detection ( body = { \"document\" : { \"text\" : text }}, params = { 'detector' : detector , 'language' : language }) # Output extra data containing the JSON-LD object print ( \"extra_data: \" , json . dumps ( output . extra_data , indent = 4 , sort_keys = True )) Java This example is based on the Java client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the JSON response . import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.cloud.Detector ; import ai.expert.nlapi.v2.cloud.DetectorConfig ; import ai.expert.nlapi.v2.message.DetectResponse ; import com.fasterxml.jackson.databind.ObjectMapper ; public class Main { public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static Detector createDetector () throws Exception { return new Detector ( DetectorConfig . builder () . withVersion ( API . Versions . V2 ) . withDetector ( \"pii\" ) . withLanguage ( API . Languages . en ) . withAuthentication ( createAuthentication ()) . build ()); } public static void main ( String [] args ) { try { String text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" ; Detector detector = createDetector (); DetectResponse detection = detector . detect ( text ); // Output the JSON-LD object contained in the response System . out . println ( \"JSON-LD output:\" ); Object jsonLd = detection . getData (). getExtraData (). get ( \"JSON-LD\" ); System . out . println ( new ObjectMapper (). writerWithDefaultPrettyPrinter (). writeValueAsString ( jsonLd )); } catch ( Exception ex ) { ex . printStackTrace (); } } } curl (Linux) The following curl command posts a document to the Personally Identifiable Information (PII) detection resource of the API's REST interface. Run the command from a shell after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/detect/pii/en \\ -H 'Authorization: Bearer token' \\ -H 'Content-Type: application/json; charset=utf-8' \\ -d '{ \"document\": { \"text\": \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan' \\' 's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" } }' The server returns a JSON object . curl (Windows) The following curl command posts a document to the Personally Identifiable Information (PII) detection resource of the API's REST interface. Open a command prompt in the folder where you installed curl and run the command after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/detect/pii/en -H \"Authorization: Bearer token\" -H \"Content-Type: application/json; charset=utf-8\" -d \"{\\\" document\\ \": {\\\" text\\ \": \\\" Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\\ \"}}\" The server returns a JSON object . The following articles describe the capabilities of the available detectors. Self-documentation resource The API provides a self-documentation resource to programmatically discover available detectors and their features. Learn more about this resource in the dedicated article .","title":"Overview"},{"location":"guide/detection/#information-detection","text":"","title":"Information detection"},{"location":"guide/detection/#detection-and-detectors","text":"Information detection identifies and extracts information from a text. The software modules performing information detection are called detectors . Available detectors are: Detector name English Spanish French German Italian pii \u2714 \u2714 writeprint \u2714 \u2714 \u2714 \u2714 \u2714 temporal-information \u2714 \u2714 \u2714 \u2714 \u2714 Detectors' API resources have paths like: detect / detector name / language code Boxed parts are placeholders, so for example: https://nlapi.expert.ai/v2/detect/pii/en is the URL of the pii detector resource performing PII detection on an English text. These resources must be requested with the POST method, submitting the text in which to detect information. In the reference section of this manual you will find all the information you need to perform information detection using the API's RESTful interface, specifically: The format of the request to be submitted to the resources. How to build resources' paths and full endpoints . The output format . Note Even if you use the API through a client that hides the REST interface, whether it is made by you or is one of those provided by expert.ai , knowing the output format helps you understand and navigate the results. Here is an example of PII detection on a short English text: Python This example is based on the Python client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program detects Personally Identifiable Information (PII) in a short English text and prints the portion of the output containing results in JSON-LD format. import json from expertai.nlapi.cloud.client import ExpertAiClient client = ExpertAiClient () text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" detector = 'pii' language = 'en' output = client . detection ( body = { \"document\" : { \"text\" : text }}, params = { 'detector' : detector , 'language' : language }) # Output extra data containing the JSON-LD object print ( \"extra_data: \" , json . dumps ( output . extra_data , indent = 4 , sort_keys = True )) Java This example is based on the Java client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the JSON response . import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.cloud.Detector ; import ai.expert.nlapi.v2.cloud.DetectorConfig ; import ai.expert.nlapi.v2.message.DetectResponse ; import com.fasterxml.jackson.databind.ObjectMapper ; public class Main { public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static Detector createDetector () throws Exception { return new Detector ( DetectorConfig . builder () . withVersion ( API . Versions . V2 ) . withDetector ( \"pii\" ) . withLanguage ( API . Languages . en ) . withAuthentication ( createAuthentication ()) . build ()); } public static void main ( String [] args ) { try { String text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" ; Detector detector = createDetector (); DetectResponse detection = detector . detect ( text ); // Output the JSON-LD object contained in the response System . out . println ( \"JSON-LD output:\" ); Object jsonLd = detection . getData (). getExtraData (). get ( \"JSON-LD\" ); System . out . println ( new ObjectMapper (). writerWithDefaultPrettyPrinter (). writeValueAsString ( jsonLd )); } catch ( Exception ex ) { ex . printStackTrace (); } } } curl (Linux) The following curl command posts a document to the Personally Identifiable Information (PII) detection resource of the API's REST interface. Run the command from a shell after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/detect/pii/en \\ -H 'Authorization: Bearer token' \\ -H 'Content-Type: application/json; charset=utf-8' \\ -d '{ \"document\": { \"text\": \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan' \\' 's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" } }' The server returns a JSON object . curl (Windows) The following curl command posts a document to the Personally Identifiable Information (PII) detection resource of the API's REST interface. Open a command prompt in the folder where you installed curl and run the command after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/detect/pii/en -H \"Authorization: Bearer token\" -H \"Content-Type: application/json; charset=utf-8\" -d \"{\\\" document\\ \": {\\\" text\\ \": \\\" Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\\ \"}}\" The server returns a JSON object . The following articles describe the capabilities of the available detectors.","title":"Detection and detectors"},{"location":"guide/detection/#self-documentation-resource","text":"The API provides a self-documentation resource to programmatically discover available detectors and their features. Learn more about this resource in the dedicated article .","title":"Self-documentation resource"},{"location":"guide/detection/detectors-info/","text":"Get information about detectors The API provides a self-documentation resource to programmatically discover available detectors and their features. It has this path: detectors Therefore, the complete URL is: https://nlapi.expert.ai/v2/detectors It must be requested with the GET method. It returns the list of available detectors along with the supported languages and, possibly, the Web address of a specific OpenAPI contract. In the reference section of this manual you will find all the information you need to use this resource, specifically: How to build the resource path and the full endpoint The output format Note Even if you use the API through a client that hides the REST interface, whether it is made by you or is one of those provided by expert.ai , knowing the output format helps you understand and navigate the results. Here is an example of obtaining detectors information: Python This example is based on the Python client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the list of taxonomies with the language they support. from expertai.nlapi.cloud.client import ExpertAiClient client = ExpertAiClient () output = client . detectors () # Detectors print ( \"Detectors: \\n \" ) for detector in output . detectors : print ( detector . name ) print ( \" \\t Languages:\" ) for language in detector . languages : print ( \" \\t\\t {} \" . format ( language . code )) print ( \" \\t Contract: {} \" . format ( detector . contract )) Java This example is based on the Java client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the JSON response . import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.message.TaxonomiesResponse ; import ai.expert.nlapi.v2.InfoAPI ; import ai.expert.nlapi.v2.InfoAPIConfig ; public class Main { public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static void main ( String [] args ) { try { InfoAPI infoAPI = new InfoAPI ( InfoAPIConfig . builder () . withAuthentication ( createAuthentication ()) . withVersion ( API . Versions . V2 ) . build ()); TaxonomiesResponse taxonomies = infoAPI . getTaxonomies (); taxonomies . prettyPrint (); } catch ( Exception ex ) { ex . printStackTrace (); } } } curl (Linux) The following curl command gets the taxonomies documentation resource of the API's REST interface. Run the command from a shell after replacing token with the actual authorization token . curl -X GET https://nlapi.expert.ai/v2/taxonomies \\ -H 'Authorization: Bearer token' The server returns a JSON object . curl (Windows) The following curl command gets the detectors documentation resource of the API's REST interface. Open a command prompt in the folder where you installed curl and run the command after replacing token with the actual authorization token . curl -X GET https://nlapi.expert.ai/v2/detectors -H \"Authorization: Bearer token\" The server returns a JSON object .","title":"Detectors info"},{"location":"guide/detection/detectors-info/#get-information-about-detectors","text":"The API provides a self-documentation resource to programmatically discover available detectors and their features. It has this path: detectors Therefore, the complete URL is: https://nlapi.expert.ai/v2/detectors It must be requested with the GET method. It returns the list of available detectors along with the supported languages and, possibly, the Web address of a specific OpenAPI contract. In the reference section of this manual you will find all the information you need to use this resource, specifically: How to build the resource path and the full endpoint The output format Note Even if you use the API through a client that hides the REST interface, whether it is made by you or is one of those provided by expert.ai , knowing the output format helps you understand and navigate the results. Here is an example of obtaining detectors information: Python This example is based on the Python client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the list of taxonomies with the language they support. from expertai.nlapi.cloud.client import ExpertAiClient client = ExpertAiClient () output = client . detectors () # Detectors print ( \"Detectors: \\n \" ) for detector in output . detectors : print ( detector . name ) print ( \" \\t Languages:\" ) for language in detector . languages : print ( \" \\t\\t {} \" . format ( language . code )) print ( \" \\t Contract: {} \" . format ( detector . contract )) Java This example is based on the Java client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the JSON response . import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.message.TaxonomiesResponse ; import ai.expert.nlapi.v2.InfoAPI ; import ai.expert.nlapi.v2.InfoAPIConfig ; public class Main { public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static void main ( String [] args ) { try { InfoAPI infoAPI = new InfoAPI ( InfoAPIConfig . builder () . withAuthentication ( createAuthentication ()) . withVersion ( API . Versions . V2 ) . build ()); TaxonomiesResponse taxonomies = infoAPI . getTaxonomies (); taxonomies . prettyPrint (); } catch ( Exception ex ) { ex . printStackTrace (); } } } curl (Linux) The following curl command gets the taxonomies documentation resource of the API's REST interface. Run the command from a shell after replacing token with the actual authorization token . curl -X GET https://nlapi.expert.ai/v2/taxonomies \\ -H 'Authorization: Bearer token' The server returns a JSON object . curl (Windows) The following curl command gets the detectors documentation resource of the API's REST interface. Open a command prompt in the folder where you installed curl and run the command after replacing token with the actual authorization token . curl -X GET https://nlapi.expert.ai/v2/detectors -H \"Authorization: Bearer token\" The server returns a JSON object .","title":"Get information about detectors"},{"location":"guide/detection/pii/","text":"PII detection The pii detector The pii detector identifies personally identifiable information ( PII ) and returns them together with their positions in the text as linked data in JSON-LD format (see also https://json-ld.org/ ). The detector's output allows you to determine if a document contains potentially sensitive data and possibly create a new version of the text in which the PII is de-identified . These are the information types pii can detect: Information type Notes Personal attributes Of a real person or a fictional character Postal address Bank account IP address E-mail address URL Financial product Credit or debit card Phone number These are the properties of each information type: Information type Property Linked data reference Personal attributes Full name of the person https://schema.org/Person First name https://schema.org/givenName Last name https://schema.org/familyName Age https://schema.org/Number Gender https://schema.org/gender Nationality https://schema.org/nationality Date of birth https://schema.org/birthDate Place of birth https://schema.org/birthPlace Date of death https://schema.org/deathDate Place of death https://schema.org/deathPlace Any date or a time related to the person https://schema.org/Date Postal address Full address https://schema.org/Text Street name and house number https://schema.org/streetAddress Country https://schema.org/addressCountry Postal code https://schema.org/postalCode Locality https://schema.org/addressLocality Region https://schema.org/addressRegion PO box number https://schema.org/postOfficeBoxNumber Bank account IBAN code https://schema.org/PropertyValue IBAN code country https://schema.org/Country IP address Address https://schema.org/Text E-mail address Address https://schema.org/email URL URL https://schema.org/URL Financial product Number of the credit/debit card https://schema.org/Text Card Verification Value (CVV) or Card Verification Code (CVC) https://schema.org/Number Card expiration date https://schema.org/Date Phone number Number https://schema.org/telephone Useful resources: How to request information detection API resources . How to interpret pii detector output . Tip To play with the JSON-LD object and get ideas for its possible uses, take a look at the JSON-LD playground site , where you can paste the JSON-LD object returned by the pii detector.","title":"PII"},{"location":"guide/detection/pii/#pii-detection","text":"","title":"PII detection"},{"location":"guide/detection/pii/#the-pii-detector","text":"The pii detector identifies personally identifiable information ( PII ) and returns them together with their positions in the text as linked data in JSON-LD format (see also https://json-ld.org/ ). The detector's output allows you to determine if a document contains potentially sensitive data and possibly create a new version of the text in which the PII is de-identified . These are the information types pii can detect: Information type Notes Personal attributes Of a real person or a fictional character Postal address Bank account IP address E-mail address URL Financial product Credit or debit card Phone number These are the properties of each information type: Information type Property Linked data reference Personal attributes Full name of the person https://schema.org/Person First name https://schema.org/givenName Last name https://schema.org/familyName Age https://schema.org/Number Gender https://schema.org/gender Nationality https://schema.org/nationality Date of birth https://schema.org/birthDate Place of birth https://schema.org/birthPlace Date of death https://schema.org/deathDate Place of death https://schema.org/deathPlace Any date or a time related to the person https://schema.org/Date Postal address Full address https://schema.org/Text Street name and house number https://schema.org/streetAddress Country https://schema.org/addressCountry Postal code https://schema.org/postalCode Locality https://schema.org/addressLocality Region https://schema.org/addressRegion PO box number https://schema.org/postOfficeBoxNumber Bank account IBAN code https://schema.org/PropertyValue IBAN code country https://schema.org/Country IP address Address https://schema.org/Text E-mail address Address https://schema.org/email URL URL https://schema.org/URL Financial product Number of the credit/debit card https://schema.org/Text Card Verification Value (CVV) or Card Verification Code (CVC) https://schema.org/Number Card expiration date https://schema.org/Date Phone number Number https://schema.org/telephone","title":"The pii detector"},{"location":"guide/detection/pii/#useful-resources","text":"How to request information detection API resources . How to interpret pii detector output . Tip To play with the JSON-LD object and get ideas for its possible uses, take a look at the JSON-LD playground site , where you can paste the JSON-LD object returned by the pii detector.","title":"Useful resources:"},{"location":"guide/detection/temporal-information/","text":"Temporal information detection Introduction The temporal-information detector identifies and returns\u2014in normalized format\u2014the following time expressions: Expression type Description Examples Time point A specific time The Declaration was signed on July 4, 1776 . The plane should take off at 3PM . In November 1996 I traveled to the United States. A big change took place in 2016 . Time span An amount of time units, from minutes up to years It took her three minutes to find the solution. Your condition will continue to improve over the next two weeks . I have known her for more than ten years . Time interval A time interval (from ... to ...) He ruled the region from July 2005 to September 2011. Value normalization Time point Time point expressions are normalized according to the ISO 8601 standard. Here are some examples of value normalization: July 4, 1776 \u2192 1776-07-04 3PM \u2192 -----T15 November 1996 \u2192 1996-11 2016 \u2192 2016 Time span Time span expressions are normalized using this format: [ sign ] quantity unit-of-measure For example: Seven days \u2192 7 day Possible units of measure are: year month day hour minute Quantity can be indefinite , for example: For some months I've been skeptical, but now I believe it. \u2192 indefinite month Weeks are returned as days, centuries as years, etc. For example: Two weeks \u2192 14 day The optional sign can be + (plus sign) or - (minus sign) and denotes time spans that \"add\" or \"subtract\" time from an explicit or implicit time point. For example: The first agreement was in 2009. After eight years the companies merged. \u2192 + 8 year , meaning eight years after 2009 These events took place 30 years ago . \u2192 - 30 year , meaning thirty years before a time that was \"this year\"\u2014but not further specified\u2014for the writer or speaker. Time interval Time interval expression are normalized using this format: \"from\" time point / \"to\" time point Left and right time points are normalized according to the ISO 8601 standard. For example: from July 2005 to September 2011 \u2192 2005-07/2011-09 Derived time points The detector is also capable of deriving time points from deixes referring to other time points. In particular, the detector can derive a time point from: \"Before/after\" expressions that are recognized as time spans and referred to a time point For example, in: I first traveled to Australia in 2002 and returned fifteen years later , the expression fifteen years later is not recognizable as a time point by itself, but is recognized as a time span (normalized to + 15 year ) and, once referred to 2002 \u2014which is recognized as a time point\u2014, makes the detector derive a new time point by adding the time span. In this case the new time point is 2017 , which is 2002 + 15. \"Equal to\" expressions referred to a time point For example, in: We met on July 27, 2014 . On the same day we became business partners. the expression On the same day is not recognizable as a time point by itself, but once referred to July 27, 2014 \u2014which is recognized as a time point\u2014and interpreted as \"the same\", makes the detector derive another occurrence of the explicit time point, in this case 2014-07-27 . Derived time points are flagged as such in the detectors' output . Useful resources How to request information detection API resources . How to interpret the output of the temporal-information detector.","title":"Temporal information"},{"location":"guide/detection/temporal-information/#temporal-information-detection","text":"","title":"Temporal information detection"},{"location":"guide/detection/temporal-information/#introduction","text":"The temporal-information detector identifies and returns\u2014in normalized format\u2014the following time expressions: Expression type Description Examples Time point A specific time The Declaration was signed on July 4, 1776 . The plane should take off at 3PM . In November 1996 I traveled to the United States. A big change took place in 2016 . Time span An amount of time units, from minutes up to years It took her three minutes to find the solution. Your condition will continue to improve over the next two weeks . I have known her for more than ten years . Time interval A time interval (from ... to ...) He ruled the region from July 2005 to September 2011.","title":"Introduction"},{"location":"guide/detection/temporal-information/#value-normalization","text":"","title":"Value normalization"},{"location":"guide/detection/temporal-information/#time-point","text":"Time point expressions are normalized according to the ISO 8601 standard. Here are some examples of value normalization: July 4, 1776 \u2192 1776-07-04 3PM \u2192 -----T15 November 1996 \u2192 1996-11 2016 \u2192 2016","title":"Time point"},{"location":"guide/detection/temporal-information/#time-span","text":"Time span expressions are normalized using this format: [ sign ] quantity unit-of-measure For example: Seven days \u2192 7 day Possible units of measure are: year month day hour minute Quantity can be indefinite , for example: For some months I've been skeptical, but now I believe it. \u2192 indefinite month Weeks are returned as days, centuries as years, etc. For example: Two weeks \u2192 14 day The optional sign can be + (plus sign) or - (minus sign) and denotes time spans that \"add\" or \"subtract\" time from an explicit or implicit time point. For example: The first agreement was in 2009. After eight years the companies merged. \u2192 + 8 year , meaning eight years after 2009 These events took place 30 years ago . \u2192 - 30 year , meaning thirty years before a time that was \"this year\"\u2014but not further specified\u2014for the writer or speaker.","title":"Time span"},{"location":"guide/detection/temporal-information/#time-interval","text":"Time interval expression are normalized using this format: \"from\" time point / \"to\" time point Left and right time points are normalized according to the ISO 8601 standard. For example: from July 2005 to September 2011 \u2192 2005-07/2011-09","title":"Time interval"},{"location":"guide/detection/temporal-information/#derived-time-points","text":"The detector is also capable of deriving time points from deixes referring to other time points. In particular, the detector can derive a time point from: \"Before/after\" expressions that are recognized as time spans and referred to a time point For example, in: I first traveled to Australia in 2002 and returned fifteen years later , the expression fifteen years later is not recognizable as a time point by itself, but is recognized as a time span (normalized to + 15 year ) and, once referred to 2002 \u2014which is recognized as a time point\u2014, makes the detector derive a new time point by adding the time span. In this case the new time point is 2017 , which is 2002 + 15. \"Equal to\" expressions referred to a time point For example, in: We met on July 27, 2014 . On the same day we became business partners. the expression On the same day is not recognizable as a time point by itself, but once referred to July 27, 2014 \u2014which is recognized as a time point\u2014and interpreted as \"the same\", makes the detector derive another occurrence of the explicit time point, in this case 2014-07-27 . Derived time points are flagged as such in the detectors' output .","title":"Derived time points"},{"location":"guide/detection/temporal-information/#useful-resources","text":"How to request information detection API resources . How to interpret the output of the temporal-information detector.","title":"Useful resources"},{"location":"guide/detection/writeprint/","text":"Writeprint detection Introduction The writeprint information detection resources\u2014one for each of the supported languages\u2014 perform a stylometric analysis of a text, ranging from readability and vocabulary richness to verb types, document structure and grammar. The detector is also capable of identifying the markers of several specific-purpose languages. Stylometric data is provided in the form of 60 indexes (see below) which, as a whole, make up for a complete fingerprint of the document, i.e. its writeprint . By comparing a number of documents on the basis of their writeprint, the literary attributes of the author are highlighted by this authorship analysis tool. Writeprint information is returned as a JSON-LD object embedded in a broader JSON object. Readability indexes These are the readability indexes: Index name Description Output data Coleman-Liau The Coleman-Liau index , which value approximates the U.S. grade level thought necessary to comprehend the text. Value and degree of difficulty Gulpease The Gulpease index , based on word length and best suited for the Italian language. Value and degree of difficulty Automated Readability Index (ARI) The Automated Readability Index , which, like the Coleman-Liau index, produces an approximate representation of the US grade level needed to comprehend the text and is best suited for the English language. Value and degree of difficulty Spelling The following indexes measure aspects related to the spelling of words and the presence of particular punctuation characters. Index name Notes Sentences starting with a capital letter (ratio) The ratio of sentences in which the first word starts with an uppercase letter to the number of sentences. Sentences starting with a small letter (ratio) The ratio of sentences in which the first word starts with a lowercase letter to the number of sentences. Emoticons per sentence Dots per sentence The presence of dots in addition to the period at the end of the sentence can be indicative of a concise language because of abbreviations. Multiple dots per sentence such as Ellipsis points and longer sequences. Question marks per sentence Indicative of the ratio of questions to the total number of sentences. Multiple question marks per sentence Exclamation marks per sentence Multiple exclamation marks per sentence Exclamation mark, question mark sequences per sentence Commas per sentence Colons per sentence Semicolons per sentence Single quotation marks per sentence Double quotation marks per sentence Text subdivision The following indexes count the occurrences or measure the length of certain subdivisions of the text, from sentences to characters. Index name Notes Sentences Tokens Words are tokens, but consecutive words recognized as a unit\u2014like credit card or red carpet \u2014and punctuation marks are also tokens. Token length per sentence Characters per sentence Atoms per sentence Words and punctuation marks are both tokens (see above) and atoms, except in the case of consecutive words recognized as a unit. In that case, the constituent words are atoms, while the multi-word unit is a single token. Tokens per sentence Phrases per sentence Grammar The following indexes count the occurrences per sentence of the different parts of speech. Index name Adjectives per sentence Adverbs per sentence Articles per sentence Auxiliaries per sentence Conjunctions per sentence Nouns per sentence Proper nouns per sentence Punctuation per sentence Prepositions per sentence Pronouns per sentence Particles per sentence Verbs per sentence Phrase types The indexes that follow count the number of different phrases per sentence. Index name Adjective phrases per sentence Conjunction phrases per sentence Adverb phrases per sentence Noun phrases per sentence Nominal predicates per sentence Preposition phrases per sentence Relative phrases per sentence Verb phrases per sentence Language variety and errors The following indexes measure various aspects of the text that are indicative of greater or lesser variety of the language and the presence of the most common errors. Index name Notes Different types of verbs This index is related to the meaning of verbs and their hypernymy relationship with other verbs. In this hierarchical relationship, to limp is a \"son\" of to walk which in its turn has the archetypal pure concept of verb of movement as its farthest ancestor. This index is the number of different archetypal verbs expressed in the text and is indicative of the variety of the language. Different types of verbs per sentence The number of distinct archetypal verbs (see the index above) is computed for each sentence, resulting in the mean value. Named entities per sentence This index is based on the API named entity recognition (NER) capability. Unknown concepts per sentence An unknown concept is a word that's not mapped to a concept in the expert.ai Knowledge Graph. Function words per sentence Function words have little or no lexical meaning, but help create fluent and more readable sentences. Commonly misspelled words per sentence This index is based on the most common writing errors. Most common words per sentence This index is based on a list of the most common writing terms. Language for specific purposes The following indexes count the presence of terms associated with language for specific purposes. Index name Academic language words per sentence Business language words per sentence Crime language words per sentence Layman language words per sentence Legal language words per sentence Military language words per sentence Political language words per sentence Social media language words per sentence Values Mean, standard deviation and absolute mean deviation are returned for all the \"per sentence\" indexes. For all but Token length per sentence , the total number of occurrences in the entire text of the document is also returned. For indexes that are simple counters (ex. Sentences ), the total elements count in the text is returned. Useful resources How to request information detection API resources . How to interpret the output of the writeprint detector.","title":"Writeprint"},{"location":"guide/detection/writeprint/#writeprint-detection","text":"","title":"Writeprint detection"},{"location":"guide/detection/writeprint/#introduction","text":"The writeprint information detection resources\u2014one for each of the supported languages\u2014 perform a stylometric analysis of a text, ranging from readability and vocabulary richness to verb types, document structure and grammar. The detector is also capable of identifying the markers of several specific-purpose languages. Stylometric data is provided in the form of 60 indexes (see below) which, as a whole, make up for a complete fingerprint of the document, i.e. its writeprint . By comparing a number of documents on the basis of their writeprint, the literary attributes of the author are highlighted by this authorship analysis tool. Writeprint information is returned as a JSON-LD object embedded in a broader JSON object.","title":"Introduction"},{"location":"guide/detection/writeprint/#readability-indexes","text":"These are the readability indexes: Index name Description Output data Coleman-Liau The Coleman-Liau index , which value approximates the U.S. grade level thought necessary to comprehend the text. Value and degree of difficulty Gulpease The Gulpease index , based on word length and best suited for the Italian language. Value and degree of difficulty Automated Readability Index (ARI) The Automated Readability Index , which, like the Coleman-Liau index, produces an approximate representation of the US grade level needed to comprehend the text and is best suited for the English language. Value and degree of difficulty","title":"Readability indexes"},{"location":"guide/detection/writeprint/#spelling","text":"The following indexes measure aspects related to the spelling of words and the presence of particular punctuation characters. Index name Notes Sentences starting with a capital letter (ratio) The ratio of sentences in which the first word starts with an uppercase letter to the number of sentences. Sentences starting with a small letter (ratio) The ratio of sentences in which the first word starts with a lowercase letter to the number of sentences. Emoticons per sentence Dots per sentence The presence of dots in addition to the period at the end of the sentence can be indicative of a concise language because of abbreviations. Multiple dots per sentence such as Ellipsis points and longer sequences. Question marks per sentence Indicative of the ratio of questions to the total number of sentences. Multiple question marks per sentence Exclamation marks per sentence Multiple exclamation marks per sentence Exclamation mark, question mark sequences per sentence Commas per sentence Colons per sentence Semicolons per sentence Single quotation marks per sentence Double quotation marks per sentence","title":"Spelling"},{"location":"guide/detection/writeprint/#text-subdivision","text":"The following indexes count the occurrences or measure the length of certain subdivisions of the text, from sentences to characters. Index name Notes Sentences Tokens Words are tokens, but consecutive words recognized as a unit\u2014like credit card or red carpet \u2014and punctuation marks are also tokens. Token length per sentence Characters per sentence Atoms per sentence Words and punctuation marks are both tokens (see above) and atoms, except in the case of consecutive words recognized as a unit. In that case, the constituent words are atoms, while the multi-word unit is a single token. Tokens per sentence Phrases per sentence","title":"Text subdivision"},{"location":"guide/detection/writeprint/#grammar","text":"The following indexes count the occurrences per sentence of the different parts of speech. Index name Adjectives per sentence Adverbs per sentence Articles per sentence Auxiliaries per sentence Conjunctions per sentence Nouns per sentence Proper nouns per sentence Punctuation per sentence Prepositions per sentence Pronouns per sentence Particles per sentence Verbs per sentence","title":"Grammar"},{"location":"guide/detection/writeprint/#phrase-types","text":"The indexes that follow count the number of different phrases per sentence. Index name Adjective phrases per sentence Conjunction phrases per sentence Adverb phrases per sentence Noun phrases per sentence Nominal predicates per sentence Preposition phrases per sentence Relative phrases per sentence Verb phrases per sentence","title":"Phrase types"},{"location":"guide/detection/writeprint/#language-variety-and-errors","text":"The following indexes measure various aspects of the text that are indicative of greater or lesser variety of the language and the presence of the most common errors. Index name Notes Different types of verbs This index is related to the meaning of verbs and their hypernymy relationship with other verbs. In this hierarchical relationship, to limp is a \"son\" of to walk which in its turn has the archetypal pure concept of verb of movement as its farthest ancestor. This index is the number of different archetypal verbs expressed in the text and is indicative of the variety of the language. Different types of verbs per sentence The number of distinct archetypal verbs (see the index above) is computed for each sentence, resulting in the mean value. Named entities per sentence This index is based on the API named entity recognition (NER) capability. Unknown concepts per sentence An unknown concept is a word that's not mapped to a concept in the expert.ai Knowledge Graph. Function words per sentence Function words have little or no lexical meaning, but help create fluent and more readable sentences. Commonly misspelled words per sentence This index is based on the most common writing errors. Most common words per sentence This index is based on a list of the most common writing terms.","title":"Language variety and errors"},{"location":"guide/detection/writeprint/#language-for-specific-purposes","text":"The following indexes count the presence of terms associated with language for specific purposes. Index name Academic language words per sentence Business language words per sentence Crime language words per sentence Layman language words per sentence Legal language words per sentence Military language words per sentence Political language words per sentence Social media language words per sentence","title":"Language for specific purposes"},{"location":"guide/detection/writeprint/#values","text":"Mean, standard deviation and absolute mean deviation are returned for all the \"per sentence\" indexes. For all but Token length per sentence , the total number of occurrences in the entire text of the document is also returned. For indexes that are simple counters (ex. Sentences ), the total elements count in the text is returned.","title":"Values"},{"location":"guide/detection/writeprint/#useful-resources","text":"How to request information detection API resources . How to interpret the output of the writeprint detector.","title":"Useful resources"},{"location":"guide/entity-recognition/","text":"Named entity recognition Named entity recognition is a type of document analysis. It determines which entities\u2014persons, places, organizations, dates, addresses, etc.\u2014are mentioned in a text and the attributes of the entity that can be inferred by semantic analysis. Named entity recognition also performs knowledge linking: Knowledge Graph information and open data\u2014Wikidata, DBpedia and GeoNames references\u2014are returned for entities corresponding to syncons of the expert.ai Knowledge Graph. In the case of actual places, geographic coordinates are also provided. Entities are also recognized in pronouns and shorter forms that refer to named mentions. This kind of by reference recognition is anaphoric because entities are recognized through anaphoras . For example in this text: Michael Jordan was one of the best basketball players of all time. Scoring was Jordan 's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half. three mentions of Michael Jordan are recognized: the full named mention: Michael Jordan the anaphoras\u2014 Jordan and he \u2014for which Michael Jordan is considered the antecedent. Full analysis includes named entities recognition, but if you are not interested in the other analyses, you can use specific resources having paths like this: analyze / context name / language code / entities Boxed parts are placeholders, so for example: https://nlapi.expert.ai/v2/analyze/standard/en/entities is the URL of the standard context resource performing named entity recognition on an English text. These resources must be requested with the POST method. In the reference section of this manual you will find all the information you need to perform named entity recognition using the API's RESTful interface, specifically: The format of the request to be submitted to the resources. How to build resources' paths and full endpoints . The output format . Note Even if you use the API through a client that hides the REST interface, whether it is made by you or is one of those provided by expert.ai , knowing the output format helps you understand and navigate the results. Here is an example of performing named entity recognition on a short English test: Python This example is based on the Python client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the list of entities with their type. from expertai.nlapi.cloud.client import ExpertAiClient client = ExpertAiClient () text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" language = 'en' output = client . specific_resource_analysis ( body = { \"document\" : { \"text\" : text }}, params = { 'language' : language , 'resource' : 'entities' }) print ( f ' { \"ENTITY\" :{ 50 }} { \"TYPE\" :{ 10 }} ' ) print ( f ' { \"------\" :{ 50 }} { \"----\" :{ 10 }} ' ) for entity in output . entities : print ( f ' { entity . lemma :{ 50 }} { entity . type_ :{ 10 }} ' ) Java This example is based on the Java client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the JSON response and the list of entities with their type. import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.cloud.Analyzer ; import ai.expert.nlapi.v2.cloud.AnalyzerConfig ; import ai.expert.nlapi.v2.message.AnalyzeResponse ; import ai.expert.nlapi.v2.model.AnalyzeDocument ; public class Main { public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static Analyzer createAnalyzer () throws Exception { return new Analyzer ( AnalyzerConfig . builder () . withVersion ( API . Versions . V2 ) . withContext ( \"standard\" ) . withLanguage ( API . Languages . en ) . withAuthentication ( createAuthentication ()) . build ()); } public static void main ( String [] args ) { try { String text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" ; Analyzer analyzer = createAnalyzer (); AnalyzeResponse entities = analyzer . entities ( text ); // Output JSON representation System . out . println ( \"JSON representation:\" ); entities . prettyPrint (); // Tab separated list of entitites' lemma and type. System . out . println ( \"Tab separated list of entities' lemma and type:\" ); AnalyzeDocument data = entities . getData (); data . getEntities (). stream (). forEach ( c -> System . out . println ( c . getLemma () + \"\\t\" + c . getType ())); } catch ( Exception ex ) { ex . printStackTrace (); } } } curl (Linux) The following curl command posts a document to the named entity recognition resource of the API's REST interface. Run the command from a shell after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en/entities \\ -H 'Authorization: Bearer token' \\ -H 'Content-Type: application/json; charset=utf-8' \\ -d '{ \"document\": { \"text\": \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan' \\' 's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" } }' The server returns a JSON object . curl (Windows) The following curl command posts a document to the named entity recognition resource of the API's REST interface. Open a command prompt in the folder where you installed curl and run the command after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en/entities -H \"Authorization: Bearer token\" -H \"Content-Type: application/json; charset=utf-8\" -d \"{\\\" document\\ \": {\\\" text\\ \": \\\" Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\\ \"}}\" The server returns a JSON object .","title":"Named entity recognition"},{"location":"guide/entity-recognition/#named-entity-recognition","text":"Named entity recognition is a type of document analysis. It determines which entities\u2014persons, places, organizations, dates, addresses, etc.\u2014are mentioned in a text and the attributes of the entity that can be inferred by semantic analysis. Named entity recognition also performs knowledge linking: Knowledge Graph information and open data\u2014Wikidata, DBpedia and GeoNames references\u2014are returned for entities corresponding to syncons of the expert.ai Knowledge Graph. In the case of actual places, geographic coordinates are also provided. Entities are also recognized in pronouns and shorter forms that refer to named mentions. This kind of by reference recognition is anaphoric because entities are recognized through anaphoras . For example in this text: Michael Jordan was one of the best basketball players of all time. Scoring was Jordan 's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half. three mentions of Michael Jordan are recognized: the full named mention: Michael Jordan the anaphoras\u2014 Jordan and he \u2014for which Michael Jordan is considered the antecedent. Full analysis includes named entities recognition, but if you are not interested in the other analyses, you can use specific resources having paths like this: analyze / context name / language code / entities Boxed parts are placeholders, so for example: https://nlapi.expert.ai/v2/analyze/standard/en/entities is the URL of the standard context resource performing named entity recognition on an English text. These resources must be requested with the POST method. In the reference section of this manual you will find all the information you need to perform named entity recognition using the API's RESTful interface, specifically: The format of the request to be submitted to the resources. How to build resources' paths and full endpoints . The output format . Note Even if you use the API through a client that hides the REST interface, whether it is made by you or is one of those provided by expert.ai , knowing the output format helps you understand and navigate the results. Here is an example of performing named entity recognition on a short English test: Python This example is based on the Python client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the list of entities with their type. from expertai.nlapi.cloud.client import ExpertAiClient client = ExpertAiClient () text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" language = 'en' output = client . specific_resource_analysis ( body = { \"document\" : { \"text\" : text }}, params = { 'language' : language , 'resource' : 'entities' }) print ( f ' { \"ENTITY\" :{ 50 }} { \"TYPE\" :{ 10 }} ' ) print ( f ' { \"------\" :{ 50 }} { \"----\" :{ 10 }} ' ) for entity in output . entities : print ( f ' { entity . lemma :{ 50 }} { entity . type_ :{ 10 }} ' ) Java This example is based on the Java client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the JSON response and the list of entities with their type. import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.cloud.Analyzer ; import ai.expert.nlapi.v2.cloud.AnalyzerConfig ; import ai.expert.nlapi.v2.message.AnalyzeResponse ; import ai.expert.nlapi.v2.model.AnalyzeDocument ; public class Main { public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static Analyzer createAnalyzer () throws Exception { return new Analyzer ( AnalyzerConfig . builder () . withVersion ( API . Versions . V2 ) . withContext ( \"standard\" ) . withLanguage ( API . Languages . en ) . withAuthentication ( createAuthentication ()) . build ()); } public static void main ( String [] args ) { try { String text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" ; Analyzer analyzer = createAnalyzer (); AnalyzeResponse entities = analyzer . entities ( text ); // Output JSON representation System . out . println ( \"JSON representation:\" ); entities . prettyPrint (); // Tab separated list of entitites' lemma and type. System . out . println ( \"Tab separated list of entities' lemma and type:\" ); AnalyzeDocument data = entities . getData (); data . getEntities (). stream (). forEach ( c -> System . out . println ( c . getLemma () + \"\\t\" + c . getType ())); } catch ( Exception ex ) { ex . printStackTrace (); } } } curl (Linux) The following curl command posts a document to the named entity recognition resource of the API's REST interface. Run the command from a shell after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en/entities \\ -H 'Authorization: Bearer token' \\ -H 'Content-Type: application/json; charset=utf-8' \\ -d '{ \"document\": { \"text\": \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan' \\' 's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" } }' The server returns a JSON object . curl (Windows) The following curl command posts a document to the named entity recognition resource of the API's REST interface. Open a command prompt in the folder where you installed curl and run the command after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en/entities -H \"Authorization: Bearer token\" -H \"Content-Type: application/json; charset=utf-8\" -d \"{\\\" document\\ \": {\\\" text\\ \": \\\" Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\\ \"}}\" The server returns a JSON object .","title":"Named entity recognition"},{"location":"guide/full-analysis/","text":"Full analysis Full analysis is the sum of all partial document analyses: Deep linguistic analysis Keyphrase extraction Named entity recognition Relation extraction Sentiment analysis Natural Language API resources carrying out full document analysis have paths like this: analyze / context name / language code Boxed parts are placeholders, so for example: https://nlapi.expert.ai/v2/analyze/standard/en is the URL of the standard context resource performing the full analysis of an English text. These resources must be requested with the POST method. In the reference section of this manual you will find all the information you need to perform full document analysis using the API's RESTful interface, specifically: The format of the request to be submitted to the resources. How to build resources' paths and full endpoints . The output format . Note Even if you use the API through a client that hides the REST interface, whether it is made by you or is one of those provided by expert.ai , knowing the output format helps you understand and navigate the results. Here is an example of performing full document analysis on a short English text: Python This example is based on the Python client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the number of items for each of the output's arrays. from expertai.nlapi.cloud.client import ExpertAiClient client = ExpertAiClient () text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.'\" language = 'en' output = client . full_analysis ( body = { \"document\" : { \"text\" : text }}, params = { 'language' : language }) # Output arrays size print ( \"Output arrays size:\" ); print ( \"knowledge: \" , len ( output . knowledge )) print ( \"paragraphs: \" , len ( output . paragraphs )) print ( \"sentences: \" , len ( output . sentences )) print ( \"phrases: \" , len ( output . phrases )) print ( \"tokens: \" , len ( output . tokens )) print ( \"mainSentences: \" , len ( output . main_sentences )) print ( \"mainPhrases: \" , len ( output . main_phrases )) print ( \"mainLemmas: \" , len ( output . main_lemmas )) print ( \"mainSyncons: \" , len ( output . main_syncons )) print ( \"topics: \" , len ( output . topics )) print ( \"entities: \" , len ( output . entities )) print ( \"entities: \" , len ( output . relations )) print ( \"sentiment.items: \" , len ( output . sentiment . items )) Java This example is based on the Java client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the number of items for each of the output's arrays. import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.cloud.Analyzer ; import ai.expert.nlapi.v2.cloud.AnalyzerConfig ; import ai.expert.nlapi.v2.message.AnalyzeResponse ; import ai.expert.nlapi.v2.model.AnalyzeDocument ; public class Main { public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static Analyzer createAnalyzer () throws Exception { return new Analyzer ( AnalyzerConfig . builder () . withVersion ( API . Versions . V2 ) . withContext ( \"standard\" ) . withLanguage ( API . Languages . en ) . withAuthentication ( createAuthentication ()) . build ()); } public static void main ( String [] args ) { try { String text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" ; Analyzer analyzer = createAnalyzer (); AnalyzeResponse analysis = analyzer . analyze ( text ); // Output JSON representation System . out . println ( \"JSON representation:\" ); analysis . prettyPrint (); // Output arrays size System . out . println ( \"Output arrays size:\" ); AnalyzeDocument data = analysis . getData (); System . out . println ( \"knowledge: \" + data . getKnowledge (). size ()); System . out . println ( \"paragraphs: \" + data . getParagraphs (). size ()); System . out . println ( \"sentences: \" + data . getSentences (). size ()); System . out . println ( \"phrases: \" + data . getPhrases (). size ()); System . out . println ( \"tokens: \" + data . getTokens (). size ()); System . out . println ( \"mainSentences: \" + data . getMainSentences (). size ()); System . out . println ( \"mainPhrases: \" + data . getMainPhrases (). size ()); System . out . println ( \"mainLemmas: \" + data . getMainLemmas (). size ()); System . out . println ( \"mainSyncons: \" + data . getMainSyncons (). size ()); System . out . println ( \"topics: \" + data . getTopics (). size ()); System . out . println ( \"entities: \" + data . getEntities (). size ()); System . out . println ( \"relations: \" + data . getRelations (). size ()); System . out . println ( \"sentiment/items: \" + data . getSentiment (). getItems (). size ()); } catch ( Exception ex ) { ex . printStackTrace (); } } } curl (Linux) The following curl command posts a document to the full document analysis resource of the API's REST interface. Run the command from a shell after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en \\ -H 'Authorization: Bearer token' \\ -H 'Content-Type: application/json; charset=utf-8' \\ -d '{ \"document\": { \"text\": \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan' \\' 's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" } }' The server returns a JSON object . curl (Windows) The following curl command posts a document to the full document analysis resource of the API's REST interface. Open a command prompt in the folder where you installed curl and run the command after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en -H \"Authorization: Bearer token\" -H \"Content-Type: application/json; charset=utf-8\" -d \"{\\\" document\\ \": {\\\" text\\ \": \\\" Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\\ \"}}\" The server returns a JSON object .","title":"Full analysis"},{"location":"guide/full-analysis/#full-analysis","text":"Full analysis is the sum of all partial document analyses: Deep linguistic analysis Keyphrase extraction Named entity recognition Relation extraction Sentiment analysis Natural Language API resources carrying out full document analysis have paths like this: analyze / context name / language code Boxed parts are placeholders, so for example: https://nlapi.expert.ai/v2/analyze/standard/en is the URL of the standard context resource performing the full analysis of an English text. These resources must be requested with the POST method. In the reference section of this manual you will find all the information you need to perform full document analysis using the API's RESTful interface, specifically: The format of the request to be submitted to the resources. How to build resources' paths and full endpoints . The output format . Note Even if you use the API through a client that hides the REST interface, whether it is made by you or is one of those provided by expert.ai , knowing the output format helps you understand and navigate the results. Here is an example of performing full document analysis on a short English text: Python This example is based on the Python client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the number of items for each of the output's arrays. from expertai.nlapi.cloud.client import ExpertAiClient client = ExpertAiClient () text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.'\" language = 'en' output = client . full_analysis ( body = { \"document\" : { \"text\" : text }}, params = { 'language' : language }) # Output arrays size print ( \"Output arrays size:\" ); print ( \"knowledge: \" , len ( output . knowledge )) print ( \"paragraphs: \" , len ( output . paragraphs )) print ( \"sentences: \" , len ( output . sentences )) print ( \"phrases: \" , len ( output . phrases )) print ( \"tokens: \" , len ( output . tokens )) print ( \"mainSentences: \" , len ( output . main_sentences )) print ( \"mainPhrases: \" , len ( output . main_phrases )) print ( \"mainLemmas: \" , len ( output . main_lemmas )) print ( \"mainSyncons: \" , len ( output . main_syncons )) print ( \"topics: \" , len ( output . topics )) print ( \"entities: \" , len ( output . entities )) print ( \"entities: \" , len ( output . relations )) print ( \"sentiment.items: \" , len ( output . sentiment . items )) Java This example is based on the Java client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the number of items for each of the output's arrays. import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.cloud.Analyzer ; import ai.expert.nlapi.v2.cloud.AnalyzerConfig ; import ai.expert.nlapi.v2.message.AnalyzeResponse ; import ai.expert.nlapi.v2.model.AnalyzeDocument ; public class Main { public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static Analyzer createAnalyzer () throws Exception { return new Analyzer ( AnalyzerConfig . builder () . withVersion ( API . Versions . V2 ) . withContext ( \"standard\" ) . withLanguage ( API . Languages . en ) . withAuthentication ( createAuthentication ()) . build ()); } public static void main ( String [] args ) { try { String text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" ; Analyzer analyzer = createAnalyzer (); AnalyzeResponse analysis = analyzer . analyze ( text ); // Output JSON representation System . out . println ( \"JSON representation:\" ); analysis . prettyPrint (); // Output arrays size System . out . println ( \"Output arrays size:\" ); AnalyzeDocument data = analysis . getData (); System . out . println ( \"knowledge: \" + data . getKnowledge (). size ()); System . out . println ( \"paragraphs: \" + data . getParagraphs (). size ()); System . out . println ( \"sentences: \" + data . getSentences (). size ()); System . out . println ( \"phrases: \" + data . getPhrases (). size ()); System . out . println ( \"tokens: \" + data . getTokens (). size ()); System . out . println ( \"mainSentences: \" + data . getMainSentences (). size ()); System . out . println ( \"mainPhrases: \" + data . getMainPhrases (). size ()); System . out . println ( \"mainLemmas: \" + data . getMainLemmas (). size ()); System . out . println ( \"mainSyncons: \" + data . getMainSyncons (). size ()); System . out . println ( \"topics: \" + data . getTopics (). size ()); System . out . println ( \"entities: \" + data . getEntities (). size ()); System . out . println ( \"relations: \" + data . getRelations (). size ()); System . out . println ( \"sentiment/items: \" + data . getSentiment (). getItems (). size ()); } catch ( Exception ex ) { ex . printStackTrace (); } } } curl (Linux) The following curl command posts a document to the full document analysis resource of the API's REST interface. Run the command from a shell after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en \\ -H 'Authorization: Bearer token' \\ -H 'Content-Type: application/json; charset=utf-8' \\ -d '{ \"document\": { \"text\": \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan' \\' 's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" } }' The server returns a JSON object . curl (Windows) The following curl command posts a document to the full document analysis resource of the API's REST interface. Open a command prompt in the folder where you installed curl and run the command after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en -H \"Authorization: Bearer token\" -H \"Content-Type: application/json; charset=utf-8\" -d \"{\\\" document\\ \": {\\\" text\\ \": \\\" Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\\ \"}}\" The server returns a JSON object .","title":"Full analysis"},{"location":"guide/keyphrase-extraction/","text":"Keyphrase extraction Keyphrase extraction is a type of document analysis that determines the relevant elements of a text: Relevant topics Main sentences Main phrases Main concepts Main lemmas Main concepts are returned as Knowledge Graph \"syncons\" and enriched through knowledge linking: open data\u2014Wikidata, DBpedia and GeoNames references\u2014are returned. In the case of actual places, geographic coordinates are also provided. Relevant topics are chosen from the Knowledge Graph. You can find a list of relevant topics in the reference section . Full analysis includes keyphrase extraction, but if you are not interested in the other analyses, you can use specific resources having paths like this: analyze / context name / language code / relevants Boxed parts are placeholders, so for example: https://nlapi.expert.ai/v2/analyze/standard/en/relevants is the URL of the standard context resource performing keyphrase extraction on an English text. These resources must be requested with the POST method. In the reference section of this manual you will find all the information you need to perform keyphrase extraction using the API's RESTful interface, specifically: The format of the request to be submitted to the resources. How to build resources' paths and full endpoints . The output format . Note Even if you use the API through a client that hides the REST interface, whether it is made by you or is one of those provided by expert.ai , knowing the output format helps you understand and navigate the results. Here is an example of performing keyphrase extraction on an English newspaper article: Python This example is based on the Python client you can find on GitHub . In the example, the following text was added in a file named document.txt . At first there was considerable trouble about getting the machine up in the air and the engine well up to speed. They did this by running along a single-rail track perhaps 200 feet long. It was also, in the early experiments, found advisable to run against the wind, because they could then have a greater time to practice in the air and not get so far away from the building where it was stored. Since they can come around to the starting-point, however, they can start with the wind even behind them; and with a strong wind behind it is an easy matter to make even more than a mile a minute. The operator takes his place lying flat on his face. This position offers less resistance to the wind. The engine is started and got up to speed. The machine is held until ready to start by a sort of trap to be sprung when all is ready; then with a tremendous flapping and snapping of the four-cylinder engine, the huge machine springs aloft. When it first turned that circle, and came near the starting-point. I was right in front it; and I said then, and I believe still, it was one of the grandest sights, if not the grandest sight, of my life. Imagine a locomotive that has left its track, and is climbing up in the air right toward you\u2014a locomotive without any wheels, we will say, but with white wings instead, we will further say-a locomotive made of aluminum. Well, now, imagine this white locomotive, with wings that spread 20 feet each way, coming right toward you with a tremendous flap of its propellers, and you will have something like what I saw. The younger brother bade me move to one side for fear it might come down suddenly; but I tell you, friends, the sensation that one feels in such a crisis is something hard to describe. The attendant at one time, when the rope came off that started it, said he was shaking from head to foot as if he had a fit of ague. His shaking was uncalled for, however, for the intrepid manager succeeded in righting up his craft, and she made one of her very best flights. I may add, however, that the apparatus is secured by patents, both in this and in foreign countries; and as nobody else has as yet succeeded in doing any thing like what they have done I hope no millionaire or syndicate will try to rob them of the invention or laurels they have so fairly and honestly earned. When Columbus discovered America he did not know what the outcome would be, and no one at that time knew; and I doubt if the wildest enthusiast caught a glimpse of what really did come from his discovery. In a like manner these two brothers have probably not even a faint glimpse of what their discovery is going to bring to the children of men. No one living can give a guess of what is coming along this line, much better than any one living could conjecture the final outcome of Columbus' experiment when lie pushed off through the trackless waters. Possibly we may be able to fly over the north pole, even if we should not succeed in tacking the \"stars and stripes\" to its uppermost end. The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the list of main lemmas. from expertai.nlapi.cloud.client import ExpertAiClient client = ExpertAiClient () file = open ( \"document.txt\" ) text = file . read () file . close () language = 'en' output = client . specific_resource_analysis ( body = { \"document\" : { \"text\" : text }}, params = { 'language' : language , 'resource' : 'relevants' }) # Main lemmas print ( \"Main lemmas:\" ) for lemma in output . main_lemmas : print ( lemma . value ) Java This example is based on the Java client you can find on GitHub . In the example, the following text was added in a file named document.txt . At first there was considerable trouble about getting the machine up in the air and the engine well up to speed. They did this by running along a single-rail track perhaps 200 feet long. It was also, in the early experiments, found advisable to run against the wind, because they could then have a greater time to practice in the air and not get so far away from the building where it was stored. Since they can come around to the starting-point, however, they can start with the wind even behind them; and with a strong wind behind it is an easy matter to make even more than a mile a minute. The operator takes his place lying flat on his face. This position offers less resistance to the wind. The engine is started and got up to speed. The machine is held until ready to start by a sort of trap to be sprung when all is ready; then with a tremendous flapping and snapping of the four-cylinder engine, the huge machine springs aloft. When it first turned that circle, and came near the starting-point. I was right in front it; and I said then, and I believe still, it was one of the grandest sights, if not the grandest sight, of my life. Imagine a locomotive that has left its track, and is climbing up in the air right toward you\u2014a locomotive without any wheels, we will say, but with white wings instead, we will further say-a locomotive made of aluminum. Well, now, imagine this white locomotive, with wings that spread 20 feet each way, coming right toward you with a tremendous flap of its propellers, and you will have something like what I saw. The younger brother bade me move to one side for fear it might come down suddenly; but I tell you, friends, the sensation that one feels in such a crisis is something hard to describe. The attendant at one time, when the rope came off that started it, said he was shaking from head to foot as if he had a fit of ague. His shaking was uncalled for, however, for the intrepid manager succeeded in righting up his craft, and she made one of her very best flights. I may add, however, that the apparatus is secured by patents, both in this and in foreign countries; and as nobody else has as yet succeeded in doing any thing like what they have done I hope no millionaire or syndicate will try to rob them of the invention or laurels they have so fairly and honestly earned. When Columbus discovered America he did not know what the outcome would be, and no one at that time knew; and I doubt if the wildest enthusiast caught a glimpse of what really did come from his discovery. In a like manner these two brothers have probably not even a faint glimpse of what their discovery is going to bring to the children of men. No one living can give a guess of what is coming along this line, much better than any one living could conjecture the final outcome of Columbus' experiment when lie pushed off through the trackless waters. Possibly we may be able to fly over the north pole, even if we should not succeed in tacking the \"stars and stripes\" to its uppermost end. The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the JSON response and the list of main lemmas. import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.cloud.Analyzer ; import ai.expert.nlapi.v2.cloud.AnalyzerConfig ; import ai.expert.nlapi.v2.message.AnalyzeResponse ; import ai.expert.nlapi.v2.model.AnalyzeDocument ; import java.io.IOException ; import java.nio.file.Files ; import java.nio.file.Path ; import java.nio.file.Paths ; import java.util.stream.Collectors ; public class Main { public static String loadText () { try { Path path = Paths . get ( \"document.txt\" ); return Files . lines ( path ). collect ( Collectors . joining ( \"\\r\\n\" )); } catch ( IOException e ) { return null ; } } public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static Analyzer createAnalyzer () throws Exception { return new Analyzer ( AnalyzerConfig . builder () . withVersion ( API . Versions . V2 ) . withContext ( \"standard\" ) . withLanguage ( API . Languages . en ) . withAuthentication ( createAuthentication ()) . build ()); } public static void main ( String [] args ) { try { String text = loadText (); Analyzer analyzer = createAnalyzer (); AnalyzeResponse relevants = analyzer . relevants ( text ); // Output JSON representation System . out . println ( \"JSON representation:\" ); relevants . prettyPrint (); // Main lemmas System . out . println ( \"Main lemmas:\" ); AnalyzeDocument data = relevants . getData (); data . getMainLemmas (). stream (). forEach ( c -> System . out . println ( c . getValue ())); } catch ( Exception ex ) { ex . printStackTrace (); } } } curl (Linux) The following curl command posts a document to the keyphrase extraction resource of the API's REST interface. Run the command from a shell after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en/relevants \\ -H 'Authorization: Bearer token' \\ -H 'Content-Type: application/json; charset=utf-8' \\ -d '{ \"document\": { \"text\": \"At first there was considerable trouble about getting the machine up in the air and the engine well up to speed. They did this by running along a single-rail track perhaps 200 feet long. It was also, in the early experiments, found advisable to run against the wind, because they could then have a greater time to practice in the air and not get so far away from the building where it was stored. Since they can come around to the starting-point, however, they can start with the wind even behind them; and with a strong wind behind it is an easy matter to make even more than a mile a minute. The operator takes his place lying flat on his face. This position offers less resistance to the wind. The engine is started and got up to speed. The machine is held until ready to start by a sort of trap to be sprung when all is ready; then with a tremendous flapping and snapping of the four-cylinder engine, the huge machine springs aloft. When it first turned that circle, and came near the starting-point. I was right in front it; and I said then, and I believe still, it was one of the grandest sights, if not the grandest sight, of my life. Imagine a locomotive that has left its track, and is climbing up in the air right toward you\u2014a locomotive without any wheels, we will say, but with white wings instead, we will further say-a locomotive made of aluminum. Well, now, imagine this white locomotive, with wings that spread 20 feet each way, coming right toward you with a tremendous flap of its propellers, and you will have something like what I saw. The younger brother bade me move to one side for fear it might come down suddenly; but I tell you, friends, the sensation that one feels in such a crisis is something hard to describe. The attendant at one time, when the rope came off that started it, said he was shaking from head to foot as if he had a fit of ague. His shaking was uncalled for, however, for the intrepid manager succeeded in righting up his craft, and she made one of her very best flights. I may add, however, that the apparatus is secured by patents, both in this and in foreign countries; and as nobody else has as yet succeeded in doing any thing like what they have done I hope no millionaire or syndicate will try to rob them of the invention or laurels they have so fairly and honestly earned. When Columbus discovered America he did not know what the outcome would be, and no one at that time knew; and I doubt if the wildest enthusiast caught a glimpse of what really did come from his discovery. In a like manner these two brothers have probably not even a faint glimpse of what their discovery is going to bring to the children of men. No one living can give a guess of what is coming along this line, much better than any one living could conjecture the final outcome of Columbus' \\' ' experiment when lie pushed off through the trackless waters. Possibly we may be able to fly over the north pole, even if we should not succeed in tacking the \\\"stars and stripes\\\" to its uppermost end.\" } }' The server returns a JSON object . curl (Windows) The following curl command posts a document to the keyphrase extraction resource of the API's REST interface. Open a command prompt in the folder where you installed curl and run the command after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en/relevants -H \"Authorization: Bearer token\" -H \"Content-Type: application/json; charset=utf-8\" -d \"{\\\" document\\ \": {\\\" text\\ \": \\\" At first there was considerable trouble about getting the machine up in the air and the engine well up to speed. They did this by running along a single-rail track perhaps 200 feet long. It was also, in the early experiments, found advisable to run against the wind, because they could then have a greater time to practice in the air and not get so far away from the building where it was stored. Since they can come around to the starting-point, however, they can start with the wind even behind them; and with a strong wind behind it is an easy matter to make even more than a mile a minute. The operator takes his place lying flat on his face. This position offers less resistance to the wind. The engine is started and got up to speed. The machine is held until ready to start by a sort of trap to be sprung when all is ready; then with a tremendous flapping and snapping of the four-cylinder engine, the huge machine springs aloft. When it first turned that circle, and came near the starting-point. I was right in front it; and I said then, and I believe still, it was one of the grandest sights, if not the grandest sight, of my life. Imagine a locomotive that has left its track, and is climbing up in the air right toward you\u2014a locomotive without any wheels, we will say, but with white wings instead, we will further say-a locomotive made of aluminum. Well, now, imagine this white locomotive, with wings that spread 20 feet each way, coming right toward you with a tremendous flap of its propellers, and you will have something like what I saw. The younger brother bade me move to one side for fear it might come down suddenly; but I tell you, friends, the sensation that one feels in such a crisis is something hard to describe. The attendant at one time, when the rope came off that started it, said he was shaking from head to foot as if he had a fit of ague. His shaking was uncalled for, however, for the intrepid manager succeeded in righting up his craft, and she made one of her very best flights. I may add, however, that the apparatus is secured by patents, both in this and in foreign countries; and as nobody else has as yet succeeded in doing any thing like what they have done I hope no millionaire or syndicate will try to rob them of the invention or laurels they have so fairly and honestly earned. When Columbus discovered America he did not know what the outcome would be, and no one at that time knew; and I doubt if the wildest enthusiast caught a glimpse of what really did come from his discovery. In a like manner these two brothers have probably not even a faint glimpse of what their discovery is going to bring to the children of men. No one living can give a guess of what is coming along this line, much better than any one living could conjecture the final outcome of Columbus' experiment when lie pushed off through the trackless waters. Possibly we may be able to fly over the north pole, even if we should not succeed in tacking the \\\\\\ \"stars and stripes\\\\\\\" to its uppermost end.\\ \"}}\" The server returns a JSON object .","title":"Keyphrase extraction"},{"location":"guide/keyphrase-extraction/#keyphrase-extraction","text":"Keyphrase extraction is a type of document analysis that determines the relevant elements of a text: Relevant topics Main sentences Main phrases Main concepts Main lemmas Main concepts are returned as Knowledge Graph \"syncons\" and enriched through knowledge linking: open data\u2014Wikidata, DBpedia and GeoNames references\u2014are returned. In the case of actual places, geographic coordinates are also provided. Relevant topics are chosen from the Knowledge Graph. You can find a list of relevant topics in the reference section . Full analysis includes keyphrase extraction, but if you are not interested in the other analyses, you can use specific resources having paths like this: analyze / context name / language code / relevants Boxed parts are placeholders, so for example: https://nlapi.expert.ai/v2/analyze/standard/en/relevants is the URL of the standard context resource performing keyphrase extraction on an English text. These resources must be requested with the POST method. In the reference section of this manual you will find all the information you need to perform keyphrase extraction using the API's RESTful interface, specifically: The format of the request to be submitted to the resources. How to build resources' paths and full endpoints . The output format . Note Even if you use the API through a client that hides the REST interface, whether it is made by you or is one of those provided by expert.ai , knowing the output format helps you understand and navigate the results. Here is an example of performing keyphrase extraction on an English newspaper article: Python This example is based on the Python client you can find on GitHub . In the example, the following text was added in a file named document.txt . At first there was considerable trouble about getting the machine up in the air and the engine well up to speed. They did this by running along a single-rail track perhaps 200 feet long. It was also, in the early experiments, found advisable to run against the wind, because they could then have a greater time to practice in the air and not get so far away from the building where it was stored. Since they can come around to the starting-point, however, they can start with the wind even behind them; and with a strong wind behind it is an easy matter to make even more than a mile a minute. The operator takes his place lying flat on his face. This position offers less resistance to the wind. The engine is started and got up to speed. The machine is held until ready to start by a sort of trap to be sprung when all is ready; then with a tremendous flapping and snapping of the four-cylinder engine, the huge machine springs aloft. When it first turned that circle, and came near the starting-point. I was right in front it; and I said then, and I believe still, it was one of the grandest sights, if not the grandest sight, of my life. Imagine a locomotive that has left its track, and is climbing up in the air right toward you\u2014a locomotive without any wheels, we will say, but with white wings instead, we will further say-a locomotive made of aluminum. Well, now, imagine this white locomotive, with wings that spread 20 feet each way, coming right toward you with a tremendous flap of its propellers, and you will have something like what I saw. The younger brother bade me move to one side for fear it might come down suddenly; but I tell you, friends, the sensation that one feels in such a crisis is something hard to describe. The attendant at one time, when the rope came off that started it, said he was shaking from head to foot as if he had a fit of ague. His shaking was uncalled for, however, for the intrepid manager succeeded in righting up his craft, and she made one of her very best flights. I may add, however, that the apparatus is secured by patents, both in this and in foreign countries; and as nobody else has as yet succeeded in doing any thing like what they have done I hope no millionaire or syndicate will try to rob them of the invention or laurels they have so fairly and honestly earned. When Columbus discovered America he did not know what the outcome would be, and no one at that time knew; and I doubt if the wildest enthusiast caught a glimpse of what really did come from his discovery. In a like manner these two brothers have probably not even a faint glimpse of what their discovery is going to bring to the children of men. No one living can give a guess of what is coming along this line, much better than any one living could conjecture the final outcome of Columbus' experiment when lie pushed off through the trackless waters. Possibly we may be able to fly over the north pole, even if we should not succeed in tacking the \"stars and stripes\" to its uppermost end. The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the list of main lemmas. from expertai.nlapi.cloud.client import ExpertAiClient client = ExpertAiClient () file = open ( \"document.txt\" ) text = file . read () file . close () language = 'en' output = client . specific_resource_analysis ( body = { \"document\" : { \"text\" : text }}, params = { 'language' : language , 'resource' : 'relevants' }) # Main lemmas print ( \"Main lemmas:\" ) for lemma in output . main_lemmas : print ( lemma . value ) Java This example is based on the Java client you can find on GitHub . In the example, the following text was added in a file named document.txt . At first there was considerable trouble about getting the machine up in the air and the engine well up to speed. They did this by running along a single-rail track perhaps 200 feet long. It was also, in the early experiments, found advisable to run against the wind, because they could then have a greater time to practice in the air and not get so far away from the building where it was stored. Since they can come around to the starting-point, however, they can start with the wind even behind them; and with a strong wind behind it is an easy matter to make even more than a mile a minute. The operator takes his place lying flat on his face. This position offers less resistance to the wind. The engine is started and got up to speed. The machine is held until ready to start by a sort of trap to be sprung when all is ready; then with a tremendous flapping and snapping of the four-cylinder engine, the huge machine springs aloft. When it first turned that circle, and came near the starting-point. I was right in front it; and I said then, and I believe still, it was one of the grandest sights, if not the grandest sight, of my life. Imagine a locomotive that has left its track, and is climbing up in the air right toward you\u2014a locomotive without any wheels, we will say, but with white wings instead, we will further say-a locomotive made of aluminum. Well, now, imagine this white locomotive, with wings that spread 20 feet each way, coming right toward you with a tremendous flap of its propellers, and you will have something like what I saw. The younger brother bade me move to one side for fear it might come down suddenly; but I tell you, friends, the sensation that one feels in such a crisis is something hard to describe. The attendant at one time, when the rope came off that started it, said he was shaking from head to foot as if he had a fit of ague. His shaking was uncalled for, however, for the intrepid manager succeeded in righting up his craft, and she made one of her very best flights. I may add, however, that the apparatus is secured by patents, both in this and in foreign countries; and as nobody else has as yet succeeded in doing any thing like what they have done I hope no millionaire or syndicate will try to rob them of the invention or laurels they have so fairly and honestly earned. When Columbus discovered America he did not know what the outcome would be, and no one at that time knew; and I doubt if the wildest enthusiast caught a glimpse of what really did come from his discovery. In a like manner these two brothers have probably not even a faint glimpse of what their discovery is going to bring to the children of men. No one living can give a guess of what is coming along this line, much better than any one living could conjecture the final outcome of Columbus' experiment when lie pushed off through the trackless waters. Possibly we may be able to fly over the north pole, even if we should not succeed in tacking the \"stars and stripes\" to its uppermost end. The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the JSON response and the list of main lemmas. import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.cloud.Analyzer ; import ai.expert.nlapi.v2.cloud.AnalyzerConfig ; import ai.expert.nlapi.v2.message.AnalyzeResponse ; import ai.expert.nlapi.v2.model.AnalyzeDocument ; import java.io.IOException ; import java.nio.file.Files ; import java.nio.file.Path ; import java.nio.file.Paths ; import java.util.stream.Collectors ; public class Main { public static String loadText () { try { Path path = Paths . get ( \"document.txt\" ); return Files . lines ( path ). collect ( Collectors . joining ( \"\\r\\n\" )); } catch ( IOException e ) { return null ; } } public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static Analyzer createAnalyzer () throws Exception { return new Analyzer ( AnalyzerConfig . builder () . withVersion ( API . Versions . V2 ) . withContext ( \"standard\" ) . withLanguage ( API . Languages . en ) . withAuthentication ( createAuthentication ()) . build ()); } public static void main ( String [] args ) { try { String text = loadText (); Analyzer analyzer = createAnalyzer (); AnalyzeResponse relevants = analyzer . relevants ( text ); // Output JSON representation System . out . println ( \"JSON representation:\" ); relevants . prettyPrint (); // Main lemmas System . out . println ( \"Main lemmas:\" ); AnalyzeDocument data = relevants . getData (); data . getMainLemmas (). stream (). forEach ( c -> System . out . println ( c . getValue ())); } catch ( Exception ex ) { ex . printStackTrace (); } } } curl (Linux) The following curl command posts a document to the keyphrase extraction resource of the API's REST interface. Run the command from a shell after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en/relevants \\ -H 'Authorization: Bearer token' \\ -H 'Content-Type: application/json; charset=utf-8' \\ -d '{ \"document\": { \"text\": \"At first there was considerable trouble about getting the machine up in the air and the engine well up to speed. They did this by running along a single-rail track perhaps 200 feet long. It was also, in the early experiments, found advisable to run against the wind, because they could then have a greater time to practice in the air and not get so far away from the building where it was stored. Since they can come around to the starting-point, however, they can start with the wind even behind them; and with a strong wind behind it is an easy matter to make even more than a mile a minute. The operator takes his place lying flat on his face. This position offers less resistance to the wind. The engine is started and got up to speed. The machine is held until ready to start by a sort of trap to be sprung when all is ready; then with a tremendous flapping and snapping of the four-cylinder engine, the huge machine springs aloft. When it first turned that circle, and came near the starting-point. I was right in front it; and I said then, and I believe still, it was one of the grandest sights, if not the grandest sight, of my life. Imagine a locomotive that has left its track, and is climbing up in the air right toward you\u2014a locomotive without any wheels, we will say, but with white wings instead, we will further say-a locomotive made of aluminum. Well, now, imagine this white locomotive, with wings that spread 20 feet each way, coming right toward you with a tremendous flap of its propellers, and you will have something like what I saw. The younger brother bade me move to one side for fear it might come down suddenly; but I tell you, friends, the sensation that one feels in such a crisis is something hard to describe. The attendant at one time, when the rope came off that started it, said he was shaking from head to foot as if he had a fit of ague. His shaking was uncalled for, however, for the intrepid manager succeeded in righting up his craft, and she made one of her very best flights. I may add, however, that the apparatus is secured by patents, both in this and in foreign countries; and as nobody else has as yet succeeded in doing any thing like what they have done I hope no millionaire or syndicate will try to rob them of the invention or laurels they have so fairly and honestly earned. When Columbus discovered America he did not know what the outcome would be, and no one at that time knew; and I doubt if the wildest enthusiast caught a glimpse of what really did come from his discovery. In a like manner these two brothers have probably not even a faint glimpse of what their discovery is going to bring to the children of men. No one living can give a guess of what is coming along this line, much better than any one living could conjecture the final outcome of Columbus' \\' ' experiment when lie pushed off through the trackless waters. Possibly we may be able to fly over the north pole, even if we should not succeed in tacking the \\\"stars and stripes\\\" to its uppermost end.\" } }' The server returns a JSON object . curl (Windows) The following curl command posts a document to the keyphrase extraction resource of the API's REST interface. Open a command prompt in the folder where you installed curl and run the command after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en/relevants -H \"Authorization: Bearer token\" -H \"Content-Type: application/json; charset=utf-8\" -d \"{\\\" document\\ \": {\\\" text\\ \": \\\" At first there was considerable trouble about getting the machine up in the air and the engine well up to speed. They did this by running along a single-rail track perhaps 200 feet long. It was also, in the early experiments, found advisable to run against the wind, because they could then have a greater time to practice in the air and not get so far away from the building where it was stored. Since they can come around to the starting-point, however, they can start with the wind even behind them; and with a strong wind behind it is an easy matter to make even more than a mile a minute. The operator takes his place lying flat on his face. This position offers less resistance to the wind. The engine is started and got up to speed. The machine is held until ready to start by a sort of trap to be sprung when all is ready; then with a tremendous flapping and snapping of the four-cylinder engine, the huge machine springs aloft. When it first turned that circle, and came near the starting-point. I was right in front it; and I said then, and I believe still, it was one of the grandest sights, if not the grandest sight, of my life. Imagine a locomotive that has left its track, and is climbing up in the air right toward you\u2014a locomotive without any wheels, we will say, but with white wings instead, we will further say-a locomotive made of aluminum. Well, now, imagine this white locomotive, with wings that spread 20 feet each way, coming right toward you with a tremendous flap of its propellers, and you will have something like what I saw. The younger brother bade me move to one side for fear it might come down suddenly; but I tell you, friends, the sensation that one feels in such a crisis is something hard to describe. The attendant at one time, when the rope came off that started it, said he was shaking from head to foot as if he had a fit of ague. His shaking was uncalled for, however, for the intrepid manager succeeded in righting up his craft, and she made one of her very best flights. I may add, however, that the apparatus is secured by patents, both in this and in foreign countries; and as nobody else has as yet succeeded in doing any thing like what they have done I hope no millionaire or syndicate will try to rob them of the invention or laurels they have so fairly and honestly earned. When Columbus discovered America he did not know what the outcome would be, and no one at that time knew; and I doubt if the wildest enthusiast caught a glimpse of what really did come from his discovery. In a like manner these two brothers have probably not even a faint glimpse of what their discovery is going to bring to the children of men. No one living can give a guess of what is coming along this line, much better than any one living could conjecture the final outcome of Columbus' experiment when lie pushed off through the trackless waters. Possibly we may be able to fly over the north pole, even if we should not succeed in tacking the \\\\\\ \"stars and stripes\\\\\\\" to its uppermost end.\\ \"}}\" The server returns a JSON object .","title":"Keyphrase extraction"},{"location":"guide/linguistic-analysis/","text":"Deep linguistic analysis overview Deep linguistic analysis is a type of document analysis that combines the following interdependent processes: Text subdivision Part-of-speech tagging Morphological analysis Lemmatization Syntactic analysis Semantic analysis The analysis is \"deep\" because: It performs the common linguistic analysis. It disambiguates the terms of the text, i.e. it determines the exact meaning of the text after considering all the other possibilities. Deep linguistic analysis also performs knowledge linking: Knowledge Graph information and open data\u2014Wikidata, DBpedia and GeoNames references\u2014are returned for text tokens corresponding to syncons of the expert.ai Knowledge Graph. In the case of actual places, geographic coordinates are also provided. Full analysis includes deep linguistic analysis, but if you are not interested in the other analyses, you can use specific resources having paths like this: analyze / context name / language code / disambiguation Boxed parts are placeholders, so for example: https://nlapi.expert.ai/v2/analyze/standard/en/disambiguation is the URL of the standard context resource performing the deep linguistic analysis of an English text. These resources must be requested with the POST method. In the manual's reference section you will find all the information required to perform deep linguistic analysis using the API's Restful interface, specifically: The format of the request to be submitted to the resources. How to build resources' paths and full endpoints . The output format . Note Even if you use the API through a client that hides the REST interface, whether it is made by you or is one of those provided by expert.ai , knowing the output format helps you understand and navigate the results. Here is an example of performing deep linguistic analysis on a short English text: Python This example is based on the Python client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints a JSON representation of the results and the list of tokens' lemmas with their part-of-speech. from expertai.nlapi.cloud.client import ExpertAiClient client = ExpertAiClient () text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" language = 'en' output = client . specific_resource_analysis ( body = { \"document\" : { \"text\" : text }}, params = { 'language' : language , 'resource' : 'disambiguation' }) # Output tokens' data print ( \"Output tokens' data:\" ); print ( f ' { \"TEXT\" :{ 20 }} { \"LEMMA\" :{ 40 }} { \"POS\" :{ 6 }} ' ) print ( f ' { \"----\" :{ 20 }} { \"-----\" :{ 40 }} { \"---\" :{ 6 }} ' ) for token in output . tokens : print ( f ' { text [ token . start : token . end ] :{ 20 }} { token . lemma :{ 40 }} { token . pos :{ 6 }} ' ) Java This example is based on the Java client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints a JSON representation of the results and the list of tokens' lemmas with their part-of-speech. import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.cloud.Analyzer ; import ai.expert.nlapi.v2.cloud.AnalyzerConfig ; import ai.expert.nlapi.v2.message.AnalyzeResponse ; import ai.expert.nlapi.v2.model.AnalyzeDocument ; public class Main { public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static Analyzer createAnalyzer () throws Exception { return new Analyzer ( AnalyzerConfig . builder () . withVersion ( API . Versions . V2 ) . withContext ( \"standard\" ) . withLanguage ( API . Languages . en ) . withAuthentication ( createAuthentication ()) . build ()); } public static void main ( String [] args ) { try { String text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" ; Analyzer analyzer = createAnalyzer (); AnalyzeResponse disambiguation = analyzer . disambiguation ( text ); // Output JSON representation System . out . println ( \"JSON representation:\" ); disambiguation . prettyPrint (); // Tokens' lemma and part-of-speech System . out . println ( \"Tab separated list of tokens' lemma and part-of-speech:\" ); AnalyzeDocument data = disambiguation . getData (); data . getTokens (). stream (). forEach ( c -> System . out . println ( c . getLemma () + \"\\t\" + c . getPos ())); } catch ( Exception ex ) { ex . printStackTrace (); } } } curl (Linux) The following curl command posts a document to the deep linguistic analysis resource of the API's REST interface. Run the command from a shell after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en/disambiguation \\ -H 'Authorization: Bearer token' \\ -H 'Content-Type: application/json; charset=utf-8' \\ -d '{ \"document\": { \"text\": \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan' \\' 's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" } }' The server returns a JSON object . curl (Windows) The following curl command posts a document to the deep linguistic analysis resource of the API's REST interface. Open a command prompt in the folder where you installed curl and run the command after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en/disambiguation -H \"Authorization: Bearer token\" -H \"Content-Type: application/json; charset=utf-8\" -d \"{\\\" document\\ \": {\\\" text\\ \": \\\" Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\\ \"}}\" The server returns a JSON object .","title":"Overview"},{"location":"guide/linguistic-analysis/#deep-linguistic-analysis-overview","text":"Deep linguistic analysis is a type of document analysis that combines the following interdependent processes: Text subdivision Part-of-speech tagging Morphological analysis Lemmatization Syntactic analysis Semantic analysis The analysis is \"deep\" because: It performs the common linguistic analysis. It disambiguates the terms of the text, i.e. it determines the exact meaning of the text after considering all the other possibilities. Deep linguistic analysis also performs knowledge linking: Knowledge Graph information and open data\u2014Wikidata, DBpedia and GeoNames references\u2014are returned for text tokens corresponding to syncons of the expert.ai Knowledge Graph. In the case of actual places, geographic coordinates are also provided. Full analysis includes deep linguistic analysis, but if you are not interested in the other analyses, you can use specific resources having paths like this: analyze / context name / language code / disambiguation Boxed parts are placeholders, so for example: https://nlapi.expert.ai/v2/analyze/standard/en/disambiguation is the URL of the standard context resource performing the deep linguistic analysis of an English text. These resources must be requested with the POST method. In the manual's reference section you will find all the information required to perform deep linguistic analysis using the API's Restful interface, specifically: The format of the request to be submitted to the resources. How to build resources' paths and full endpoints . The output format . Note Even if you use the API through a client that hides the REST interface, whether it is made by you or is one of those provided by expert.ai , knowing the output format helps you understand and navigate the results. Here is an example of performing deep linguistic analysis on a short English text: Python This example is based on the Python client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints a JSON representation of the results and the list of tokens' lemmas with their part-of-speech. from expertai.nlapi.cloud.client import ExpertAiClient client = ExpertAiClient () text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" language = 'en' output = client . specific_resource_analysis ( body = { \"document\" : { \"text\" : text }}, params = { 'language' : language , 'resource' : 'disambiguation' }) # Output tokens' data print ( \"Output tokens' data:\" ); print ( f ' { \"TEXT\" :{ 20 }} { \"LEMMA\" :{ 40 }} { \"POS\" :{ 6 }} ' ) print ( f ' { \"----\" :{ 20 }} { \"-----\" :{ 40 }} { \"---\" :{ 6 }} ' ) for token in output . tokens : print ( f ' { text [ token . start : token . end ] :{ 20 }} { token . lemma :{ 40 }} { token . pos :{ 6 }} ' ) Java This example is based on the Java client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints a JSON representation of the results and the list of tokens' lemmas with their part-of-speech. import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.cloud.Analyzer ; import ai.expert.nlapi.v2.cloud.AnalyzerConfig ; import ai.expert.nlapi.v2.message.AnalyzeResponse ; import ai.expert.nlapi.v2.model.AnalyzeDocument ; public class Main { public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static Analyzer createAnalyzer () throws Exception { return new Analyzer ( AnalyzerConfig . builder () . withVersion ( API . Versions . V2 ) . withContext ( \"standard\" ) . withLanguage ( API . Languages . en ) . withAuthentication ( createAuthentication ()) . build ()); } public static void main ( String [] args ) { try { String text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" ; Analyzer analyzer = createAnalyzer (); AnalyzeResponse disambiguation = analyzer . disambiguation ( text ); // Output JSON representation System . out . println ( \"JSON representation:\" ); disambiguation . prettyPrint (); // Tokens' lemma and part-of-speech System . out . println ( \"Tab separated list of tokens' lemma and part-of-speech:\" ); AnalyzeDocument data = disambiguation . getData (); data . getTokens (). stream (). forEach ( c -> System . out . println ( c . getLemma () + \"\\t\" + c . getPos ())); } catch ( Exception ex ) { ex . printStackTrace (); } } } curl (Linux) The following curl command posts a document to the deep linguistic analysis resource of the API's REST interface. Run the command from a shell after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en/disambiguation \\ -H 'Authorization: Bearer token' \\ -H 'Content-Type: application/json; charset=utf-8' \\ -d '{ \"document\": { \"text\": \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan' \\' 's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" } }' The server returns a JSON object . curl (Windows) The following curl command posts a document to the deep linguistic analysis resource of the API's REST interface. Open a command prompt in the folder where you installed curl and run the command after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en/disambiguation -H \"Authorization: Bearer token\" -H \"Content-Type: application/json; charset=utf-8\" -d \"{\\\" document\\ \": {\\\" text\\ \": \\\" Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\\ \"}}\" The server returns a JSON object .","title":"Deep linguistic analysis overview"},{"location":"guide/linguistic-analysis/lemmatization/","text":"Lemmatization Lemmatization is the deep linguistic analysis process that tags tokens and atoms with their corresponding lemmas . For example, for this sentence: Michael Jordan was one of the best basketball players of all time. lemmatization produces this output for detected tokens: Token Lemma Michael Jordan Michael Jordan was (to) be one one of of the the best good basketball players basketball player of of all all time time . n/a In the case of collocations, lemmatization is also applied to constituent atoms. For example the token: basketball players is a collocation composed of two atoms, for which lemmas are: basketball player In the case of anaphoras , the lemma is that of the antecedent or postcedent. For example, in the following text: Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half. lemmatization recognizes Jordan and he in the second sentence as anaphoras, both of which have Michael Jordan as their antecedent in the first sentence, so the returned lemma for the anaphoras will be Michael Jordan . Lemmatization output is part of the JSON object returned by deep linguistic analysis .","title":"Lemmatization"},{"location":"guide/linguistic-analysis/lemmatization/#lemmatization","text":"Lemmatization is the deep linguistic analysis process that tags tokens and atoms with their corresponding lemmas . For example, for this sentence: Michael Jordan was one of the best basketball players of all time. lemmatization produces this output for detected tokens: Token Lemma Michael Jordan Michael Jordan was (to) be one one of of the the best good basketball players basketball player of of all all time time . n/a In the case of collocations, lemmatization is also applied to constituent atoms. For example the token: basketball players is a collocation composed of two atoms, for which lemmas are: basketball player In the case of anaphoras , the lemma is that of the antecedent or postcedent. For example, in the following text: Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half. lemmatization recognizes Jordan and he in the second sentence as anaphoras, both of which have Michael Jordan as their antecedent in the first sentence, so the returned lemma for the anaphoras will be Michael Jordan . Lemmatization output is part of the JSON object returned by deep linguistic analysis .","title":"Lemmatization"},{"location":"guide/linguistic-analysis/morphological-analysis/","text":"Morphological analysis Morphological analysis is the deep linguistic analysis process that determines lexical and grammatical features of each token in addition to the part-of-speech . The result of the analysis is a list of Universal features . For example, the morphological analysis of the first token of this sentence: I saw a dandelion on my lawn. gives: Case=Nom|Number=Sing|Person=1|PronType=Prs which is a list of feature-value pairs corresponding to: Pair Feature label Feature description Value label Value description Case=Nom Case Case Nom Nominative Number=Sing Number Number Sing Singular Person=1 Person Person 1 First PronType=Prs PronType Pronoun type Prs Personal Morphological analysis output is part of the JSON object returned by deep linguistic analysis .","title":"Morphological analysis"},{"location":"guide/linguistic-analysis/morphological-analysis/#morphological-analysis","text":"Morphological analysis is the deep linguistic analysis process that determines lexical and grammatical features of each token in addition to the part-of-speech . The result of the analysis is a list of Universal features . For example, the morphological analysis of the first token of this sentence: I saw a dandelion on my lawn. gives: Case=Nom|Number=Sing|Person=1|PronType=Prs which is a list of feature-value pairs corresponding to: Pair Feature label Feature description Value label Value description Case=Nom Case Case Nom Nominative Number=Sing Number Number Sing Singular Person=1 Person Person 1 First PronType=Prs PronType Pronoun type Prs Personal Morphological analysis output is part of the JSON object returned by deep linguistic analysis .","title":"Morphological analysis"},{"location":"guide/linguistic-analysis/pos-tagging/","text":"Part-of-speech tagging Standard tagging Standard part-of-speech tagging is the deep linguistic analysis process that marks up each token with the corresponding Universal POS tag . For example, for this sentence: Michael Jordan was one of the best basketball players of all time. standard part-of-speech tagging produces this output: Token Part-of-speech Universal POS tag Michael Jordan Proper noun PROPN was Auxiliary AUX one Numeral NUM of Adposition ADP the Determiner DET best Adjective ADJ basketball players Noun NOUN of Adposition ADP all Determiner DET time Noun NOUN . Punctuation PUNCT Standard part-of-speech tagging output is part of the JSON object returned by deep linguistic analysis . Custom tagging In addition to standard part-of-speech tagging, deep linguistic analysis marks up both tokens and atoms with custom expert.ai type labels . expert.ai types combine part-of-speech information with entity type information. For example, for the following sentence: Please Travis, take me to Avalon. Do you mind if I bring my dog Bella with me? custom tagging is: Token Type description Custom expert.ai label Please Adverb ADV Travis Proper name of a human being NPR.NPH , Punctuation PNT take Verb VER me Pronoun PRO to Preposition PRE Avalon Proper noun of an extra-terrestrial or imaginary place GEX . Punctuation PNT Do Auxiliary verb AUX you Pronoun PRO mind Verb VER if Conjunction CON I Pronoun PRO bring Verb VER my Adjective ADJ dog Noun NOU Bella Proper noun of an animal NPR.ANM with Preposition PRE me Pronoun PRO ? Punctuation PNT As mentioned above, the expert.ai type is also attributed to atoms, while standard part-of-speech tagging stops at the token level. Custom part-of-speech tagging output is part of the JSON object returned by deep linguistic analysis .","title":"Part-of-speech tagging"},{"location":"guide/linguistic-analysis/pos-tagging/#part-of-speech-tagging","text":"","title":"Part-of-speech tagging"},{"location":"guide/linguistic-analysis/pos-tagging/#standard-tagging","text":"Standard part-of-speech tagging is the deep linguistic analysis process that marks up each token with the corresponding Universal POS tag . For example, for this sentence: Michael Jordan was one of the best basketball players of all time. standard part-of-speech tagging produces this output: Token Part-of-speech Universal POS tag Michael Jordan Proper noun PROPN was Auxiliary AUX one Numeral NUM of Adposition ADP the Determiner DET best Adjective ADJ basketball players Noun NOUN of Adposition ADP all Determiner DET time Noun NOUN . Punctuation PUNCT Standard part-of-speech tagging output is part of the JSON object returned by deep linguistic analysis .","title":"Standard tagging"},{"location":"guide/linguistic-analysis/pos-tagging/#custom-tagging","text":"In addition to standard part-of-speech tagging, deep linguistic analysis marks up both tokens and atoms with custom expert.ai type labels . expert.ai types combine part-of-speech information with entity type information. For example, for the following sentence: Please Travis, take me to Avalon. Do you mind if I bring my dog Bella with me? custom tagging is: Token Type description Custom expert.ai label Please Adverb ADV Travis Proper name of a human being NPR.NPH , Punctuation PNT take Verb VER me Pronoun PRO to Preposition PRE Avalon Proper noun of an extra-terrestrial or imaginary place GEX . Punctuation PNT Do Auxiliary verb AUX you Pronoun PRO mind Verb VER if Conjunction CON I Pronoun PRO bring Verb VER my Adjective ADJ dog Noun NOU Bella Proper noun of an animal NPR.ANM with Preposition PRE me Pronoun PRO ? Punctuation PNT As mentioned above, the expert.ai type is also attributed to atoms, while standard part-of-speech tagging stops at the token level. Custom part-of-speech tagging output is part of the JSON object returned by deep linguistic analysis .","title":"Custom tagging"},{"location":"guide/linguistic-analysis/semantic-analysis/","text":"Semantic analysis Semantic analysis is the deep linguistic analysis process that maps tokens to Knowledge Graph entries. This is the process that makes linguistic analysys \"deep\" by attributing a meaning to each term of the text. Note : tokens corresponding to parts-of-speech like punctuation, conjunctions, articles, prepositions and pronouns are not mapped to Knowledge Graph entries. That it isn't because they lack meaning, but because part-of-speech tagging and morphological analysis provide enough information. Meaning attribution is a relatively easy task if a term is unambiguous. The problem arises when a term has multiple meanings . For example take the word: banks which can be interpreted as: Simple present tense, third person singular of the verb to bank in the sense of \"to deposit in a bank\" Plural form of the noun bank in the sense of \"financial institution\" Plural form of the noun bank in the sense of \"slope beside a body of water\" and more. In this case automatic disambiguation is needed, and this is the precisely what semantic analysis does using the context's Knowledge Graph and the relationships it contains. Semantic analysis output is part of the JSON object returned by deep linguistic analysis .","title":"Semantic analysis"},{"location":"guide/linguistic-analysis/semantic-analysis/#semantic-analysis","text":"Semantic analysis is the deep linguistic analysis process that maps tokens to Knowledge Graph entries. This is the process that makes linguistic analysys \"deep\" by attributing a meaning to each term of the text. Note : tokens corresponding to parts-of-speech like punctuation, conjunctions, articles, prepositions and pronouns are not mapped to Knowledge Graph entries. That it isn't because they lack meaning, but because part-of-speech tagging and morphological analysis provide enough information. Meaning attribution is a relatively easy task if a term is unambiguous. The problem arises when a term has multiple meanings . For example take the word: banks which can be interpreted as: Simple present tense, third person singular of the verb to bank in the sense of \"to deposit in a bank\" Plural form of the noun bank in the sense of \"financial institution\" Plural form of the noun bank in the sense of \"slope beside a body of water\" and more. In this case automatic disambiguation is needed, and this is the precisely what semantic analysis does using the context's Knowledge Graph and the relationships it contains. Semantic analysis output is part of the JSON object returned by deep linguistic analysis .","title":"Semantic analysis"},{"location":"guide/linguistic-analysis/subdivision/","text":"Text subdivision The text subdivision process is the part of the deep linguistic analysis that detects text structure in terms of: Paragraphs Sentences Phrases Tokens Atoms During this process, the phrase type is also determined. A token can be: A collocation , that is a sequence of consecutive words recognized as a unit, like credit card or red carpet . A single word A punctuation mark By definition, an atom is something that cannot be further divided. In the case of single words or punctuation marks, the atoms coincide with the tokens, while in the case of collocations, for each token of that type there will be as many atoms as there are words that make up the token. As an example of text subdivision, consider this text: Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half. Michael Jordan was also a baseball player and an actor. It gets divided in two paragraphs: 1. Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half. 2. Michael Jordan was also a baseball player and an actor. The first paragraph is divided in two sentences: 1. Michael Jordan was one of the best basketball players of all time. 2. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half. The first sentence is divided in six phrases: 1. Michael Jordan 2. was 3. one 4. of the best basketball players 5. of all time 6. . The fourth phrase is divided into four tokens: 1. of 2. the 3. best 4. basketball players Since atoms and tokens coincide except in the case of collocations, atoms are returned only in that case, so the fourth token is divided in two atoms: 1. basketball 2. player For each subdivision the process returns: The position The reference to the lower level constituent subdivisions Text subdivision output is part of the JSON object returned by deep linguistic analysis .","title":"Text subdivision"},{"location":"guide/linguistic-analysis/subdivision/#text-subdivision","text":"The text subdivision process is the part of the deep linguistic analysis that detects text structure in terms of: Paragraphs Sentences Phrases Tokens Atoms During this process, the phrase type is also determined. A token can be: A collocation , that is a sequence of consecutive words recognized as a unit, like credit card or red carpet . A single word A punctuation mark By definition, an atom is something that cannot be further divided. In the case of single words or punctuation marks, the atoms coincide with the tokens, while in the case of collocations, for each token of that type there will be as many atoms as there are words that make up the token. As an example of text subdivision, consider this text: Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half. Michael Jordan was also a baseball player and an actor. It gets divided in two paragraphs: 1. Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half. 2. Michael Jordan was also a baseball player and an actor. The first paragraph is divided in two sentences: 1. Michael Jordan was one of the best basketball players of all time. 2. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half. The first sentence is divided in six phrases: 1. Michael Jordan 2. was 3. one 4. of the best basketball players 5. of all time 6. . The fourth phrase is divided into four tokens: 1. of 2. the 3. best 4. basketball players Since atoms and tokens coincide except in the case of collocations, atoms are returned only in that case, so the fourth token is divided in two atoms: 1. basketball 2. player For each subdivision the process returns: The position The reference to the lower level constituent subdivisions Text subdivision output is part of the JSON object returned by deep linguistic analysis .","title":"Text subdivision"},{"location":"guide/linguistic-analysis/syntactic-analysis/","text":"Syntactic analysis In the context of deep linguistic analysis , syntactic analysis is the parsing process that detects the universal dependency relation between each token and the sentence root token or another token. The process assigns a dependency relation label to each token. For example, for this sentence: The company has developed an entirely new category of products. syntactic analysis determines the head token index and the dependency label as follows: Token index Token text Head token index Universal dependency label 0 The 1 det 1 company 3 nsubj 2 has 3 aux 3 developed 3 root 4 an 7 det 5 entirely 7 advmod 6 new 7 amod 7 category 3 obj 8 of 9 case 9 product 7 nmod 10 . 3 punct Syntactic analysis output is part of the JSON object returned by deep linguistic analysis . Dependencies can be represented in various ways , such as a tree or arrow arcs .","title":"Syntactic analysis"},{"location":"guide/linguistic-analysis/syntactic-analysis/#syntactic-analysis","text":"In the context of deep linguistic analysis , syntactic analysis is the parsing process that detects the universal dependency relation between each token and the sentence root token or another token. The process assigns a dependency relation label to each token. For example, for this sentence: The company has developed an entirely new category of products. syntactic analysis determines the head token index and the dependency label as follows: Token index Token text Head token index Universal dependency label 0 The 1 det 1 company 3 nsubj 2 has 3 aux 3 developed 3 root 4 an 7 det 5 entirely 7 advmod 6 new 7 amod 7 category 3 obj 8 of 9 case 9 product 7 nmod 10 . 3 punct Syntactic analysis output is part of the JSON object returned by deep linguistic analysis . Dependencies can be represented in various ways , such as a tree or arrow arcs .","title":"Syntactic analysis"},{"location":"guide/relation-extraction/","text":"Relation extraction Relation extraction is a type of document analysis that labels concepts expressed in the text with their semantic role . Relation extraction also performs knowledge linking: Knowledge Graph information and open data\u2014Wikidata, DBpedia and GeoNames references\u2014are returned for relation items corresponding to syncons of the expert.ai Knowledge Graph. In the case of actual places, geographic coordinates are also provided. Full analysis includes relation extraction, but if you are not interested in the other analyses, you can use specific resources having paths like this: analyze / context name / language code / relations Boxed parts are placeholders, so for example: https://nlapi.expert.ai/v2/analyze/standard/en/relations is the URL of the standard context resource performing relation extraction on an English text. These resources must be requested with the POST method. In the reference section of this manual you will find all the information you need to perform relation extraction using the API's RESTful interface, specifically: The format of the request to be submitted to the resources. How to build resources' paths and full endpoints . The output format . Note Even if you use the API through a client that hides the REST interface, whether it is made by you or is one of those provided by expert.ai , knowing the output format helps you understand and navigate the results. Here is an example of performing relation extraction on a short English text: Python This example is based on the Python client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints a shallow (no nesting) representation of relations. from expertai.nlapi.cloud.client import ExpertAiClient client = ExpertAiClient () text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" language = 'en' output = client . specific_resource_analysis ( body = { \"document\" : { \"text\" : text }}, params = { 'language' : language , 'resource' : 'relations' }) # Output relations' data print ( \"Output relations' data:\" ); for relation in output . relations : print ( relation . verb . lemma , \":\" ); for related in relation . related : print ( \" \\t \" , \"(\" , related . relation , \")\" , related . lemma ); Java This example is based on the Java client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the JSON response . import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.cloud.Analyzer ; import ai.expert.nlapi.v2.cloud.AnalyzerConfig ; import ai.expert.nlapi.v2.message.AnalyzeResponse ; public class Main { public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static Analyzer createAnalyzer () throws Exception { return new Analyzer ( AnalyzerConfig . builder () . withVersion ( API . Versions . V2 ) . withContext ( \"standard\" ) . withLanguage ( API . Languages . en ) . withAuthentication ( createAuthentication ()) . build ()); } public static void main ( String [] args ) { try { String text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" ; Analyzer analyzer = createAnalyzer (); AnalyzeResponse relations = analyzer . relations ( text ); // Output JSON representation System . out . println ( \"JSON representation:\" ); relations . prettyPrint (); } catch ( Exception ex ) { ex . printStackTrace (); } } } curl (Linux) The following curl command posts a document to the named entity recognition resource of the API's REST interface. Run the command from a shell after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en/relations \\ -H 'Authorization: Bearer token' \\ -H 'Content-Type: application/json; charset=utf-8' \\ -d '{ \"document\": { \"text\": \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan' \\' 's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" } }' The server returns a JSON object . curl (Windows) The following curl command posts a document to the named entity recognition resource of the API's REST interface. Open a command prompt in the folder where you installed curl and run the command after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en/relations -H \"Authorization: Bearer token\" -H \"Content-Type: application/json; charset=utf-8\" -d \"{\\\" document\\ \": {\\\" text\\ \": \\\" Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\\ \"}}\" The server returns a JSON object .","title":"Relation extraction"},{"location":"guide/relation-extraction/#relation-extraction","text":"Relation extraction is a type of document analysis that labels concepts expressed in the text with their semantic role . Relation extraction also performs knowledge linking: Knowledge Graph information and open data\u2014Wikidata, DBpedia and GeoNames references\u2014are returned for relation items corresponding to syncons of the expert.ai Knowledge Graph. In the case of actual places, geographic coordinates are also provided. Full analysis includes relation extraction, but if you are not interested in the other analyses, you can use specific resources having paths like this: analyze / context name / language code / relations Boxed parts are placeholders, so for example: https://nlapi.expert.ai/v2/analyze/standard/en/relations is the URL of the standard context resource performing relation extraction on an English text. These resources must be requested with the POST method. In the reference section of this manual you will find all the information you need to perform relation extraction using the API's RESTful interface, specifically: The format of the request to be submitted to the resources. How to build resources' paths and full endpoints . The output format . Note Even if you use the API through a client that hides the REST interface, whether it is made by you or is one of those provided by expert.ai , knowing the output format helps you understand and navigate the results. Here is an example of performing relation extraction on a short English text: Python This example is based on the Python client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints a shallow (no nesting) representation of relations. from expertai.nlapi.cloud.client import ExpertAiClient client = ExpertAiClient () text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" language = 'en' output = client . specific_resource_analysis ( body = { \"document\" : { \"text\" : text }}, params = { 'language' : language , 'resource' : 'relations' }) # Output relations' data print ( \"Output relations' data:\" ); for relation in output . relations : print ( relation . verb . lemma , \":\" ); for related in relation . related : print ( \" \\t \" , \"(\" , related . relation , \")\" , related . lemma ); Java This example is based on the Java client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the JSON response . import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.cloud.Analyzer ; import ai.expert.nlapi.v2.cloud.AnalyzerConfig ; import ai.expert.nlapi.v2.message.AnalyzeResponse ; public class Main { public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static Analyzer createAnalyzer () throws Exception { return new Analyzer ( AnalyzerConfig . builder () . withVersion ( API . Versions . V2 ) . withContext ( \"standard\" ) . withLanguage ( API . Languages . en ) . withAuthentication ( createAuthentication ()) . build ()); } public static void main ( String [] args ) { try { String text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" ; Analyzer analyzer = createAnalyzer (); AnalyzeResponse relations = analyzer . relations ( text ); // Output JSON representation System . out . println ( \"JSON representation:\" ); relations . prettyPrint (); } catch ( Exception ex ) { ex . printStackTrace (); } } } curl (Linux) The following curl command posts a document to the named entity recognition resource of the API's REST interface. Run the command from a shell after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en/relations \\ -H 'Authorization: Bearer token' \\ -H 'Content-Type: application/json; charset=utf-8' \\ -d '{ \"document\": { \"text\": \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan' \\' 's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" } }' The server returns a JSON object . curl (Windows) The following curl command posts a document to the named entity recognition resource of the API's REST interface. Open a command prompt in the folder where you installed curl and run the command after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en/relations -H \"Authorization: Bearer token\" -H \"Content-Type: application/json; charset=utf-8\" -d \"{\\\" document\\ \": {\\\" text\\ \": \\\" Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\\ \"}}\" The server returns a JSON object .","title":"Relation extraction"},{"location":"guide/sentiment-analysis/","text":"Sentiment analysis Sentiment analysis is a type of document analysis that determines how positive or negative the tone of the text is. Sentiment analysis also performs knowledge linking: Knowledge Graph information and open data\u2014Wikidata, DBpedia and GeoNames references\u2014are returned for text items that express sentiment given they correspond to syncons of the expert.ai Knowledge Graph. In the case of actual places, geographic coordinates are also provided. Full analysis includes sentiment analysis, but if you are not interested in the other analyses, you can use specific resources having paths like this: analyze / context name / language code / sentiment Boxed parts are placeholders, so for example: https://nlapi.expert.ai/v2/analyze/standard/en/sentiment is the URL of the standard context resource performing the sentiment analysis of an English text. These resources must be requested with the POST method. In the reference section of this manual you will find all the information you need to perform sentiment analysis using the API's RESTful interface, specifically: The format of the request to be submitted to the resources. How to build resources' paths and full endpoints . The output format . Note Even if you use the API through a client that hides the REST interface, whether it is made by you or is one of those provided by expert.ai , knowing the output format helps you understand and navigate the results. Here is an example of performing sentiment analysis on a short English text: Python This example is based on the Python client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the overall sentiment. from expertai.nlapi.cloud.client import ExpertAiClient client = ExpertAiClient () text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" language = 'en' output = client . specific_resource_analysis ( body = { \"document\" : { \"text\" : text }}, params = { 'language' : language , 'resource' : 'sentiment' }) # Output overall sentiment print ( \"Output overall sentiment:\" ) print ( output . sentiment . overall ) Java This example is based on the Java client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the JSON response and the overall sentiment. import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.cloud.Analyzer ; import ai.expert.nlapi.v2.cloud.AnalyzerConfig ; import ai.expert.nlapi.v2.message.AnalyzeResponse ; import ai.expert.nlapi.v2.model.AnalyzeDocument ; public class Main { public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static Analyzer createAnalyzer () throws Exception { return new Analyzer ( AnalyzerConfig . builder () . withVersion ( API . Versions . V2 ) . withContext ( \"standard\" ) . withLanguage ( API . Languages . en ) . withAuthentication ( createAuthentication ()) . build ()); } public static void main ( String [] args ) { try { String text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" ; Analyzer analyzer = createAnalyzer (); AnalyzeResponse sentiment = analyzer . sentiment ( text ); // Output JSON representation System . out . println ( \"JSON representation:\" ); sentiment . prettyPrint (); // Overall sentiment. System . out . println ( \"Overall sentiment:\" ); AnalyzeDocument data = sentiment . getData (); System . out . println ( data . getSentiment (). getOverall ()); } catch ( Exception ex ) { ex . printStackTrace (); } } } curl (Linux) The following curl command posts a document to the named entity recognition resource of the API's REST interface. Run the command from a shell after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en/sentiment \\ -H 'Authorization: Bearer token' \\ -H 'Content-Type: application/json; charset=utf-8' \\ -d '{ \"document\": { \"text\": \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan' \\' 's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" } }' The server returns a JSON object . curl (Windows) The following curl command posts a document to the named entity recognition resource of the API's REST interface. Open a command prompt in the folder where you installed curl and run the command after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en/sentiment -H \"Authorization: Bearer token\" -H \"Content-Type: application/json; charset=utf-8\" -d \"{\\\" document\\ \": {\\\" text\\ \": \\\" Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\\ \"}}\" The server returns a JSON object","title":"Sentiment analysis"},{"location":"guide/sentiment-analysis/#sentiment-analysis","text":"Sentiment analysis is a type of document analysis that determines how positive or negative the tone of the text is. Sentiment analysis also performs knowledge linking: Knowledge Graph information and open data\u2014Wikidata, DBpedia and GeoNames references\u2014are returned for text items that express sentiment given they correspond to syncons of the expert.ai Knowledge Graph. In the case of actual places, geographic coordinates are also provided. Full analysis includes sentiment analysis, but if you are not interested in the other analyses, you can use specific resources having paths like this: analyze / context name / language code / sentiment Boxed parts are placeholders, so for example: https://nlapi.expert.ai/v2/analyze/standard/en/sentiment is the URL of the standard context resource performing the sentiment analysis of an English text. These resources must be requested with the POST method. In the reference section of this manual you will find all the information you need to perform sentiment analysis using the API's RESTful interface, specifically: The format of the request to be submitted to the resources. How to build resources' paths and full endpoints . The output format . Note Even if you use the API through a client that hides the REST interface, whether it is made by you or is one of those provided by expert.ai , knowing the output format helps you understand and navigate the results. Here is an example of performing sentiment analysis on a short English text: Python This example is based on the Python client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the overall sentiment. from expertai.nlapi.cloud.client import ExpertAiClient client = ExpertAiClient () text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" language = 'en' output = client . specific_resource_analysis ( body = { \"document\" : { \"text\" : text }}, params = { 'language' : language , 'resource' : 'sentiment' }) # Output overall sentiment print ( \"Output overall sentiment:\" ) print ( output . sentiment . overall ) Java This example is based on the Java client you can find on GitHub . The client gets user credentials from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints the JSON response and the overall sentiment. import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.cloud.Analyzer ; import ai.expert.nlapi.v2.cloud.AnalyzerConfig ; import ai.expert.nlapi.v2.message.AnalyzeResponse ; import ai.expert.nlapi.v2.model.AnalyzeDocument ; public class Main { public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static Analyzer createAnalyzer () throws Exception { return new Analyzer ( AnalyzerConfig . builder () . withVersion ( API . Versions . V2 ) . withContext ( \"standard\" ) . withLanguage ( API . Languages . en ) . withAuthentication ( createAuthentication ()) . build ()); } public static void main ( String [] args ) { try { String text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" ; Analyzer analyzer = createAnalyzer (); AnalyzeResponse sentiment = analyzer . sentiment ( text ); // Output JSON representation System . out . println ( \"JSON representation:\" ); sentiment . prettyPrint (); // Overall sentiment. System . out . println ( \"Overall sentiment:\" ); AnalyzeDocument data = sentiment . getData (); System . out . println ( data . getSentiment (). getOverall ()); } catch ( Exception ex ) { ex . printStackTrace (); } } } curl (Linux) The following curl command posts a document to the named entity recognition resource of the API's REST interface. Run the command from a shell after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en/sentiment \\ -H 'Authorization: Bearer token' \\ -H 'Content-Type: application/json; charset=utf-8' \\ -d '{ \"document\": { \"text\": \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan' \\' 's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" } }' The server returns a JSON object . curl (Windows) The following curl command posts a document to the named entity recognition resource of the API's REST interface. Open a command prompt in the folder where you installed curl and run the command after replacing token with the actual authorization token . curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en/sentiment -H \"Authorization: Bearer token\" -H \"Content-Type: application/json; charset=utf-8\" -d \"{\\\" document\\ \": {\\\" text\\ \": \\\" Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\\ \"}}\" The server returns a JSON object","title":"Sentiment analysis"},{"location":"how-to/","text":"Developer how-to Programmatic use The expert.ai Natural Language API is a cloud-based service with a REST interface. This means that in order to use it a program must be able to access the Web and carry out an HTTP conversation with the API interface. Whenever the program has to analyze a document, classify a document or detect information inside a document, it must request the most suitable API resource, similarly to what you do when you request the page of a site with a Web browser. If you use one of the client packages available on GitHub , the details of the conversation are hidden, otherwise the program must use an HTTP client to request the API resources. In both cases what happens is the same: the program, via the\u2014explicit or hidden\u2014HTTP client, transmits a request to the API server. The request contains the address of the resource of interest\u2014its URL, or endpoint \u2014and the text of the document. For this type of request the POST method is used. Faced with this request, the server responds synchronously (after an amount of time depending on the type of processing requested and the complexity/length of the text) with the results of the processing. If the program integrates an expert.ai client package, the interpretation of the result is simple because it involves examining the properties of an object. Otherwise it must be considered that the result is a JSON object that should be \"de-serialized\" in order to parse it. In this manual you will find all the information about the format of the request and the JSON objects that the API resources return, so you can easily create your own parser. Authentication and authorization Each API request must contain an authorization token. The bearer authentication mechanism is used, so the token must be obtained with an authentication operation and then specified as a header in each request. The authentication operation is carried out by requesting\u2014with a conversation identical to that described above\u2014a special resource that is not strictly part of the API because it is shared by all the expert.ai cloud services. Its address is: https://developer.expert.ai/oauth2/token This resource must also be requested with the POST method and the body of the request must be a JSON object like this: { \"username\": \" yourusername \", \"password\": \" yourpassword \" } with yourusername and yourpassword replaced by the developer credentials obtained by registering on the expert.ai developer portal . The Content-Type header of the request must be set to: application/json; charset=utf-8 The response is the token and is a plain text like this: eyJraWQiOiJlZXEzSnB5WWxzeTJ4eTFuQnd4eDVZaEo3YWEwWWdMXC9DaUYyalJGMkxScz0iLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiIxZnR2YTQ5MjJ2MWlibzQ3M2JxZDRwM3VjcyIsInRva2VuX3VzZSI6ImFjY2VzcyIsInNjb3BlIjoiYXBpXC9jb250ZXh0cy5yZWFkIiwiYXV0aF90aW1lIjoxNTkzNzc5NTc4LCJpc3MiOiJodHRwczpcL1wvY29nbml0by1pZHAuZXUtd2VzdC0xLmFtYXpvbmF3cy5jb21cL2V1LXdlc3QtMV9BVUhnUTA4Q0IiLCJleHAiOjE1OTM3ODMxNzgsImlhdCI6MTU5Mzc3OTU3OCwidmVyc2lvbiI6MiwianRpIjoiM2EwODA0MDEtZGVmMC00NDNmLWI2OWItNGJhNTc5ZGJhOWY1IiwiY2xpZW50X2lkIjoiMWZ0dmE0OTIydjFpYm80NzNicWQ0cDN1Y3MifQ.LLjeg4su7X-ftlC5ReXoCPkQe-Mw2EZmBbG9tex_NZOaDY4tnBJZEgneboI0CcJHXZcZZJZ8U19dG9OcClEpEEytRJFWZ3hGXhSMXYpScn21oSmiyNwNi2f3Tv9t-nRv3ksmlsx7IZZoxTnc0ECXF10bdR55OMF1Z7DZ3k2fyWF2ClD8hAwJQEYKAZq4UfMVDjbYSOA7Hm7SChc3mx5XmLzPFtVSJ4MONjDBZiM5bOUj22dqWnf90-8i9mY5T40HI2JhD99tQI8HCXQWpNxax_dH_5W9AC3MHmNZA_d6xBEna8H8QbjpQdNyhvxN1M1JsJaEvRP7zhRuCqJmhj2sLA The application program must therefore \"know\" the credentials of the developer and obtain the authorization token through them. If you use an expert.ai client package, the details of the authentication and authorization dialogue are hidden. Otherwise, when the program requests API resources it must include the Authorization header in each request using this format: Bearer token with token replaced by the actual token. As mentioned above, authorization tokens have a duration, they expire after a certain time. If the application continues to make requests with an expired token it will get 401 Unauthorized errors. In that case it must request a new token to replace the old one. OpenAPI specification The expert.ai Natural Language API is described by documents conforming to the OpenAPI specification . These documents are human-readable, but are meant to be also interpreted by a machine. This allows, using special tools, to automatically generate client code. The OpenAPI documents can therefore be considered contracts between the API supplier and the developer community: a client developed upon the OpenAPI specification \"signs\" the contract and is certain to be compatible with the API. The main contract of the current version of the API is the nlapi.yaml file published in a GitHub repository . The main contract covers all the generic use cases, while sub-contracts can exist that describe special uses of the API. The fact that they are described in separate contracts does not mean those parts are \"outside\" the API, but only that the main contract is not meant to be burdened with the description of special use cases. For example, the geotax-w-geojson.yaml OpenAPI document describes a special use of the geotax classification resource, that is how to use the resource to have a GeoJSON format output and how the output is structured. Similarly, the pii.yaml OpenAPI document formally describes the specific output of the pii detector .","title":"Developer how-to"},{"location":"how-to/#developer-how-to","text":"","title":"Developer how-to"},{"location":"how-to/#programmatic-use","text":"The expert.ai Natural Language API is a cloud-based service with a REST interface. This means that in order to use it a program must be able to access the Web and carry out an HTTP conversation with the API interface. Whenever the program has to analyze a document, classify a document or detect information inside a document, it must request the most suitable API resource, similarly to what you do when you request the page of a site with a Web browser. If you use one of the client packages available on GitHub , the details of the conversation are hidden, otherwise the program must use an HTTP client to request the API resources. In both cases what happens is the same: the program, via the\u2014explicit or hidden\u2014HTTP client, transmits a request to the API server. The request contains the address of the resource of interest\u2014its URL, or endpoint \u2014and the text of the document. For this type of request the POST method is used. Faced with this request, the server responds synchronously (after an amount of time depending on the type of processing requested and the complexity/length of the text) with the results of the processing. If the program integrates an expert.ai client package, the interpretation of the result is simple because it involves examining the properties of an object. Otherwise it must be considered that the result is a JSON object that should be \"de-serialized\" in order to parse it. In this manual you will find all the information about the format of the request and the JSON objects that the API resources return, so you can easily create your own parser.","title":"Programmatic use"},{"location":"how-to/#authentication-and-authorization","text":"Each API request must contain an authorization token. The bearer authentication mechanism is used, so the token must be obtained with an authentication operation and then specified as a header in each request. The authentication operation is carried out by requesting\u2014with a conversation identical to that described above\u2014a special resource that is not strictly part of the API because it is shared by all the expert.ai cloud services. Its address is: https://developer.expert.ai/oauth2/token This resource must also be requested with the POST method and the body of the request must be a JSON object like this: { \"username\": \" yourusername \", \"password\": \" yourpassword \" } with yourusername and yourpassword replaced by the developer credentials obtained by registering on the expert.ai developer portal . The Content-Type header of the request must be set to: application/json; charset=utf-8 The response is the token and is a plain text like this: eyJraWQiOiJlZXEzSnB5WWxzeTJ4eTFuQnd4eDVZaEo3YWEwWWdMXC9DaUYyalJGMkxScz0iLCJhbGciOiJSUzI1NiJ9.eyJzdWIiOiIxZnR2YTQ5MjJ2MWlibzQ3M2JxZDRwM3VjcyIsInRva2VuX3VzZSI6ImFjY2VzcyIsInNjb3BlIjoiYXBpXC9jb250ZXh0cy5yZWFkIiwiYXV0aF90aW1lIjoxNTkzNzc5NTc4LCJpc3MiOiJodHRwczpcL1wvY29nbml0by1pZHAuZXUtd2VzdC0xLmFtYXpvbmF3cy5jb21cL2V1LXdlc3QtMV9BVUhnUTA4Q0IiLCJleHAiOjE1OTM3ODMxNzgsImlhdCI6MTU5Mzc3OTU3OCwidmVyc2lvbiI6MiwianRpIjoiM2EwODA0MDEtZGVmMC00NDNmLWI2OWItNGJhNTc5ZGJhOWY1IiwiY2xpZW50X2lkIjoiMWZ0dmE0OTIydjFpYm80NzNicWQ0cDN1Y3MifQ.LLjeg4su7X-ftlC5ReXoCPkQe-Mw2EZmBbG9tex_NZOaDY4tnBJZEgneboI0CcJHXZcZZJZ8U19dG9OcClEpEEytRJFWZ3hGXhSMXYpScn21oSmiyNwNi2f3Tv9t-nRv3ksmlsx7IZZoxTnc0ECXF10bdR55OMF1Z7DZ3k2fyWF2ClD8hAwJQEYKAZq4UfMVDjbYSOA7Hm7SChc3mx5XmLzPFtVSJ4MONjDBZiM5bOUj22dqWnf90-8i9mY5T40HI2JhD99tQI8HCXQWpNxax_dH_5W9AC3MHmNZA_d6xBEna8H8QbjpQdNyhvxN1M1JsJaEvRP7zhRuCqJmhj2sLA The application program must therefore \"know\" the credentials of the developer and obtain the authorization token through them. If you use an expert.ai client package, the details of the authentication and authorization dialogue are hidden. Otherwise, when the program requests API resources it must include the Authorization header in each request using this format: Bearer token with token replaced by the actual token. As mentioned above, authorization tokens have a duration, they expire after a certain time. If the application continues to make requests with an expired token it will get 401 Unauthorized errors. In that case it must request a new token to replace the old one.","title":"Authentication and authorization"},{"location":"how-to/#openapi-specification","text":"The expert.ai Natural Language API is described by documents conforming to the OpenAPI specification . These documents are human-readable, but are meant to be also interpreted by a machine. This allows, using special tools, to automatically generate client code. The OpenAPI documents can therefore be considered contracts between the API supplier and the developer community: a client developed upon the OpenAPI specification \"signs\" the contract and is certain to be compatible with the API. The main contract of the current version of the API is the nlapi.yaml file published in a GitHub repository . The main contract covers all the generic use cases, while sub-contracts can exist that describe special uses of the API. The fact that they are described in separate contracts does not mean those parts are \"outside\" the API, but only that the main contract is not meant to be burdened with the description of special use cases. For example, the geotax-w-geojson.yaml OpenAPI document describes a special use of the geotax classification resource, that is how to use the resource to have a GeoJSON format output and how the output is structured. Similarly, the pii.yaml OpenAPI document formally describes the specific output of the pii detector .","title":"OpenAPI specification"},{"location":"quick-start/first-call/","text":"3. Make your first call Once you have a client tool set up, you can try out named entity recognition on a short English text. First call with curl Linux Run the following command from a shell after replacing the string token (only that, the string Bearer must not be changed) with the actual authorization token you obtained in the previous step : curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en/entities \\ -H 'Authorization: Bearer token' \\ -H 'Content-Type: application/json; charset=utf-8' \\ -d '{ \"document\": { \"text\" : \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" } }' Windows Open a command prompt in the folder where you installed curl and run the following command after replacing the string token (only that, the string Bearer must not be changed) with the actual authorization token you obtained in the previous step : curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en/entities -H \"Authorization: Bearer token\" -H \"Content-Type: application/json\" -d \"{\\\" document\\ \": {\\\" text\\ \": \\\" Michael Jordan was one of the best basketball players of all time. Scoring was Jordan stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\\ \"}}\" The command result is a JSON object containing recognized entities. First call with Postman Run Postman. Expand the expert.ai Natural Language API collection. Expand the Document analysis folder. Select entities . Select Send in the right panel. The result is a JSON object containing recognized entities. First call with an expert.ai client package Python In order to use the API you must have an expert.ai developer account. Visit the developer portal and sign up to get one. The Python client needs to know your account credentials. It can get them from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables before running the script below. The script prints out the named entities recognized in a short English text. from expertai.nlapi.cloud.client import ExpertAiClient client = ExpertAiClient () text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" language = 'en' output = client . specific_resource_analysis ( body = { \"document\" : { \"text\" : text }}, params = { 'language' : language , 'resource' : 'entities' }) print ( f ' { \"ENTITY\" :{ 50 }} { \"TYPE\" :{ 10 }} ' ) print ( f ' { \"------\" :{ 50 }} { \"----\" :{ 10 }} ' ) for entity in output . entities : print ( f ' { entity . lemma :{ 50 }} { entity . type_ :{ 10 }} ' ) Java In order to use the API you must have an expert.ai developer account. Visit the developer portal and sign up to get one. The Java client needs to know your account credentials. It can get them from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints a JSON object containing the entities recognized in a short English text. import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.cloud.Analyzer ; import ai.expert.nlapi.v2.cloud.AnalyzerConfig ; import ai.expert.nlapi.v2.message.AnalyzeResponse ; public class Main { public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static Analyzer createAnalyzer () throws Exception { return new Analyzer ( AnalyzerConfig . builder () . withVersion ( API . Versions . V2 ) . withContext ( \"standard\" ) . withLanguage ( API . Languages . en ) . withAuthentication ( createAuthentication ()) . build ()); } public static void main ( String [] args ) { try { String text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but \" + \"he still holds a defensive NBA record, with eight steals in a half.\" ; Analyzer analyzer = createAnalyzer (); AnalyzeResponse entities = analyzer . entities ( text ); // Output JSON representation entities . prettyPrint (); } catch ( Exception ex ) { ex . printStackTrace (); } } } The next step Now that you've had a taste of the API, discover its capabilites and learn more about using it programmatically .","title":"3. Make your first call"},{"location":"quick-start/first-call/#3-make-your-first-call","text":"Once you have a client tool set up, you can try out named entity recognition on a short English text.","title":"3. Make your first call"},{"location":"quick-start/first-call/#first-call-with-curl","text":"Linux Run the following command from a shell after replacing the string token (only that, the string Bearer must not be changed) with the actual authorization token you obtained in the previous step : curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en/entities \\ -H 'Authorization: Bearer token' \\ -H 'Content-Type: application/json; charset=utf-8' \\ -d '{ \"document\": { \"text\" : \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" } }' Windows Open a command prompt in the folder where you installed curl and run the following command after replacing the string token (only that, the string Bearer must not be changed) with the actual authorization token you obtained in the previous step : curl -X POST https://nlapi.expert.ai/v2/analyze/standard/en/entities -H \"Authorization: Bearer token\" -H \"Content-Type: application/json\" -d \"{\\\" document\\ \": {\\\" text\\ \": \\\" Michael Jordan was one of the best basketball players of all time. Scoring was Jordan stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\\ \"}}\" The command result is a JSON object containing recognized entities.","title":"First call with curl"},{"location":"quick-start/first-call/#first-call-with-postman","text":"Run Postman. Expand the expert.ai Natural Language API collection. Expand the Document analysis folder. Select entities . Select Send in the right panel. The result is a JSON object containing recognized entities.","title":"First call with Postman"},{"location":"quick-start/first-call/#first-call-with-an-expertai-client-package","text":"Python In order to use the API you must have an expert.ai developer account. Visit the developer portal and sign up to get one. The Python client needs to know your account credentials. It can get them from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables before running the script below. The script prints out the named entities recognized in a short English text. from expertai.nlapi.cloud.client import ExpertAiClient client = ExpertAiClient () text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" language = 'en' output = client . specific_resource_analysis ( body = { \"document\" : { \"text\" : text }}, params = { 'language' : language , 'resource' : 'entities' }) print ( f ' { \"ENTITY\" :{ 50 }} { \"TYPE\" :{ 10 }} ' ) print ( f ' { \"------\" :{ 50 }} { \"----\" :{ 10 }} ' ) for entity in output . entities : print ( f ' { entity . lemma :{ 50 }} { entity . type_ :{ 10 }} ' ) Java In order to use the API you must have an expert.ai developer account. Visit the developer portal and sign up to get one. The Java client needs to know your account credentials. It can get them from two environment variables: EAI_USERNAME EAI_PASSWORD Set those variables with your account credentials before running the sample program below. The program prints a JSON object containing the entities recognized in a short English text. import ai.expert.nlapi.security.Authentication ; import ai.expert.nlapi.security.Authenticator ; import ai.expert.nlapi.security.BasicAuthenticator ; import ai.expert.nlapi.security.DefaultCredentialsProvider ; import ai.expert.nlapi.v2.API ; import ai.expert.nlapi.v2.cloud.Analyzer ; import ai.expert.nlapi.v2.cloud.AnalyzerConfig ; import ai.expert.nlapi.v2.message.AnalyzeResponse ; public class Main { public static Authentication createAuthentication () throws Exception { DefaultCredentialsProvider credentialsProvider = new DefaultCredentialsProvider (); Authenticator authenticator = new BasicAuthenticator ( credentialsProvider ); return new Authentication ( authenticator ); } public static Analyzer createAnalyzer () throws Exception { return new Analyzer ( AnalyzerConfig . builder () . withVersion ( API . Versions . V2 ) . withContext ( \"standard\" ) . withLanguage ( API . Languages . en ) . withAuthentication ( createAuthentication ()) . build ()); } public static void main ( String [] args ) { try { String text = \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but \" + \"he still holds a defensive NBA record, with eight steals in a half.\" ; Analyzer analyzer = createAnalyzer (); AnalyzeResponse entities = analyzer . entities ( text ); // Output JSON representation entities . prettyPrint (); } catch ( Exception ex ) { ex . printStackTrace (); } } }","title":"First call with an expert.ai client package"},{"location":"quick-start/first-call/#the-next-step","text":"Now that you've had a taste of the API, discover its capabilites and learn more about using it programmatically .","title":"The next step"},{"location":"quick-start/setup-client-tools/","text":"1. Setup the client tools The Natural Language API can be used from a variety of programs and ready-to-use tools. For example, you can use utilities like curl and Postman or use one of the expert.ai client packages. Get curl If you prefer to make your first call from the command line, use curl. If you use Linux there's a chance that it is already installed on your computer, otherwise download it from the curl site . You are now ready for the next step: get the authorization token . Get Postman If you like Postman, you can make your first call with a predefined collection that you can also use to try all the other API's capabilities. Get the latest version of Postman from the Postman site and install the program following the instructions. Download the sample collection . Run Postman and import the sample collection. You are now ready for the next step: get the authorization token . Get an expert.ai client package If you prefer to jump-start and use an expert.ai client package, you can get the Python or the Java client code from GitHub. Once you've the client code in your development environment, you can skip step 2 of this quick-start procedure and make the first call .","title":"1. Setup the client tools"},{"location":"quick-start/setup-client-tools/#1-setup-the-client-tools","text":"The Natural Language API can be used from a variety of programs and ready-to-use tools. For example, you can use utilities like curl and Postman or use one of the expert.ai client packages.","title":"1. Setup the client tools"},{"location":"quick-start/setup-client-tools/#get-curl","text":"If you prefer to make your first call from the command line, use curl. If you use Linux there's a chance that it is already installed on your computer, otherwise download it from the curl site . You are now ready for the next step: get the authorization token .","title":"Get curl"},{"location":"quick-start/setup-client-tools/#get-postman","text":"If you like Postman, you can make your first call with a predefined collection that you can also use to try all the other API's capabilities. Get the latest version of Postman from the Postman site and install the program following the instructions. Download the sample collection . Run Postman and import the sample collection. You are now ready for the next step: get the authorization token .","title":"Get Postman"},{"location":"quick-start/setup-client-tools/#get-an-expertai-client-package","text":"If you prefer to jump-start and use an expert.ai client package, you can get the Python or the Java client code from GitHub. Once you've the client code in your development environment, you can skip step 2 of this quick-start procedure and make the first call .","title":"Get an expert.ai client package"},{"location":"quick-start/token/","text":"2. Get the token You need a valid authorization token to use the API. To get the token you must have an expert.ai developer account. Visit the developer portal and sign up to get one. If you are going to make your first call using an interactive tool like curl or Postman, follow the instructions below to get the token by using the same tool. If, instead, you are going to make the first call using an expert.ai client package, you can go to the next step since the client is able to get the token by itself. Get the token with curl Linux To get the authorization token with curl, run the following command from a shell after replacing yourusername and yourpassword with your account credentials: curl -X POST https://developer.expert.ai/oauth2/token \\ -H 'Content-Type: application/json; charset=utf-8' \\ -d '{ \"username\": \"yourusername\", \"password\": \"yourpassword\" }' Windows To get the authorization token with curl, open a command prompt in the folder where you installed curl and run the following command after replacing yourusername and yourpassword with your account credentials: curl -X POST https://developer.expert.ai/oauth2/token -H \"Content-Type: application/json; charset=utf-8\" -d \"{\\\" username\\ \": \\\" yourusername\\ \", \\\" password\\ \": \\\" yourpassword\\ \"}\" The token is the command output, note it down or copy it to the clipboard. Get and save the token with Postman To obtain and save the token with Postman: Run Postman. Expand the expert.ai Natural Language API collection you previously imported . Select token . Edit the raw Body of the request replacing yourusername and yourpassword with your account credentials. Select Send in the right panel \u2014 the token will appear in the response. Select the expert.ai Natural Language API collection. Select the Variables tab. Set the current value of variable token to the actual authorization token. Select Save . The next step You are now ready to make your first call .","title":"2. Get the token"},{"location":"quick-start/token/#2-get-the-token","text":"You need a valid authorization token to use the API. To get the token you must have an expert.ai developer account. Visit the developer portal and sign up to get one. If you are going to make your first call using an interactive tool like curl or Postman, follow the instructions below to get the token by using the same tool. If, instead, you are going to make the first call using an expert.ai client package, you can go to the next step since the client is able to get the token by itself.","title":"2. Get the token"},{"location":"quick-start/token/#get-the-token-with-curl","text":"Linux To get the authorization token with curl, run the following command from a shell after replacing yourusername and yourpassword with your account credentials: curl -X POST https://developer.expert.ai/oauth2/token \\ -H 'Content-Type: application/json; charset=utf-8' \\ -d '{ \"username\": \"yourusername\", \"password\": \"yourpassword\" }' Windows To get the authorization token with curl, open a command prompt in the folder where you installed curl and run the following command after replacing yourusername and yourpassword with your account credentials: curl -X POST https://developer.expert.ai/oauth2/token -H \"Content-Type: application/json; charset=utf-8\" -d \"{\\\" username\\ \": \\\" yourusername\\ \", \\\" password\\ \": \\\" yourpassword\\ \"}\" The token is the command output, note it down or copy it to the clipboard.","title":"Get the token with curl"},{"location":"quick-start/token/#get-and-save-the-token-with-postman","text":"To obtain and save the token with Postman: Run Postman. Expand the expert.ai Natural Language API collection you previously imported . Select token . Edit the raw Body of the request replacing yourusername and yourpassword with your account credentials. Select Send in the right panel \u2014 the token will appear in the response. Select the expert.ai Natural Language API collection. Select the Variables tab. Set the current value of variable token to the actual authorization token. Select Save .","title":"Get and save the token with Postman"},{"location":"quick-start/token/#the-next-step","text":"You are now ready to make your first call .","title":"The next step"},{"location":"reference/","text":"REST interface reference Endpoints The addresses of the API resources Request format A description of the common request format HTTP status codes An explanation of the codes the API can return Output A description of the responses that can be obtained from API resources Positions How to interpret text blocks' positioning data Phrase types Phrase types detected by text subdivision Dependency representation How to represent syntactic analysis output Standard context topics Topics detected by keyphrase extraction Entity types The types recognized by entity recognition Categories Categories detected by classification expert.ai types Custom part-of-speech tags GeoJSON classification Use the GeoTax classification to obtain standard GIS data Emotional traits main groups How to obtain extra data about the main clusters of emotional traits The formal definition of the API's REST interface is available on the developer portal .","title":"Contents"},{"location":"reference/#rest-interface-reference","text":"Endpoints The addresses of the API resources Request format A description of the common request format HTTP status codes An explanation of the codes the API can return Output A description of the responses that can be obtained from API resources Positions How to interpret text blocks' positioning data Phrase types Phrase types detected by text subdivision Dependency representation How to represent syntactic analysis output Standard context topics Topics detected by keyphrase extraction Entity types The types recognized by entity recognition Categories Categories detected by classification expert.ai types Custom part-of-speech tags GeoJSON classification Use the GeoTax classification to obtain standard GIS data Emotional traits main groups How to obtain extra data about the main clusters of emotional traits The formal definition of the API's REST interface is available on the developer portal .","title":"REST interface reference"},{"location":"reference/categories/","text":"Categories Category properties Document classification returns categories drawn from the specified taxonomy. You can get the list of categories identified by each document classification resource by requesting the corresponding API self-documentation resource . Each category has two explicit properties, id and label . id is the identifying code, label is the description. Each category also has an implicit property which is its path within the taxonomy. The path is the sequence of categories that goes from the farthest ancestor to the category itself. For example, the path of the American black bear inside the animal kingdom \"category tree\" is: Eukarya Animalia Chordata Mammalia Carnivora Ursidae Ursus Ursus americanus If the category tree is flat\u2014it has only one hierarchical level\u2014the path coincides with the category itself. In the classification output, the path is returned in the hierarchy property. It is an array containing the values of the label property for all the categories along the path. Categories having the same value for the id property in different language versions of the same taxonomy are conceptually the same. For example, in the different versions of the geotax taxonomy for the five supported languages, the category for Cambodia has the same value for id , but different values for label : iptc taxonomy The properties of the categories of the iptc taxonomy reflect those of the Media Topics taxonomy. In particular, the id property corresponds to the numeric part of the Media Topics subject code, while the label property corresponds to its name. As mentioned above, the labels vary by language. Use the self-documentation resources to get the complete list of recognized categories. geotax taxonomy The categories of the geotax taxonomy correspond to all countries of the world. In the particular cases of United States of America and United Kingdom there are also categories corresponding to the different states or countries that make up the federation or kingdom. For example, there is both a category for the United Kingdom ( id = 184. ) and one for Wales ( id = 18404. ). In such cases, in the category tree the categories corresponding to member countries are nested by a level, that is, they are \"children\" of the categories corresponding to the federation or kingdom. for example: United Kingdom England Northern Ireland Scotland Wales All categories of the tree can be output. So for example in the case of this input text: He was born in 1930 in Cardiff. the output will be: id label 184. United Kingdom 18404. Wales As mentioned above, the labels of the categories vary by language. Always use the self-documentation resources to get the complete list of recognized categories. emotional-traits taxonomy These are the emotional-traits taxonomy category trees: id label (English) label (German) 300 Group Distress Gruppe Unbehagen 301 Disgust Ekel 302 Repulsion 311 Guilt Schuldgef\u00fchl 312 Shame Scham 313 Embarrassment Verlegenheit 322 Regret Bedauern 331 Boredom Langeweile 400 Group Resentment Gruppe Groll 402 Hatred Hass 403 Offence Beileidigung 411 Jealousy Eifersucht 412 Envy Neid 500 Group Dejection Gruppe Niedergeschlagenheit 501 Sadness Traurigkeit 502 Torment 503 Suffering Leiden 511 Disappointment Entt\u00e4uschung 512 Disillusion 513 Resignation Resignation 600 Group Surprise Gruppe \u00dcberraschung 601 Surprise \u00dcberraschung 700 Group Delight Gruppe Vergn\u00fcgen 701 Happiness Freude 702 Excitement Begeisterung 703 Joy 704 Amusement Belustigung 705 Well-Being Wohlsein 711 Satisfaction Zufriedenheit 721 Relief Erleichterung 800 Group Fondness Gruppe Sympathie 801 Like M\u00f6gen 802 Trust Vertrauen 803 Affection Zuneigung 804 Love Liebe 805 Passion Leidenschaft 812 Empathy Einf\u00fchlung 813 Compassion Mitgef\u00fchl Note You may notice that some categories in the tree for English do not have a correspondent in the tree for German. The reason is that in German the distinction between some categories is not as clear as in English, so it was chosen to collapse similar categories: 301 and 302 were collapsed into 301, 501 and 502 into 501, 511 and 512 into 511 and 701 and 703 in 701. The categories that can be returned in output\u2014the recognized emotional traits\u2014are only those at the 2nd level of the hierarchy, the \"leaves\" of the tree. The 1st level categories function as groups. The information of the group an emotion belongs to is available in output in the hierarchy property, which represents the full path of the output category inside the tree. For example: ... \"frequency\" : 50.26 , \"hierarchy\" : [ \"Group Delight\" , \"Amusement\" ] , \"id\" : \"0704\" , \"label\" : \"Amusement\" ... It is also possible to get the main groups as an additional output. behavioral-traits taxonomy The behavioral-traits taxonomy categories are: id label (English) label (German) 1000 Sociality Geselligkeit 1100 Sociality low Geselligkeit niedrig 1101 Asociality Ungeselligkeit 1102 Impoliteness Unh\u00f6flichkeit 1103 Ungratefulness Undankbarkeit 1104 Emotionality Empfindlichkeit 1105 Isolation Vereinsamung 1106 Disagreement Meinungsverschiedenheit 1200 Sociality fair Geselligkeit fair 1201 Seriousness Ernsthaftigkeit 1202 Introversion Introvertiertheit 1203 Unreservedness Unverbl\u00fcmtheit 1204 Humour Humor 1205 Sexuality Sexualit\u00e4t 1300 Sociality high Geselligkeit hoch 1301 Extroversion Extravertiertheit 1302 Pleasantness Freundlichkeit 1303 Trustfulness Zutraulichkeit 1304 Gratefulness Dankbarkeit 1305 Empathy Einf\u00fchlung 2000 Action Aktivit\u00e4t 2100 Action low Aktivit\u00e4t niedrig 2101 Sedentariness Faulheit 2102 Passivity Passivit\u00e4t 2200 Action fair Aktivit\u00e4t fair 2201 Calmness Gelassenheit 2300 Action high Aktivit\u00e4t hoch 2301 Initiative Tatendrang 2302 Dynamism Tatkraft 3000 Openness Aufgeschlossenheit 3100 Openness low Aufgeschlossenheit niedrig 3101 Rejection Ablehnung 3102 Apathy Gleichg\u00fcltigkeit 3103 Apprehension Besorgtheit 3104 Traditionalism Traditionalismus 3105 Conformism Konformismus 3106 Negativity Pessimismus 3107 Bias Voreingenommenheit 3200 Openness fair Aufgeschlossenheit fair 3201 Cautiousness Vorsichtigkeit 3300 Openness high Aufgeschlossenheit hoch 3301 Progressiveness Fortschrittlichkeit 3302 Acceptance Akzeptanz 3303 Courage Mut 3304 Positivity Optimismus 3305 Curiosity Neugier 4000 Consciousness Bewusstheit 4100 Consciousness low Bewusstheit niedrig 4101 Superficiality Oberfl\u00e4chlichkeit 4102 Unawareness Unwissenheit 4103 Disorganization Unordnung 4104 Insecurity Verunsicherung 4105 Ignorance Ignoranz 4106 Illusion Illusion 4300 Consciousness high Bewusstheit hoch 4301 Awareness Bewusstsein 4302 Spirituality Spiritualit\u00e4t 4303 Concern Besorgnis 4304 Knowledge Kenntnis 4305 Self-confidence Selbstbewusstsein 4306 Organization Ordnung 5000 Ethics Ethik 5100 Ethics low Ethik niedrig 5101 Violence Gewaltt\u00e4tigkeit 5102 Extremism Extremismus 5103 Discrimination Diskriminierung 5104 Dishonesty Unehrlichkeit 5105 Neglect Vernachl\u00e4ssigung 5106 Unlawfulness Ungesetzlichkeit 5107 Irresponsibility Verantwortungslosigkeit 5300 Ethics high Ethik hoch 5301 Inclusiveness Inklusion 5302 Honesty Ehrlichkeit 5303 Compassion Mitgef\u00fchl 5304 Commitment Engagement 5305 Lawfulness Gesetzlichkeit 5306 Solidarity Solidarit\u00e4t 6000 Capability Leistungsverm\u00f6gen 6100 Capability low Leistungsverm\u00f6gen niedrig 6101 Lack of intelligence Einf\u00e4ltigkeit 6102 Inexperience Unerfahrenheit 6103 Incompetence Unf\u00e4higkeit 6200 Capability fair Leistungsverm\u00f6gen fair 6201 Rationality Vern\u00fcnftigkeit 6300 Capability high Leistungsverm\u00f6gen hoch 6301 Smartness Klugheit 6302 Creativity Kreativit\u00e4t 6303 Competence Kompetenz 7000 Moderation Konsumverhalten 7100 Moderation low Konsumverhalten niedrig 7101 Dissoluteness Ausschweifung 7102 Gluttony Essgier 7103 Materialism Materialismus 7104 Addiction Sucht 7200 Moderation fair Konsumverhalten fair 7201 Healthy lifestyle Gesunde Lebensweise 7300 Moderation high Konsumverhalten hoch 7301 Self-restraint Selbstbeherrschung The categories that can be returned in output\u2014the recognized behavioral traits\u2014are only those at the 3rd level of the hierarchy, the \"leaves\" of the tree. The 1st and 2nd level categories are used to group the other. The information of the group and sub-group a personality trait belongs to is available in output in the hierarchy property, which represents the full path of the output category inside the tree. For example: ... \"frequency\" : 75.25 , \"hierarchy\" : [ \"Moderation\" , \"Moderation low\" , \"Gluttony\" ] , \"id\" : \"7102\" , \"label\" : \"Gluttony\" ...","title":"Categories"},{"location":"reference/categories/#categories","text":"","title":"Categories"},{"location":"reference/categories/#category-properties","text":"Document classification returns categories drawn from the specified taxonomy. You can get the list of categories identified by each document classification resource by requesting the corresponding API self-documentation resource . Each category has two explicit properties, id and label . id is the identifying code, label is the description. Each category also has an implicit property which is its path within the taxonomy. The path is the sequence of categories that goes from the farthest ancestor to the category itself. For example, the path of the American black bear inside the animal kingdom \"category tree\" is: Eukarya Animalia Chordata Mammalia Carnivora Ursidae Ursus Ursus americanus If the category tree is flat\u2014it has only one hierarchical level\u2014the path coincides with the category itself. In the classification output, the path is returned in the hierarchy property. It is an array containing the values of the label property for all the categories along the path. Categories having the same value for the id property in different language versions of the same taxonomy are conceptually the same. For example, in the different versions of the geotax taxonomy for the five supported languages, the category for Cambodia has the same value for id , but different values for label :","title":"Category properties"},{"location":"reference/categories/#iptc-taxonomy","text":"The properties of the categories of the iptc taxonomy reflect those of the Media Topics taxonomy. In particular, the id property corresponds to the numeric part of the Media Topics subject code, while the label property corresponds to its name. As mentioned above, the labels vary by language. Use the self-documentation resources to get the complete list of recognized categories.","title":"iptc taxonomy"},{"location":"reference/categories/#geotax-taxonomy","text":"The categories of the geotax taxonomy correspond to all countries of the world. In the particular cases of United States of America and United Kingdom there are also categories corresponding to the different states or countries that make up the federation or kingdom. For example, there is both a category for the United Kingdom ( id = 184. ) and one for Wales ( id = 18404. ). In such cases, in the category tree the categories corresponding to member countries are nested by a level, that is, they are \"children\" of the categories corresponding to the federation or kingdom. for example: United Kingdom England Northern Ireland Scotland Wales All categories of the tree can be output. So for example in the case of this input text: He was born in 1930 in Cardiff. the output will be: id label 184. United Kingdom 18404. Wales As mentioned above, the labels of the categories vary by language. Always use the self-documentation resources to get the complete list of recognized categories.","title":"geotax taxonomy"},{"location":"reference/categories/#emotional-traits-taxonomy","text":"These are the emotional-traits taxonomy category trees: id label (English) label (German) 300 Group Distress Gruppe Unbehagen 301 Disgust Ekel 302 Repulsion 311 Guilt Schuldgef\u00fchl 312 Shame Scham 313 Embarrassment Verlegenheit 322 Regret Bedauern 331 Boredom Langeweile 400 Group Resentment Gruppe Groll 402 Hatred Hass 403 Offence Beileidigung 411 Jealousy Eifersucht 412 Envy Neid 500 Group Dejection Gruppe Niedergeschlagenheit 501 Sadness Traurigkeit 502 Torment 503 Suffering Leiden 511 Disappointment Entt\u00e4uschung 512 Disillusion 513 Resignation Resignation 600 Group Surprise Gruppe \u00dcberraschung 601 Surprise \u00dcberraschung 700 Group Delight Gruppe Vergn\u00fcgen 701 Happiness Freude 702 Excitement Begeisterung 703 Joy 704 Amusement Belustigung 705 Well-Being Wohlsein 711 Satisfaction Zufriedenheit 721 Relief Erleichterung 800 Group Fondness Gruppe Sympathie 801 Like M\u00f6gen 802 Trust Vertrauen 803 Affection Zuneigung 804 Love Liebe 805 Passion Leidenschaft 812 Empathy Einf\u00fchlung 813 Compassion Mitgef\u00fchl Note You may notice that some categories in the tree for English do not have a correspondent in the tree for German. The reason is that in German the distinction between some categories is not as clear as in English, so it was chosen to collapse similar categories: 301 and 302 were collapsed into 301, 501 and 502 into 501, 511 and 512 into 511 and 701 and 703 in 701. The categories that can be returned in output\u2014the recognized emotional traits\u2014are only those at the 2nd level of the hierarchy, the \"leaves\" of the tree. The 1st level categories function as groups. The information of the group an emotion belongs to is available in output in the hierarchy property, which represents the full path of the output category inside the tree. For example: ... \"frequency\" : 50.26 , \"hierarchy\" : [ \"Group Delight\" , \"Amusement\" ] , \"id\" : \"0704\" , \"label\" : \"Amusement\" ... It is also possible to get the main groups as an additional output.","title":"emotional-traits taxonomy"},{"location":"reference/categories/#behavioral-traits-taxonomy","text":"The behavioral-traits taxonomy categories are: id label (English) label (German) 1000 Sociality Geselligkeit 1100 Sociality low Geselligkeit niedrig 1101 Asociality Ungeselligkeit 1102 Impoliteness Unh\u00f6flichkeit 1103 Ungratefulness Undankbarkeit 1104 Emotionality Empfindlichkeit 1105 Isolation Vereinsamung 1106 Disagreement Meinungsverschiedenheit 1200 Sociality fair Geselligkeit fair 1201 Seriousness Ernsthaftigkeit 1202 Introversion Introvertiertheit 1203 Unreservedness Unverbl\u00fcmtheit 1204 Humour Humor 1205 Sexuality Sexualit\u00e4t 1300 Sociality high Geselligkeit hoch 1301 Extroversion Extravertiertheit 1302 Pleasantness Freundlichkeit 1303 Trustfulness Zutraulichkeit 1304 Gratefulness Dankbarkeit 1305 Empathy Einf\u00fchlung 2000 Action Aktivit\u00e4t 2100 Action low Aktivit\u00e4t niedrig 2101 Sedentariness Faulheit 2102 Passivity Passivit\u00e4t 2200 Action fair Aktivit\u00e4t fair 2201 Calmness Gelassenheit 2300 Action high Aktivit\u00e4t hoch 2301 Initiative Tatendrang 2302 Dynamism Tatkraft 3000 Openness Aufgeschlossenheit 3100 Openness low Aufgeschlossenheit niedrig 3101 Rejection Ablehnung 3102 Apathy Gleichg\u00fcltigkeit 3103 Apprehension Besorgtheit 3104 Traditionalism Traditionalismus 3105 Conformism Konformismus 3106 Negativity Pessimismus 3107 Bias Voreingenommenheit 3200 Openness fair Aufgeschlossenheit fair 3201 Cautiousness Vorsichtigkeit 3300 Openness high Aufgeschlossenheit hoch 3301 Progressiveness Fortschrittlichkeit 3302 Acceptance Akzeptanz 3303 Courage Mut 3304 Positivity Optimismus 3305 Curiosity Neugier 4000 Consciousness Bewusstheit 4100 Consciousness low Bewusstheit niedrig 4101 Superficiality Oberfl\u00e4chlichkeit 4102 Unawareness Unwissenheit 4103 Disorganization Unordnung 4104 Insecurity Verunsicherung 4105 Ignorance Ignoranz 4106 Illusion Illusion 4300 Consciousness high Bewusstheit hoch 4301 Awareness Bewusstsein 4302 Spirituality Spiritualit\u00e4t 4303 Concern Besorgnis 4304 Knowledge Kenntnis 4305 Self-confidence Selbstbewusstsein 4306 Organization Ordnung 5000 Ethics Ethik 5100 Ethics low Ethik niedrig 5101 Violence Gewaltt\u00e4tigkeit 5102 Extremism Extremismus 5103 Discrimination Diskriminierung 5104 Dishonesty Unehrlichkeit 5105 Neglect Vernachl\u00e4ssigung 5106 Unlawfulness Ungesetzlichkeit 5107 Irresponsibility Verantwortungslosigkeit 5300 Ethics high Ethik hoch 5301 Inclusiveness Inklusion 5302 Honesty Ehrlichkeit 5303 Compassion Mitgef\u00fchl 5304 Commitment Engagement 5305 Lawfulness Gesetzlichkeit 5306 Solidarity Solidarit\u00e4t 6000 Capability Leistungsverm\u00f6gen 6100 Capability low Leistungsverm\u00f6gen niedrig 6101 Lack of intelligence Einf\u00e4ltigkeit 6102 Inexperience Unerfahrenheit 6103 Incompetence Unf\u00e4higkeit 6200 Capability fair Leistungsverm\u00f6gen fair 6201 Rationality Vern\u00fcnftigkeit 6300 Capability high Leistungsverm\u00f6gen hoch 6301 Smartness Klugheit 6302 Creativity Kreativit\u00e4t 6303 Competence Kompetenz 7000 Moderation Konsumverhalten 7100 Moderation low Konsumverhalten niedrig 7101 Dissoluteness Ausschweifung 7102 Gluttony Essgier 7103 Materialism Materialismus 7104 Addiction Sucht 7200 Moderation fair Konsumverhalten fair 7201 Healthy lifestyle Gesunde Lebensweise 7300 Moderation high Konsumverhalten hoch 7301 Self-restraint Selbstbeherrschung The categories that can be returned in output\u2014the recognized behavioral traits\u2014are only those at the 3rd level of the hierarchy, the \"leaves\" of the tree. The 1st and 2nd level categories are used to group the other. The information of the group and sub-group a personality trait belongs to is available in output in the hierarchy property, which represents the full path of the output category inside the tree. For example: ... \"frequency\" : 75.25 , \"hierarchy\" : [ \"Moderation\" , \"Moderation low\" , \"Gluttony\" ] , \"id\" : \"7102\" , \"label\" : \"Gluttony\" ...","title":"behavioral-traits taxonomy"},{"location":"reference/dependency-representation/","text":"Dependency representation Here is an example of representing dependencies\u2014the output of syntactic analysis \u2014with arrow arcs. If you perform the deep linguistic analysis of this sentence: The company has developed an entirely new category of products. you'll get a response like this: { \"success\" : true , \"data\" : { \"content\" : \"The company has developed an entirely new category of products.\" , \"language\" : \"EN\" , \"tokens\" : [ { \"syncon\" : \"noun.concepts.cause\" , \"start\" : 0 , \"end\" : 3 , \"type\" : \"ART\" , \"lemma\" : \"The\" , \"pos\" : \"DET\" , \"id\" : 0 , \"head\" : 1 , \"dep\" : \"det\" }, { \"syncon\" : \"noun.organization.company\" , \"start\" : 4 , \"end\" : 11 , \"type\" : \"NOU\" , \"lemma\" : \"company\" , \"pos\" : \"NOUN\" , \"id\" : 1 , \"head\" : 3 , \"dep\" : \"nsubj\" }, { \"syncon\" : \"noun.concepts.cause\" , \"start\" : 12 , \"end\" : 15 , \"type\" : \"AUX\" , \"lemma\" : \"has\" , \"pos\" : \"AUX\" , \"id\" : 2 , \"head\" : 3 , \"dep\" : \"aux\" }, { \"syncon\" : \"verb.general_action.develop\" , \"start\" : 16 , \"end\" : 25 , \"type\" : \"VER\" , \"lemma\" : \"develop\" , \"pos\" : \"VERB\" , \"id\" : 3 , \"head\" : 3 , \"dep\" : \"root\" }, { \"syncon\" : \"noun.concepts.cause\" , \"start\" : 26 , \"end\" : 28 , \"type\" : \"ART\" , \"lemma\" : \"an\" , \"pos\" : \"DET\" , \"id\" : 4 , \"head\" : 7 , \"dep\" : \"det\" }, { \"syncon\" : \"adv.manner.fully\" , \"start\" : 29 , \"end\" : 37 , \"type\" : \"ADV\" , \"lemma\" : \"entirely\" , \"pos\" : \"ADV\" , \"id\" : 5 , \"head\" : 7 , \"dep\" : \"advmod\" }, { \"syncon\" : \"adj.new\" , \"start\" : 38 , \"end\" : 41 , \"type\" : \"ADJ\" , \"lemma\" : \"new\" , \"pos\" : \"ADJ\" , \"id\" : 6 , \"head\" : 7 , \"dep\" : \"amod\" }, { \"syncon\" : \"noun.object_group.category\" , \"start\" : 42 , \"end\" : 50 , \"type\" : \"NOU\" , \"lemma\" : \"category\" , \"pos\" : \"NOUN\" , \"id\" : 7 , \"head\" : 3 , \"dep\" : \"obj\" }, { \"syncon\" : \"noun.concepts.cause\" , \"start\" : 51 , \"end\" : 53 , \"type\" : \"PRE\" , \"lemma\" : \"of\" , \"pos\" : \"ADP\" , \"id\" : 8 , \"head\" : 9 , \"dep\" : \"case\" }, { \"syncon\" : \"noun.artifact.products\" , \"start\" : 54 , \"end\" : 62 , \"type\" : \"NOU\" , \"lemma\" : \"product\" , \"pos\" : \"NOUN\" , \"id\" : 9 , \"head\" : 7 , \"dep\" : \"nmod\" }, { \"syncon\" : \"noun.concepts.cause\" , \"start\" : 62 , \"end\" : 63 , \"type\" : \"PNT\" , \"lemma\" : \".\" , \"pos\" : \"PUNCT\" , \"id\" : 10 , \"head\" : 3 , \"dep\" : \"punct\" } ], \"phrases\" : [ { \"tokens\" : [ 0 , 1 ], \"type\" : \"NP\" , \"start\" : 0 , \"end\" : 11 }, { \"tokens\" : [ 2 , 3 ], \"type\" : \"VP\" , \"start\" : 12 , \"end\" : 25 }, { \"tokens\" : [ 4 , 5 , 6 , 7 ], \"type\" : \"NP\" , \"start\" : 26 , \"end\" : 50 }, { \"tokens\" : [ 8 , 9 ], \"type\" : \"PP\" , \"start\" : 51 , \"end\" : 62 }, { \"tokens\" : [ 10 ], \"type\" : \"NA\" , \"start\" : 62 , \"end\" : 63 }, { \"tokens\" : [], \"type\" : \"CR\" , \"start\" : 63 , \"end\" : 63 } ], \"sentences\" : [ { \"phrases\" : [ 0 , 1 , 2 , 3 , 4 , 5 ], \"start\" : 0 , \"end\" : 63 } ], \"paragraphs\" : [ { \"sentences\" : [ 0 ], \"start\" : 0 , \"end\" : 63 } ] } } Use the tokens array because it contains the dependency structure of the sentence. Create a box for each token. Label the box with the token text. This can be extracted from the value of the content property of the outer data object using the start and the end properties of the token. To decorate the box: You can assign a color corresponding to the part-of-speech, that is the value of the pos property. For example, you can use pink for verbs, light blue for nouns, etc. You can also put the part-of-speech tag as an additional label inside the box. You can create a tooltip with the token's lemma property. Put a special mark over the token with the property dep set to root For all the non-root tokens, draw a dependency arc starting from the box of the token which id property value is equal to the value of head property. For example, if the current token has head = 3, start drawing the arc from the box of the token with id = 3 and put the arrow head in the current token's box. Decorate the arcs with the names of the dependencies you find in the dep property and you're done.","title":"Dependency representation"},{"location":"reference/dependency-representation/#dependency-representation","text":"Here is an example of representing dependencies\u2014the output of syntactic analysis \u2014with arrow arcs. If you perform the deep linguistic analysis of this sentence: The company has developed an entirely new category of products. you'll get a response like this: { \"success\" : true , \"data\" : { \"content\" : \"The company has developed an entirely new category of products.\" , \"language\" : \"EN\" , \"tokens\" : [ { \"syncon\" : \"noun.concepts.cause\" , \"start\" : 0 , \"end\" : 3 , \"type\" : \"ART\" , \"lemma\" : \"The\" , \"pos\" : \"DET\" , \"id\" : 0 , \"head\" : 1 , \"dep\" : \"det\" }, { \"syncon\" : \"noun.organization.company\" , \"start\" : 4 , \"end\" : 11 , \"type\" : \"NOU\" , \"lemma\" : \"company\" , \"pos\" : \"NOUN\" , \"id\" : 1 , \"head\" : 3 , \"dep\" : \"nsubj\" }, { \"syncon\" : \"noun.concepts.cause\" , \"start\" : 12 , \"end\" : 15 , \"type\" : \"AUX\" , \"lemma\" : \"has\" , \"pos\" : \"AUX\" , \"id\" : 2 , \"head\" : 3 , \"dep\" : \"aux\" }, { \"syncon\" : \"verb.general_action.develop\" , \"start\" : 16 , \"end\" : 25 , \"type\" : \"VER\" , \"lemma\" : \"develop\" , \"pos\" : \"VERB\" , \"id\" : 3 , \"head\" : 3 , \"dep\" : \"root\" }, { \"syncon\" : \"noun.concepts.cause\" , \"start\" : 26 , \"end\" : 28 , \"type\" : \"ART\" , \"lemma\" : \"an\" , \"pos\" : \"DET\" , \"id\" : 4 , \"head\" : 7 , \"dep\" : \"det\" }, { \"syncon\" : \"adv.manner.fully\" , \"start\" : 29 , \"end\" : 37 , \"type\" : \"ADV\" , \"lemma\" : \"entirely\" , \"pos\" : \"ADV\" , \"id\" : 5 , \"head\" : 7 , \"dep\" : \"advmod\" }, { \"syncon\" : \"adj.new\" , \"start\" : 38 , \"end\" : 41 , \"type\" : \"ADJ\" , \"lemma\" : \"new\" , \"pos\" : \"ADJ\" , \"id\" : 6 , \"head\" : 7 , \"dep\" : \"amod\" }, { \"syncon\" : \"noun.object_group.category\" , \"start\" : 42 , \"end\" : 50 , \"type\" : \"NOU\" , \"lemma\" : \"category\" , \"pos\" : \"NOUN\" , \"id\" : 7 , \"head\" : 3 , \"dep\" : \"obj\" }, { \"syncon\" : \"noun.concepts.cause\" , \"start\" : 51 , \"end\" : 53 , \"type\" : \"PRE\" , \"lemma\" : \"of\" , \"pos\" : \"ADP\" , \"id\" : 8 , \"head\" : 9 , \"dep\" : \"case\" }, { \"syncon\" : \"noun.artifact.products\" , \"start\" : 54 , \"end\" : 62 , \"type\" : \"NOU\" , \"lemma\" : \"product\" , \"pos\" : \"NOUN\" , \"id\" : 9 , \"head\" : 7 , \"dep\" : \"nmod\" }, { \"syncon\" : \"noun.concepts.cause\" , \"start\" : 62 , \"end\" : 63 , \"type\" : \"PNT\" , \"lemma\" : \".\" , \"pos\" : \"PUNCT\" , \"id\" : 10 , \"head\" : 3 , \"dep\" : \"punct\" } ], \"phrases\" : [ { \"tokens\" : [ 0 , 1 ], \"type\" : \"NP\" , \"start\" : 0 , \"end\" : 11 }, { \"tokens\" : [ 2 , 3 ], \"type\" : \"VP\" , \"start\" : 12 , \"end\" : 25 }, { \"tokens\" : [ 4 , 5 , 6 , 7 ], \"type\" : \"NP\" , \"start\" : 26 , \"end\" : 50 }, { \"tokens\" : [ 8 , 9 ], \"type\" : \"PP\" , \"start\" : 51 , \"end\" : 62 }, { \"tokens\" : [ 10 ], \"type\" : \"NA\" , \"start\" : 62 , \"end\" : 63 }, { \"tokens\" : [], \"type\" : \"CR\" , \"start\" : 63 , \"end\" : 63 } ], \"sentences\" : [ { \"phrases\" : [ 0 , 1 , 2 , 3 , 4 , 5 ], \"start\" : 0 , \"end\" : 63 } ], \"paragraphs\" : [ { \"sentences\" : [ 0 ], \"start\" : 0 , \"end\" : 63 } ] } } Use the tokens array because it contains the dependency structure of the sentence. Create a box for each token. Label the box with the token text. This can be extracted from the value of the content property of the outer data object using the start and the end properties of the token. To decorate the box: You can assign a color corresponding to the part-of-speech, that is the value of the pos property. For example, you can use pink for verbs, light blue for nouns, etc. You can also put the part-of-speech tag as an additional label inside the box. You can create a tooltip with the token's lemma property. Put a special mark over the token with the property dep set to root For all the non-root tokens, draw a dependency arc starting from the box of the token which id property value is equal to the value of head property. For example, if the current token has head = 3, start drawing the arc from the box of the token with id = 3 and put the arrow head in the current token's box. Decorate the arcs with the names of the dependencies you find in the dep property and you're done.","title":"Dependency representation"},{"location":"reference/emotional-traits-main-groups/","text":"Main groups of emotional traits The emotional-traits document classification resources can be used in a standard way, composing their endpoints as indicated in this reference section article . In this case, the output of the resource is also standard. It is also possible to obtain extra output containing the main groups of emotional traits . To achieve this you need to add a query string parameter to the resource endpoint. The parameter is called features and must be set to extradata . For example, this is the resource endpoint for English with the addition of the parameter: https://nlapi.expert.ai/v2/categorize/emotional-traits/en ?features=extradata The resulting output has this format: { \"success\": Boolean success flag , \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , \"categories\": as in the standard classification output , \"extraData\": { \"groups\": Main group(s) data } } } It is the standard classification output with the addition of the extraData object which in turn contains the main groups data. The groups property is an array, each item of which is an object representing a main group, for example: \"extraData\" : { \"groups\" : [ { \"id\" : \"0500\" , \"label\" : \"Group Dejection\" , \"position\" : 1 } ] } These are the properties returned for each group: Description Property name in the JSON object Identification number or name of the group category in the category tree id Group category description label Ranking of the group position The group with the highest rank has position set to 1. Other groups, when present, have consecutive values.","title":"Main groups of emotional traits"},{"location":"reference/emotional-traits-main-groups/#main-groups-of-emotional-traits","text":"The emotional-traits document classification resources can be used in a standard way, composing their endpoints as indicated in this reference section article . In this case, the output of the resource is also standard. It is also possible to obtain extra output containing the main groups of emotional traits . To achieve this you need to add a query string parameter to the resource endpoint. The parameter is called features and must be set to extradata . For example, this is the resource endpoint for English with the addition of the parameter: https://nlapi.expert.ai/v2/categorize/emotional-traits/en ?features=extradata The resulting output has this format: { \"success\": Boolean success flag , \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , \"categories\": as in the standard classification output , \"extraData\": { \"groups\": Main group(s) data } } } It is the standard classification output with the addition of the extraData object which in turn contains the main groups data. The groups property is an array, each item of which is an object representing a main group, for example: \"extraData\" : { \"groups\" : [ { \"id\" : \"0500\" , \"label\" : \"Group Dejection\" , \"position\" : 1 } ] } These are the properties returned for each group: Description Property name in the JSON object Identification number or name of the group category in the category tree id Group category description label Ranking of the group position The group with the highest rank has position set to 1. Other groups, when present, have consecutive values.","title":"Main groups of emotional traits"},{"location":"reference/endpoints/","text":"REST interface endpoints Common format The endpoints of the REST interface are the Web addresses of the API resources. The addresses of document analysis, document classification and information detection resources must be requested with the HTTP POST method by sending the document to be processed. The addresses of self-documentation resources must be requested with the HTTP GET method without sending any data. All endpoints share this format: https://nlapi.expert.ai/v2/ resource path Resources' output is documented in a specific sub-section of this manual. Document analysis resources Full document analysis resources have paths like this: analyze / context name / language The boxed parts are placeholders: context name must be replaced with the name of the context language code must be replaced with the ISO 639-1 language code For example, this resource: https://nlapi.expert.ai/v2/ analyze/standard/en performs the full analysis of the English text submitted as POST data using the standard context. Partial analysis resources paths have a third parameter, the analysis name: analyze / context / language / analysis For example, this resource: https://nlapi.expert.ai/v2/ analyze/standard/en/disambiguation performs only the deep linguistic analysis of an English text. The mapping between API capabilities and analysis names follows: Capability Analysis name Example path for standard context and English language Full document analysis n/a analyze/standard/en Deep linguistic analysis disambiguation analyze/standard/en/disambiguaton Keyphrase extraction relevants analyze/standard/en/relevants Named entities recognition entities analyze/standard/en/entities Relation extraction relations analyze/standard/en/relations Sentiment analysis sentiment analyze/standard/en/sentiment Available languages \u200b\u200bdepend on the context . Some examples of document analysis endpoints follow. Full analysis of a Spanish text with the standard context: https://nlapi.expert.ai/v2/analyze/standard/es Named entity recognition on a French text with the standard context: https://nlapi.expert.ai/v2/analyze/standard/fr/entities Keyphrase extraction on an Italian text with the standard context: https://nlapi.expert.ai/v2/analyze/standard/it/relevants Document classification resources The path of document classification resources has this format: categorize / taxonomy name / language code The boxed parts are placeholders: taxonomy name must be replaced with the name of the taxonomy language code must be replaced with the ISO 639-1 language code For example: https://nlapi.expert.ai/v2/ categorize/iptc/en is the resource that classifies English texts according to the iptc taxonomy. Available languages \u200b\u200bdepend on the taxonomy . Other examples of document classification endpoints: Classification of a Spanish text with the iptc taxonomy: https://nlapi.expert.ai/v2/categorize/iptc/es Classification of a German text with the geotax taxonomy: https://nlapi.expert.ai/v2/categorize/geotax/de Information detection resources The path of information detection resources has this format: detect / detector naame / language code The boxed parts are placeholders: detector name must be replaced with the name of the detector language code must be replaced with the ISO 639-1 language code For example: https://nlapi.expert.ai/v2/ detect/pii/en is the resource that detects information in English texts using the pii detector. Available languages \u200b\u200bdepend on the detector . Self-documentation resources contexts The contexts resource returns information about the contexts that can be used for document analysis. It must be requested with the GET method and has this path: contexts taxonomies The taxonomies resource returns the list of the taxonomies that can be used for document classification. It must be requested with the GET method and has this path: taxonomies taxonomies child resources The resource that returns the category \u200btree \u200bfor a given taxonomy in a given language must be requested with the GET method and has this path: taxonomies/ taxonomy name / language code The boxed parts are placeholders: taxonomy name must be replaced with the name of the taxonomy language code must be replaced with the ISO 639-1 language code For example, this resource: https://nlapi.expert.ai/v2/ taxonomies/iptc/en returns the categories' tree of the English iptc taxonomy. detectors The detectors resource returns the list of the detectors that can be used for information detection. It must be requested with the GET method and has this path: detectors","title":"Endpoints"},{"location":"reference/endpoints/#rest-interface-endpoints","text":"","title":"REST interface endpoints"},{"location":"reference/endpoints/#common-format","text":"The endpoints of the REST interface are the Web addresses of the API resources. The addresses of document analysis, document classification and information detection resources must be requested with the HTTP POST method by sending the document to be processed. The addresses of self-documentation resources must be requested with the HTTP GET method without sending any data. All endpoints share this format: https://nlapi.expert.ai/v2/ resource path Resources' output is documented in a specific sub-section of this manual.","title":"Common format"},{"location":"reference/endpoints/#document-analysis-resources","text":"Full document analysis resources have paths like this: analyze / context name / language The boxed parts are placeholders: context name must be replaced with the name of the context language code must be replaced with the ISO 639-1 language code For example, this resource: https://nlapi.expert.ai/v2/ analyze/standard/en performs the full analysis of the English text submitted as POST data using the standard context. Partial analysis resources paths have a third parameter, the analysis name: analyze / context / language / analysis For example, this resource: https://nlapi.expert.ai/v2/ analyze/standard/en/disambiguation performs only the deep linguistic analysis of an English text. The mapping between API capabilities and analysis names follows: Capability Analysis name Example path for standard context and English language Full document analysis n/a analyze/standard/en Deep linguistic analysis disambiguation analyze/standard/en/disambiguaton Keyphrase extraction relevants analyze/standard/en/relevants Named entities recognition entities analyze/standard/en/entities Relation extraction relations analyze/standard/en/relations Sentiment analysis sentiment analyze/standard/en/sentiment Available languages \u200b\u200bdepend on the context . Some examples of document analysis endpoints follow. Full analysis of a Spanish text with the standard context: https://nlapi.expert.ai/v2/analyze/standard/es Named entity recognition on a French text with the standard context: https://nlapi.expert.ai/v2/analyze/standard/fr/entities Keyphrase extraction on an Italian text with the standard context: https://nlapi.expert.ai/v2/analyze/standard/it/relevants","title":"Document analysis resources"},{"location":"reference/endpoints/#document-classification-resources","text":"The path of document classification resources has this format: categorize / taxonomy name / language code The boxed parts are placeholders: taxonomy name must be replaced with the name of the taxonomy language code must be replaced with the ISO 639-1 language code For example: https://nlapi.expert.ai/v2/ categorize/iptc/en is the resource that classifies English texts according to the iptc taxonomy. Available languages \u200b\u200bdepend on the taxonomy . Other examples of document classification endpoints: Classification of a Spanish text with the iptc taxonomy: https://nlapi.expert.ai/v2/categorize/iptc/es Classification of a German text with the geotax taxonomy: https://nlapi.expert.ai/v2/categorize/geotax/de","title":"Document classification resources"},{"location":"reference/endpoints/#information-detection-resources","text":"The path of information detection resources has this format: detect / detector naame / language code The boxed parts are placeholders: detector name must be replaced with the name of the detector language code must be replaced with the ISO 639-1 language code For example: https://nlapi.expert.ai/v2/ detect/pii/en is the resource that detects information in English texts using the pii detector. Available languages \u200b\u200bdepend on the detector .","title":"Information detection resources"},{"location":"reference/endpoints/#self-documentation-resources","text":"","title":"Self-documentation resources"},{"location":"reference/endpoints/#contexts","text":"The contexts resource returns information about the contexts that can be used for document analysis. It must be requested with the GET method and has this path: contexts","title":"contexts"},{"location":"reference/endpoints/#taxonomies","text":"The taxonomies resource returns the list of the taxonomies that can be used for document classification. It must be requested with the GET method and has this path: taxonomies","title":"taxonomies"},{"location":"reference/endpoints/#taxonomies-child-resources","text":"The resource that returns the category \u200btree \u200bfor a given taxonomy in a given language must be requested with the GET method and has this path: taxonomies/ taxonomy name / language code The boxed parts are placeholders: taxonomy name must be replaced with the name of the taxonomy language code must be replaced with the ISO 639-1 language code For example, this resource: https://nlapi.expert.ai/v2/ taxonomies/iptc/en returns the categories' tree of the English iptc taxonomy.","title":"taxonomies child resources"},{"location":"reference/endpoints/#detectors","text":"The detectors resource returns the list of the detectors that can be used for information detection. It must be requested with the GET method and has this path: detectors","title":"detectors"},{"location":"reference/entity-types/","text":"Entity types Here is the list of possible types that can be returned by named entity recognition and relation extraction . Uppercase labels are used for named entities. If the label is lowercase, and this can be given in the attributes of named entities or in the elements related to the verb in a relation, it means it is a generic entity. For example, in the text Felipe is a florist. florist can be labeled nph because a florist is a person, but not a specific one. Felipe , on the other hand, is a specific person identified by a proper name, so he is labeled with NPH . Label Description Example ADR Street address Who lived at 221B Baker Street ? ANM Animal Felix is an anthropomorphic black cat. BLD Building While in London I attended a concert at the Royal Albert Hall . COM Company, business Tesla Inc. sold 10% of its Bitcoin holdings. DAT Date Napoleon died on May 5, 1821 . DEV Device My new Galaxy smartphone has seven cameras. DOC Document I appeal to the Geneva Convention ! EVN Event Felice Gimondi won the Tour de France in 1965. FDD Food, beverage Frank likes to drink Guinness beer. GEA Physical geographic feature I crossed the Mississipi river with my boat GEO Administrative geographic area Alaska is the least densely populated state in the United States . GEX Extended geography The astronauts have landed on Mars . HOU Hours The eclipse reached its peak at 3pm . LEN Legal entity Of course I pay the FICA tax. MAI Email address For any questions do not hesitate to write to helpme@somedomain.com . MEA Measure The chest is five feet wide and 40 inches tall. MMD Mass media I read it in the Guardian . MON Money I sold half of my stock and made six hundred thousand dollars . NPH Person Hakeem Olajuwon dunked effortlessly. NPR Unrecognized entity with a proper noun I like GYYYJJJ7 soooo much! ORG Organization, institution, society Now they threaten to quit the United Nations if they are not heard. PCT Percentage The richest 10% of adults in the world own 85% of global wealth. PHO Phone number For poor database design, call (214) 748-3647 . PPH Physical phenomena The COVID-19 infection is slowing down. PRD Product The Rolex Daytona is an wonderful watch. VCL Vehicle A Ferrari 250 GTO was the most expensive car ever sold. WEB Web address Find the best technical documentation at docs.expert.ai . WRK Work of human intelligence Grease is a funny musical romantic comedy.","title":"Entity types"},{"location":"reference/entity-types/#entity-types","text":"Here is the list of possible types that can be returned by named entity recognition and relation extraction . Uppercase labels are used for named entities. If the label is lowercase, and this can be given in the attributes of named entities or in the elements related to the verb in a relation, it means it is a generic entity. For example, in the text Felipe is a florist. florist can be labeled nph because a florist is a person, but not a specific one. Felipe , on the other hand, is a specific person identified by a proper name, so he is labeled with NPH . Label Description Example ADR Street address Who lived at 221B Baker Street ? ANM Animal Felix is an anthropomorphic black cat. BLD Building While in London I attended a concert at the Royal Albert Hall . COM Company, business Tesla Inc. sold 10% of its Bitcoin holdings. DAT Date Napoleon died on May 5, 1821 . DEV Device My new Galaxy smartphone has seven cameras. DOC Document I appeal to the Geneva Convention ! EVN Event Felice Gimondi won the Tour de France in 1965. FDD Food, beverage Frank likes to drink Guinness beer. GEA Physical geographic feature I crossed the Mississipi river with my boat GEO Administrative geographic area Alaska is the least densely populated state in the United States . GEX Extended geography The astronauts have landed on Mars . HOU Hours The eclipse reached its peak at 3pm . LEN Legal entity Of course I pay the FICA tax. MAI Email address For any questions do not hesitate to write to helpme@somedomain.com . MEA Measure The chest is five feet wide and 40 inches tall. MMD Mass media I read it in the Guardian . MON Money I sold half of my stock and made six hundred thousand dollars . NPH Person Hakeem Olajuwon dunked effortlessly. NPR Unrecognized entity with a proper noun I like GYYYJJJ7 soooo much! ORG Organization, institution, society Now they threaten to quit the United Nations if they are not heard. PCT Percentage The richest 10% of adults in the world own 85% of global wealth. PHO Phone number For poor database design, call (214) 748-3647 . PPH Physical phenomena The COVID-19 infection is slowing down. PRD Product The Rolex Daytona is an wonderful watch. VCL Vehicle A Ferrari 250 GTO was the most expensive car ever sold. WEB Web address Find the best technical documentation at docs.expert.ai . WRK Work of human intelligence Grease is a funny musical romantic comedy.","title":"Entity types"},{"location":"reference/expert-ai-types/","text":"expert.ai types The following table lists the possible custom type labels that can be assigned to tokens and atoms by custom part-of-speech tagging . Label Description ADJ Adjective ADV Adverb ART Article AUX Auxiliary verb CON Conjunction NOU Noun NOU.ADR Street address NOU.DAT Date NOU.HOU Hour NOU.MAI Email address NOU.MEA Measure NOU.MON Money NOU.PCT Percentage NOU.PHO Phone number NOU.WEB Web address NPR Proper noun NPR.ANM Proper noun of an animal NPR.BLD Proper noun of a building NPR.COM Proper noun of a business/company NPR.DEV Proper noun of a device NPR.DOC Proper noun of a document NPR.EVN Proper noun of an event NPR.FDD Proper noun of a food/beverage NPR.GEA Proper noun of a physical geographic feature NPR.GEO Proper noun of an administrative geographic area NPR.GEX Proper noun of an extra-terrestrial or imaginary place NPR.LEN Proper noun of a legal/fiscal entity NPR.MMD Proper noun of a mass media NPR.NPH Proper noun of a human being NPR.ORG Proper noun of an organization/society/institution NPR.PPH Proper noun of a physical phenomena NPR.PRD Proper noun of a product NPR.VCL Proper noun of a vehicle NPR.WRK Proper noun of a work of human intelligence PNT Punctuation mark PRE Preposition PRO Pronoun PRT Particle VER Verb","title":"expert.ai types"},{"location":"reference/expert-ai-types/#expertai-types","text":"The following table lists the possible custom type labels that can be assigned to tokens and atoms by custom part-of-speech tagging . Label Description ADJ Adjective ADV Adverb ART Article AUX Auxiliary verb CON Conjunction NOU Noun NOU.ADR Street address NOU.DAT Date NOU.HOU Hour NOU.MAI Email address NOU.MEA Measure NOU.MON Money NOU.PCT Percentage NOU.PHO Phone number NOU.WEB Web address NPR Proper noun NPR.ANM Proper noun of an animal NPR.BLD Proper noun of a building NPR.COM Proper noun of a business/company NPR.DEV Proper noun of a device NPR.DOC Proper noun of a document NPR.EVN Proper noun of an event NPR.FDD Proper noun of a food/beverage NPR.GEA Proper noun of a physical geographic feature NPR.GEO Proper noun of an administrative geographic area NPR.GEX Proper noun of an extra-terrestrial or imaginary place NPR.LEN Proper noun of a legal/fiscal entity NPR.MMD Proper noun of a mass media NPR.NPH Proper noun of a human being NPR.ORG Proper noun of an organization/society/institution NPR.PPH Proper noun of a physical phenomena NPR.PRD Proper noun of a product NPR.VCL Proper noun of a vehicle NPR.WRK Proper noun of a work of human intelligence PNT Punctuation mark PRE Preposition PRO Pronoun PRT Particle VER Verb","title":"expert.ai types"},{"location":"reference/geojson/","text":"GeoJSON classification The geotax document classification resources can be used in a standard way, composing their endpoints as indicated in the article in the reference section. In this case, the output of the resource is also standard. It is also possible to obtain an output containing standard GeoJSON data, which is useful for locating identified countries in GIS systems. To achieve this you need to add a query string parameter to the resource endpoint. The parameter is called features and must be set to extradata . For example, this is the resource endpoint for English with the addition of the parameter: https://nlapi.expert.ai/v2/categorize/geotax/en?features=extradata The resulting output has this format: { \"success\": Boolean success flag , \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , \"categories\": as in the standard classification output , \"extraData\": { \"geojson\": GeoJSON data } } } It is the standard classification output with the addition of the extraData object which in turn contains the geojson object. The GeoJSON data contains identification data for the countries and the coordinates of their geographic center, for example: ... \"content\" : \"Last year I moved from San Francisco to New York.\" , \"extraData\" : { \"geojson\" : { \"features\" : [ { \"geometry\" : { \"coordinates\" : [ -98.57 , 39.82 ], \"type\" : \"Point\" }, \"properties\" : { \"geonameId\" : \"6252001\" , \"id\" : \"/185.\" , \"name\" : \"/United States of America\" }, \"type\" : \"Feature\" }, { \"geometry\" : { \"coordinates\" : [ -120 , 37 ], \"type\" : \"Point\" }, \"properties\" : { \"geonameId\" : \"5332921\" , \"id\" : \"/185./18505.\" , \"name\" : \"/United States of America/California\" }, \"type\" : \"Feature\" }, { \"geometry\" : { \"coordinates\" : [ -75 , 43 ], \"type\" : \"Point\" }, \"properties\" : { \"geonameId\" : \"5128638\" , \"id\" : \"/185./18533.\" , \"name\" : \"/United States of America/New York State\" }, \"type\" : \"Feature\" } ], \"type\" : \"FeatureCollection\" } } ...","title":"GeoJSON classification"},{"location":"reference/geojson/#geojson-classification","text":"The geotax document classification resources can be used in a standard way, composing their endpoints as indicated in the article in the reference section. In this case, the output of the resource is also standard. It is also possible to obtain an output containing standard GeoJSON data, which is useful for locating identified countries in GIS systems. To achieve this you need to add a query string parameter to the resource endpoint. The parameter is called features and must be set to extradata . For example, this is the resource endpoint for English with the addition of the parameter: https://nlapi.expert.ai/v2/categorize/geotax/en?features=extradata The resulting output has this format: { \"success\": Boolean success flag , \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , \"categories\": as in the standard classification output , \"extraData\": { \"geojson\": GeoJSON data } } } It is the standard classification output with the addition of the extraData object which in turn contains the geojson object. The GeoJSON data contains identification data for the countries and the coordinates of their geographic center, for example: ... \"content\" : \"Last year I moved from San Francisco to New York.\" , \"extraData\" : { \"geojson\" : { \"features\" : [ { \"geometry\" : { \"coordinates\" : [ -98.57 , 39.82 ], \"type\" : \"Point\" }, \"properties\" : { \"geonameId\" : \"6252001\" , \"id\" : \"/185.\" , \"name\" : \"/United States of America\" }, \"type\" : \"Feature\" }, { \"geometry\" : { \"coordinates\" : [ -120 , 37 ], \"type\" : \"Point\" }, \"properties\" : { \"geonameId\" : \"5332921\" , \"id\" : \"/185./18505.\" , \"name\" : \"/United States of America/California\" }, \"type\" : \"Feature\" }, { \"geometry\" : { \"coordinates\" : [ -75 , 43 ], \"type\" : \"Point\" }, \"properties\" : { \"geonameId\" : \"5128638\" , \"id\" : \"/185./18533.\" , \"name\" : \"/United States of America/New York State\" }, \"type\" : \"Feature\" } ], \"type\" : \"FeatureCollection\" } } ...","title":"GeoJSON classification"},{"location":"reference/http-status-codes/","text":"HTTP status codes 200 OK The request has been accepted and processed. The response contains a JSON object . 400 Bad Request The server cannot process the request due to something that is perceived to be a client error, for example malformed request syntax. It can happen when the JSON object posted with the request is invalid. For example, the following is not a valid JSON object because both the external curly braces and the quotation marks around the properties names are missing. document: { text: \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" } 401 Unauthorized The reasons for this state can be: invalid credentials were specified when requesting the authorization token the authorization token is missing the authorization token is not valid the authorization token has expired 403 Forbidden This code is returned when requesting resources that are outside the scope of the plan that the user subscribed or when one or more of the plan limits\u2014for example the characters per month limit\u2014have been reached. 404 Not Found The server can not find the requested resource, the URL is wrong. 413 Request Entity Too Large The request is larger than the limit defined by the plan the user subscribed. 500 Internal Server Error The server has encountered a situation it doesn't know how to handle.","title":"HTTP status codes"},{"location":"reference/http-status-codes/#http-status-codes","text":"","title":"HTTP status codes"},{"location":"reference/http-status-codes/#200-ok","text":"The request has been accepted and processed. The response contains a JSON object .","title":"200 OK"},{"location":"reference/http-status-codes/#400-bad-request","text":"The server cannot process the request due to something that is perceived to be a client error, for example malformed request syntax. It can happen when the JSON object posted with the request is invalid. For example, the following is not a valid JSON object because both the external curly braces and the quotation marks around the properties names are missing. document: { text: \"Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half.\" }","title":"400 Bad Request"},{"location":"reference/http-status-codes/#401-unauthorized","text":"The reasons for this state can be: invalid credentials were specified when requesting the authorization token the authorization token is missing the authorization token is not valid the authorization token has expired","title":"401 Unauthorized"},{"location":"reference/http-status-codes/#403-forbidden","text":"This code is returned when requesting resources that are outside the scope of the plan that the user subscribed or when one or more of the plan limits\u2014for example the characters per month limit\u2014have been reached.","title":"403 Forbidden"},{"location":"reference/http-status-codes/#404-not-found","text":"The server can not find the requested resource, the URL is wrong.","title":"404 Not Found"},{"location":"reference/http-status-codes/#413-request-entity-too-large","text":"The request is larger than the limit defined by the plan the user subscribed.","title":"413 Request Entity Too Large"},{"location":"reference/http-status-codes/#500-internal-server-error","text":"The server has encountered a situation it doesn't know how to handle.","title":"500 Internal Server Error"},{"location":"reference/output/","text":"API resources output In case of success or managed errors, the API's REST interface resources return HTTPS status 200 OK and a response containing an UTF-8 encoded JSON object. Success In case of success, all the document analysis , document classification and information detection resources share this response format: { \"success\": true, \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , resource-specific output } } Self-documention resources, instead, have their peculiar output format . The Boolean property success indicates that processing was successful while the data object contains the results. The data object has a similar structure for all the resources: content is the analyzed text. language is the ISO 639-1 language code . version gives information about the technology for reporting purposes. After these properties the resource-specific output follows. For more information about the output of specific resources, refer to the following: Document analysis: Full analysis Deep linguistic analysis Keyphrase extraction Named entity recognition Relation extraction Sentiment analysis Document classification Information detection: PII detection Writeprint detection Managed errors In case of a managed error, all the API resources return a specific response . Other errors In case of unmanaged application errors or other anomalies, the API returns specific HTTP status codes .","title":"Overview"},{"location":"reference/output/#api-resources-output","text":"In case of success or managed errors, the API's REST interface resources return HTTPS status 200 OK and a response containing an UTF-8 encoded JSON object.","title":"API resources output"},{"location":"reference/output/#success","text":"In case of success, all the document analysis , document classification and information detection resources share this response format: { \"success\": true, \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , resource-specific output } } Self-documention resources, instead, have their peculiar output format . The Boolean property success indicates that processing was successful while the data object contains the results. The data object has a similar structure for all the resources: content is the analyzed text. language is the ISO 639-1 language code . version gives information about the technology for reporting purposes. After these properties the resource-specific output follows. For more information about the output of specific resources, refer to the following: Document analysis: Full analysis Deep linguistic analysis Keyphrase extraction Named entity recognition Relation extraction Sentiment analysis Document classification Information detection: PII detection Writeprint detection","title":"Success"},{"location":"reference/output/#managed-errors","text":"In case of a managed error, all the API resources return a specific response .","title":"Managed errors"},{"location":"reference/output/#other-errors","text":"In case of unmanaged application errors or other anomalies, the API returns specific HTTP status codes .","title":"Other errors"},{"location":"reference/output/classification/","text":"Document classification output The API resources performing document classification return a JSON object with this format: { \"success\": Boolean success flag , \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , \"categories\": [] } } For the description of the contents , language and version properties, see the API resources output overview . Each item of the categories array represents a category, for example: { \"frequency\" : 70.62 , \"hierarchy\" : [ \"Sport\" , \"Competition discipline\" , \"Basketball\" ], \"id\" : \"20000851\" , \"label\" : \"Basketball\" , \"namespace\" : \"iptc_en_1.0\" , \"positions\" : [ { \"end\" : 14 , \"start\" : 0 }, { \"end\" : 53 , \"start\" : 35 }, { \"end\" : 139 , \"start\" : 136 } ], \"score\" : 4005.0 , \"winner\" : true } namespace is the name of the software module containing the reference taxonomy. id , label and hierarchy identify the category . score is the cumulative score that was attributed to the category. frequency is the percentage ratio of the category score to the sum of all categories' scores. winner is a Boolean flag set to true if the category was considered particularly relevant. positions is an array containing the positions of the text blocks that contributed to category score.","title":"Document classification"},{"location":"reference/output/classification/#document-classification-output","text":"The API resources performing document classification return a JSON object with this format: { \"success\": Boolean success flag , \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , \"categories\": [] } } For the description of the contents , language and version properties, see the API resources output overview . Each item of the categories array represents a category, for example: { \"frequency\" : 70.62 , \"hierarchy\" : [ \"Sport\" , \"Competition discipline\" , \"Basketball\" ], \"id\" : \"20000851\" , \"label\" : \"Basketball\" , \"namespace\" : \"iptc_en_1.0\" , \"positions\" : [ { \"end\" : 14 , \"start\" : 0 }, { \"end\" : 53 , \"start\" : 35 }, { \"end\" : 139 , \"start\" : 136 } ], \"score\" : 4005.0 , \"winner\" : true } namespace is the name of the software module containing the reference taxonomy. id , label and hierarchy identify the category . score is the cumulative score that was attributed to the category. frequency is the percentage ratio of the category score to the sum of all categories' scores. winner is a Boolean flag set to true if the category was considered particularly relevant. positions is an array containing the positions of the text blocks that contributed to category score.","title":"Document classification output"},{"location":"reference/output/detection/","text":"Information detection output The structure of the output of the API resources performing information detection varies based on the detector. Find more information about the output of each detector in this section: pii writeprint temporal-information","title":"Overview"},{"location":"reference/output/detection/#information-detection-output","text":"The structure of the output of the API resources performing information detection varies based on the detector. Find more information about the output of each detector in this section: pii writeprint temporal-information","title":"Information detection output"},{"location":"reference/output/detection/pii/","text":"PII detector output Introduction The PII detector API resource returns a JSON object with this format: { \"success\": Boolean success flag , \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , \"knowledge\": [], \"paragraphs\": [], \"sentences\": [], \"phrases\": [], \"tokens\": [], \"entities\": [], \"extractions\": [], \"extraData\": {} } } For the description of the contents , language and version properties, see the API resources output overview . You can ignore all the arrays except extractions because they are simply functional to the production of the fundamental output that is inside the extradata object. If you are still interested, since arrays are the product of other API features, then: For knowledge see the description of full analysis output . For: paragraphs sentences phrases tokens see the description of deep linguistic analysis output . For: mainSentences mainPhrases mainLemmas mainSyncons topics see the description of keyphrase extraction output . For entities see the description of named entity recognition output . The extractions array and the extraData object both contain detected PII in two alternative formats. The extractions array represents PII with a proprietary expert.ai format, while the JSON-LD property of the extraData object is a JSON-LD representation of the same information. It's up to you to choose the format you prefer. Simple Vs composite information The PII detector returns simple and composite information. Simple information\u2014like phone numbers and e-mail addresses\u2014have only one property. Composite information have two or more properties, like a postal address which is composed of a street name, a locality, a ZIP code and a region. extraData object The extraData object only property is JSON-LD , for example: \"extraData\" : { \"JSON-LD\" : { \"@context\" : { ... }, \"@graph\" : [ { \"@id\" : \"https://schema.org/email?email=m.gut%40bfu.edu\" , \"@type\" : \"https://schema.org/email\" , \"email\" : \"m.gut@bfu.edu\" , \"matches\" : [ { \"end\" : 211 , \"name\" : \"email\" , \"start\" : 197 , \"value\" : \"m.gut@bfu.edu\" } ] }, { \"@id\" : \"https://schema.org/telephone?telephone=(210)%20617-5256\" , \"@type\" : \"https://schema.org/telephone\" , \"matches\" : [ { \"end\" : 153 , \"name\" : \"telephone\" , \"start\" : 138 , \"value\" : \"(210) 617-5256\" } ], \"telephone\" : \"(210) 617-5256\" }, { \"@id\" : \"https://schema.org/telephone?telephone=(210)%20949-3006\" , \"@type\" : \"https://schema.org/telephone\" , \"matches\" : [ { \"end\" : 181 , \"name\" : \"telephone\" , \"start\" : 166 , \"value\" : \"(210) 949-3006\" } ], \"telephone\" : \"(210) 949-3006\" }, { \"@id\" : \"https://schema.org/PostalAddress?address=7400%20Merton%20Minter%20Blvd.%2C%20San%20Antonio%2C%20TX%2C%2078229-4404\" , \"@type\" : \"https://schema.org/PostalAddress\" , \"address\" : \"7400 Merton Minter Blvd., San Antonio, TX, 78229-4404\" , \"addressCountry\" : \"United States of America\" , \"addressLocality\" : \"San Antonio\" , \"addressRegion\" : \"Texas\" , \"matches\" : [ { \"end\" : 88 , \"name\" : \"streetAddress\" , \"start\" : 64 , \"value\" : \"7400 Merton Minter Blvd.\" }, { \"end\" : 123 , \"name\" : \"postalCode\" , \"start\" : 112 , \"value\" : \"78229-4404\" }, { \"end\" : 123 , \"name\" : \"address\" , \"start\" : 64 , \"value\" : \"7400 Merton Minter Blvd., 111E, San Antonio, TX 78229-4404\" }, { \"end\" : 111 , \"name\" : \"addressLocality\" , \"start\" : 96 , \"value\" : \"San Antonio, TX\" }, { \"end\" : 111 , \"name\" : \"addressRegion\" , \"start\" : 96 , \"value\" : \"San Antonio, TX\" }, { \"end\" : 111 , \"name\" : \"addressCountry\" , \"start\" : 96 , \"value\" : \"San Antonio, TX\" } ], \"postalCode\" : \"78229-4404\" , \"streetAddress\" : \"7400 Merton Minter Blvd.\" }, { \"@id\" : \"https://schema.org/Person?person=Mark%20Gutenberg\" , \"@type\" : \"https://schema.org/Person\" , \"birthDate\" : \"1984-12-08\" , \"birthPlace\" : \"Hamburg\" , \"familyName\" : \"Gutenberg\" , \"gender\" : \"M\" , \"givenName\" : \"Mark\" , \"matches\" : [ { \"end\" : 54 , \"name\" : \"familyName\" , \"start\" : 39 , \"value\" : \"Mark Gutenberg\" }, { \"end\" : 54 , \"name\" : \"gender\" , \"start\" : 39 , \"value\" : \"Mark Gutenberg\" }, { \"end\" : 54 , \"name\" : \"givenName\" , \"start\" : 39 , \"value\" : \"Mark Gutenberg\" }, { \"end\" : 54 , \"name\" : \"person\" , \"start\" : 39 , \"value\" : \"Mark Gutenberg\" }, { \"end\" : 260 , \"name\" : \"birthPlace\" , \"start\" : 243 , \"value\" : \"HAMBURG, GERMANY\" }, { \"end\" : 282 , \"name\" : \"birthDate\" , \"start\" : 272 , \"value\" : \"12/8/1984\" } ], \"person\" : \"Mark Gutenberg\" } ] } } The value of the JSON-LD property is the JSON-LD object. The characteristic of the JSON-LD format is to provide linked data. Specifically, PII information types and properties are linked to schema.org public vocabulary definitions. For example, the type of the information representing a postal address corresponds to the https://schema.org/PostalAddress definition and the type's properties correspond to schema.org definitions too. For the description of the JSON-LD format refer to the official documentation . The @graph property of the JSON-LD object contains the actual PII. @graph is an array, each item of which represents a simple or composite information. These are all the PII that may be present: Information type Property Linked data reference Personal attributes https://schema.org/Person person https://schema.org/Person givenName https://schema.org/givenName familyName https://schema.org/familyName age https://schema.org/Number gender https://schema.org/gender nationality https://schema.org/nationality birthDate https://schema.org/birthDate birthPlace https://schema.org/birthPlace deathDate https://schema.org/deathDate deathPlace https://schema.org/deathPlace dateTime * https://schema.org/Date Postal address https://schema.org/PostalAddress address https://schema.org/Text streetAddress https://schema.org/streetAddress addressCountry https://schema.org/addressCountry addressLocality https://schema.org/addressLocality addressRegion https://schema.org/addressRegion postalCode https://schema.org/postalCode postOfficeBoxNumber https://schema.org/postOfficeBoxNumber Bank account https://schema.org/BankAccount IBAN https://schema.org/PropertyValue IBANcountry https://schema.org/Country IP address https://schema.org/additionalProperty IP https://schema.org/Text E-mail address https://schema.org/email email https://schema.org/email URL https://schema.org/URL URL https://schema.org/URL Financial product (credi/debit card) https://schema.org/FinancialProduct creditDebitNumber https://schema.org/Text CVV https://schema.org/Number expirationDate https://schema.org/Date Phone number https://schema.org/telephone telephone https://schema.org/telephone * dateTime is an array, since there can be more than one value associated with the person. The matches array of each information item contains the occurrences of the properties in the text. Each item of the array corresponds to a property. Item properties are: name : property name start : zero-based index of the first character of the occurrence in the text end : zero-based index of the first character after the occurrence in the text value : the portion of text from which the property value was taken extractions array To understand the contents of the extractions array you must know that information detection can also be seen as a process of extracting records of data from the text. Each record contains data fields and its structure\u2014the possible fields\u2014is called template . A template can be compared to a table and the template fields to the columns of the table, as shown in the following figure. So for example instances of the PII_PERSON template are records that contain fields like: familyName gender givenName birthPlace birthDate Every item of the extractions array represents an extraction record. For example, the following item is a record that's an instance of the PII_PERSON template: { \"fields\" : [ { \"name\" : \"familyName\" , \"positions\" : [ { \"end\" : 54 , \"start\" : 39 } ], \"value\" : \"Gutenberg\" }, { \"name\" : \"gender\" , \"positions\" : [ { \"end\" : 54 , \"start\" : 39 } ], \"value\" : \"M\" }, { \"name\" : \"givenName\" , \"positions\" : [ { \"end\" : 54 , \"start\" : 39 } ], \"value\" : \"Mark\" }, { \"name\" : \"person\" , \"positions\" : [ { \"end\" : 54 , \"start\" : 39 } ], \"value\" : \"Mark Gutenberg\" }, { \"name\" : \"birthPlace\" , \"positions\" : [ { \"end\" : 260 , \"start\" : 243 } ], \"value\" : \"Hamburg\" }, { \"name\" : \"birthDate\" , \"positions\" : [ { \"end\" : 282 , \"start\" : 272 } ], \"value\" : \"1984-12-08\" } ], \"namespace\" : \"pii_en_1.0\" , \"template\" : \"PII_PERSON\" } In each item: namespace is the name of the software module performing the extraction. template is the name of the template. fields is the array of record fields. Each item of the fields array item represents an extracted value where: name is the field's name. value is the field's value. positions is an array containing the extracted field's positions . These are all the templates and related fields: Information type Template Field Personal attributes PII_PERSON person givenName familyName age gender nationality birthDate birthPlace deathDate deathPlace dateTime Postal address PII_ADDRESS address streetAddress addressCountry addressLocality addressRegion postalCode postOfficeBoxNumber Bank account PII_BANKACCOUNT IBAN IBANcountry IP address PII_IP IP E-mail address PII_EMAIL email URL PII_URL URL Financial product (credi/debit card) PII_FINANCIALPRODUCT creditDebitNumber CVV expirationDate Phone number PII_TELEPHONE telephone","title":"PII"},{"location":"reference/output/detection/pii/#pii-detector-output","text":"","title":"PII detector output"},{"location":"reference/output/detection/pii/#introduction","text":"The PII detector API resource returns a JSON object with this format: { \"success\": Boolean success flag , \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , \"knowledge\": [], \"paragraphs\": [], \"sentences\": [], \"phrases\": [], \"tokens\": [], \"entities\": [], \"extractions\": [], \"extraData\": {} } } For the description of the contents , language and version properties, see the API resources output overview . You can ignore all the arrays except extractions because they are simply functional to the production of the fundamental output that is inside the extradata object. If you are still interested, since arrays are the product of other API features, then: For knowledge see the description of full analysis output . For: paragraphs sentences phrases tokens see the description of deep linguistic analysis output . For: mainSentences mainPhrases mainLemmas mainSyncons topics see the description of keyphrase extraction output . For entities see the description of named entity recognition output . The extractions array and the extraData object both contain detected PII in two alternative formats. The extractions array represents PII with a proprietary expert.ai format, while the JSON-LD property of the extraData object is a JSON-LD representation of the same information. It's up to you to choose the format you prefer.","title":"Introduction"},{"location":"reference/output/detection/pii/#simple-vs-composite-information","text":"The PII detector returns simple and composite information. Simple information\u2014like phone numbers and e-mail addresses\u2014have only one property. Composite information have two or more properties, like a postal address which is composed of a street name, a locality, a ZIP code and a region.","title":"Simple Vs composite information"},{"location":"reference/output/detection/pii/#extradata-object","text":"The extraData object only property is JSON-LD , for example: \"extraData\" : { \"JSON-LD\" : { \"@context\" : { ... }, \"@graph\" : [ { \"@id\" : \"https://schema.org/email?email=m.gut%40bfu.edu\" , \"@type\" : \"https://schema.org/email\" , \"email\" : \"m.gut@bfu.edu\" , \"matches\" : [ { \"end\" : 211 , \"name\" : \"email\" , \"start\" : 197 , \"value\" : \"m.gut@bfu.edu\" } ] }, { \"@id\" : \"https://schema.org/telephone?telephone=(210)%20617-5256\" , \"@type\" : \"https://schema.org/telephone\" , \"matches\" : [ { \"end\" : 153 , \"name\" : \"telephone\" , \"start\" : 138 , \"value\" : \"(210) 617-5256\" } ], \"telephone\" : \"(210) 617-5256\" }, { \"@id\" : \"https://schema.org/telephone?telephone=(210)%20949-3006\" , \"@type\" : \"https://schema.org/telephone\" , \"matches\" : [ { \"end\" : 181 , \"name\" : \"telephone\" , \"start\" : 166 , \"value\" : \"(210) 949-3006\" } ], \"telephone\" : \"(210) 949-3006\" }, { \"@id\" : \"https://schema.org/PostalAddress?address=7400%20Merton%20Minter%20Blvd.%2C%20San%20Antonio%2C%20TX%2C%2078229-4404\" , \"@type\" : \"https://schema.org/PostalAddress\" , \"address\" : \"7400 Merton Minter Blvd., San Antonio, TX, 78229-4404\" , \"addressCountry\" : \"United States of America\" , \"addressLocality\" : \"San Antonio\" , \"addressRegion\" : \"Texas\" , \"matches\" : [ { \"end\" : 88 , \"name\" : \"streetAddress\" , \"start\" : 64 , \"value\" : \"7400 Merton Minter Blvd.\" }, { \"end\" : 123 , \"name\" : \"postalCode\" , \"start\" : 112 , \"value\" : \"78229-4404\" }, { \"end\" : 123 , \"name\" : \"address\" , \"start\" : 64 , \"value\" : \"7400 Merton Minter Blvd., 111E, San Antonio, TX 78229-4404\" }, { \"end\" : 111 , \"name\" : \"addressLocality\" , \"start\" : 96 , \"value\" : \"San Antonio, TX\" }, { \"end\" : 111 , \"name\" : \"addressRegion\" , \"start\" : 96 , \"value\" : \"San Antonio, TX\" }, { \"end\" : 111 , \"name\" : \"addressCountry\" , \"start\" : 96 , \"value\" : \"San Antonio, TX\" } ], \"postalCode\" : \"78229-4404\" , \"streetAddress\" : \"7400 Merton Minter Blvd.\" }, { \"@id\" : \"https://schema.org/Person?person=Mark%20Gutenberg\" , \"@type\" : \"https://schema.org/Person\" , \"birthDate\" : \"1984-12-08\" , \"birthPlace\" : \"Hamburg\" , \"familyName\" : \"Gutenberg\" , \"gender\" : \"M\" , \"givenName\" : \"Mark\" , \"matches\" : [ { \"end\" : 54 , \"name\" : \"familyName\" , \"start\" : 39 , \"value\" : \"Mark Gutenberg\" }, { \"end\" : 54 , \"name\" : \"gender\" , \"start\" : 39 , \"value\" : \"Mark Gutenberg\" }, { \"end\" : 54 , \"name\" : \"givenName\" , \"start\" : 39 , \"value\" : \"Mark Gutenberg\" }, { \"end\" : 54 , \"name\" : \"person\" , \"start\" : 39 , \"value\" : \"Mark Gutenberg\" }, { \"end\" : 260 , \"name\" : \"birthPlace\" , \"start\" : 243 , \"value\" : \"HAMBURG, GERMANY\" }, { \"end\" : 282 , \"name\" : \"birthDate\" , \"start\" : 272 , \"value\" : \"12/8/1984\" } ], \"person\" : \"Mark Gutenberg\" } ] } } The value of the JSON-LD property is the JSON-LD object. The characteristic of the JSON-LD format is to provide linked data. Specifically, PII information types and properties are linked to schema.org public vocabulary definitions. For example, the type of the information representing a postal address corresponds to the https://schema.org/PostalAddress definition and the type's properties correspond to schema.org definitions too. For the description of the JSON-LD format refer to the official documentation . The @graph property of the JSON-LD object contains the actual PII. @graph is an array, each item of which represents a simple or composite information. These are all the PII that may be present: Information type Property Linked data reference Personal attributes https://schema.org/Person person https://schema.org/Person givenName https://schema.org/givenName familyName https://schema.org/familyName age https://schema.org/Number gender https://schema.org/gender nationality https://schema.org/nationality birthDate https://schema.org/birthDate birthPlace https://schema.org/birthPlace deathDate https://schema.org/deathDate deathPlace https://schema.org/deathPlace dateTime * https://schema.org/Date Postal address https://schema.org/PostalAddress address https://schema.org/Text streetAddress https://schema.org/streetAddress addressCountry https://schema.org/addressCountry addressLocality https://schema.org/addressLocality addressRegion https://schema.org/addressRegion postalCode https://schema.org/postalCode postOfficeBoxNumber https://schema.org/postOfficeBoxNumber Bank account https://schema.org/BankAccount IBAN https://schema.org/PropertyValue IBANcountry https://schema.org/Country IP address https://schema.org/additionalProperty IP https://schema.org/Text E-mail address https://schema.org/email email https://schema.org/email URL https://schema.org/URL URL https://schema.org/URL Financial product (credi/debit card) https://schema.org/FinancialProduct creditDebitNumber https://schema.org/Text CVV https://schema.org/Number expirationDate https://schema.org/Date Phone number https://schema.org/telephone telephone https://schema.org/telephone * dateTime is an array, since there can be more than one value associated with the person. The matches array of each information item contains the occurrences of the properties in the text. Each item of the array corresponds to a property. Item properties are: name : property name start : zero-based index of the first character of the occurrence in the text end : zero-based index of the first character after the occurrence in the text value : the portion of text from which the property value was taken","title":"extraData object"},{"location":"reference/output/detection/pii/#extractions-array","text":"To understand the contents of the extractions array you must know that information detection can also be seen as a process of extracting records of data from the text. Each record contains data fields and its structure\u2014the possible fields\u2014is called template . A template can be compared to a table and the template fields to the columns of the table, as shown in the following figure. So for example instances of the PII_PERSON template are records that contain fields like: familyName gender givenName birthPlace birthDate Every item of the extractions array represents an extraction record. For example, the following item is a record that's an instance of the PII_PERSON template: { \"fields\" : [ { \"name\" : \"familyName\" , \"positions\" : [ { \"end\" : 54 , \"start\" : 39 } ], \"value\" : \"Gutenberg\" }, { \"name\" : \"gender\" , \"positions\" : [ { \"end\" : 54 , \"start\" : 39 } ], \"value\" : \"M\" }, { \"name\" : \"givenName\" , \"positions\" : [ { \"end\" : 54 , \"start\" : 39 } ], \"value\" : \"Mark\" }, { \"name\" : \"person\" , \"positions\" : [ { \"end\" : 54 , \"start\" : 39 } ], \"value\" : \"Mark Gutenberg\" }, { \"name\" : \"birthPlace\" , \"positions\" : [ { \"end\" : 260 , \"start\" : 243 } ], \"value\" : \"Hamburg\" }, { \"name\" : \"birthDate\" , \"positions\" : [ { \"end\" : 282 , \"start\" : 272 } ], \"value\" : \"1984-12-08\" } ], \"namespace\" : \"pii_en_1.0\" , \"template\" : \"PII_PERSON\" } In each item: namespace is the name of the software module performing the extraction. template is the name of the template. fields is the array of record fields. Each item of the fields array item represents an extracted value where: name is the field's name. value is the field's value. positions is an array containing the extracted field's positions . These are all the templates and related fields: Information type Template Field Personal attributes PII_PERSON person givenName familyName age gender nationality birthDate birthPlace deathDate deathPlace dateTime Postal address PII_ADDRESS address streetAddress addressCountry addressLocality addressRegion postalCode postOfficeBoxNumber Bank account PII_BANKACCOUNT IBAN IBANcountry IP address PII_IP IP E-mail address PII_EMAIL email URL PII_URL URL Financial product (credi/debit card) PII_FINANCIALPRODUCT creditDebitNumber CVV expirationDate Phone number PII_TELEPHONE telephone","title":"extractions array"},{"location":"reference/output/detection/temporal-information/","text":"Temporal information detector output Introduction The Temporal information detector API resource returns a JSON object with this format: { \"success\": Boolean success flag , \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , \"knowledge\": [], \"paragraphs\": [], \"sentences\": [], \"phrases\": [], \"tokens\": [], \"entities\": [], \"extractions\": [], \"extraData\": {} } } For a description of the contents , language and version properties, see the API resources output overview . The main output is the extradata object and normally you can ignore all the rest because its only functional to the production of that object. However, you'll need to consider also the sentences array if you want to create a visualization in which sentences containing temporal information expressions are highlighted in the text. If you are still interested, consider that output arrays are the product of other API capabilities, so: For knowledge see the description of full analysis output . For: paragraphs sentences phrases tokens see the description of deep linguistic analysis output . For entities see the description of named entity recognition output . extraData object If you have this input text: I first traveled to Australia in 2004. I returned seven years later. After that, I lived in New Zealand from February 2013 to November 2014. this is an example of the extraData object you can obtain: \"extraData\" : { \"script-version\" : \"v1.1.0 rev. 2021-07-20T13:58:19.796Z\" , \"JSON-LD\" : { \"@context\" : { ... }, \"@graph\" : { \"@id\" : \"TemporalInformation\" , \"items\" : [ { \"@id\" : \"3411a0251418664029e3e2c6cb01f70b\" , \"@type\" : \"schema:PropertyValue\" , \"name\" : \"TimePoint\" , \"value\" : \"2004\" , \"matches\" : [ { \"@id\" : \"008743bc89848b9d52537c9b52362e27\" , \"start\" : 33 , \"end\" : 37 , \"sentence\" : 0 , \"text\" : \"2004\" } ] }, { \"@id\" : \"3a495e7fda460928c1f56a022d519d54\" , \"@type\" : \"schema:PropertyValue\" , \"name\" : \"TimePoint\" , \"value\" : \"2011\" , \"matches\" : [ { \"@id\" : \"1fd2bcd8b1f2b96fc4c15e7447e6c634\" , \"start\" : 50 , \"end\" : 67 , \"sentence\" : 1 , \"text\" : \"seven years later\" , \"derived\" : true , \"referenceTimePoint\" : \"008743bc89848b9d52537c9b52362e27\" , \"referenceTimeSpan\" : \"11620a7b84820db2e05f8824fd4b10cd\" } ] }, { \"@id\" : \"42850d057130d3162cf6f8f0707bcc3e\" , \"@type\" : \"schema:PropertyValue\" , \"name\" : \"TimeInterval\" , \"value\" : \"2013-02/2014-11\" , \"matches\" : [ { \"@id\" : \"d745613c4b3b182f73cc0173e26879fd\" , \"start\" : 104 , \"end\" : 139 , \"sentence\" : 2 , \"text\" : \"from February 2013 to November 2014\" } ] }, { \"@id\" : \"e7d7ffa0181dba4317d6077d873e7675\" , \"@type\" : \"schema:PropertyValue\" , \"name\" : \"TimePoint\" , \"value\" : \"2013-02\" , \"matches\" : [ { \"@id\" : \"766fe2f27808dccabba5f7fb065248d8\" , \"start\" : 104 , \"end\" : 122 , \"sentence\" : 2 , \"text\" : \"from February 2013\" } ] }, { \"@id\" : \"6a5825809662d2ca7d7f851f95349cac\" , \"@type\" : \"schema:PropertyValue\" , \"name\" : \"TimePoint\" , \"value\" : \"2014-11\" , \"matches\" : [ { \"@id\" : \"f9cdca3b7b7e5e0614d717a2e03dc2e7\" , \"start\" : 123 , \"end\" : 139 , \"sentence\" : 2 , \"text\" : \"to November 2014\" } ] }, { \"@id\" : \"a5f53924b0dc85e3f44a02dbf2f2ddc1\" , \"@type\" : \"schema:PropertyValue\" , \"name\" : \"TimeSpan\" , \"value\" : \"+ 7 year\" , \"matches\" : [ { \"@id\" : \"11620a7b84820db2e05f8824fd4b10cd\" , \"start\" : 50 , \"end\" : 67 , \"sentence\" : 1 , \"text\" : \"seven years later\" } ] } ] } } } script-version is the version of the scripting engine used by the detector, normally you can ignore it. JSON-LD is a JSON-LD object. For the description of the JSON-LD format refer to the official documentation . The @graph property of the JSON-LD object has two properties: @id : object identifier, is always TemporalInformation items : array containing the temporal information The information items Detected temporal information is stored in the items array. Each member of the array represents all the detected instances of a temporal information. Item properties are: @id : unique identifier @type : schema.org type name : type of time expression, which is: TimePoint for time points TimeSpan for time spans TimeInterval for time intervals value : normalized information value matches : occurrences of the information in the text. Each array item corresponds to a part of the text and has these properties: @id : unique identifier start : zero-based position of the first character of the portion of text expressing the information end : zero-based position of the character after the last of the portion of text expressing the information sentence : index inside the sentences array of the sentence containing the occurrence text : portion of text expressing the information (Optional, for time points) derived : when this property is present, its value is always true , indicating the match refers to a time point that has been derived from other time expressions (Optional, for time spans) indefiniteQuantity : when present, its value is the portion of text that expresses the indefinite quantity, for example the word some (Optional, for time spans) duration : when present, its value is the portion of text indicating the time span represents a duration, for example the word for in for seven years (Optional, for derived time points ) referenceTimePoint : the @id value of the items array member with name equal to TimePoint acting as the base time point from which the match was derived (Optional, for derived time points ) referenceTimeSpan : the @id value of the items array member with name equal to TimeSpan acting as the time span added to\u2014or subtracted\u2014from the base time point to derive the match","title":"Temporal information"},{"location":"reference/output/detection/temporal-information/#temporal-information-detector-output","text":"","title":"Temporal information detector output"},{"location":"reference/output/detection/temporal-information/#introduction","text":"The Temporal information detector API resource returns a JSON object with this format: { \"success\": Boolean success flag , \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , \"knowledge\": [], \"paragraphs\": [], \"sentences\": [], \"phrases\": [], \"tokens\": [], \"entities\": [], \"extractions\": [], \"extraData\": {} } } For a description of the contents , language and version properties, see the API resources output overview . The main output is the extradata object and normally you can ignore all the rest because its only functional to the production of that object. However, you'll need to consider also the sentences array if you want to create a visualization in which sentences containing temporal information expressions are highlighted in the text. If you are still interested, consider that output arrays are the product of other API capabilities, so: For knowledge see the description of full analysis output . For: paragraphs sentences phrases tokens see the description of deep linguistic analysis output . For entities see the description of named entity recognition output .","title":"Introduction"},{"location":"reference/output/detection/temporal-information/#extradata-object","text":"If you have this input text: I first traveled to Australia in 2004. I returned seven years later. After that, I lived in New Zealand from February 2013 to November 2014. this is an example of the extraData object you can obtain: \"extraData\" : { \"script-version\" : \"v1.1.0 rev. 2021-07-20T13:58:19.796Z\" , \"JSON-LD\" : { \"@context\" : { ... }, \"@graph\" : { \"@id\" : \"TemporalInformation\" , \"items\" : [ { \"@id\" : \"3411a0251418664029e3e2c6cb01f70b\" , \"@type\" : \"schema:PropertyValue\" , \"name\" : \"TimePoint\" , \"value\" : \"2004\" , \"matches\" : [ { \"@id\" : \"008743bc89848b9d52537c9b52362e27\" , \"start\" : 33 , \"end\" : 37 , \"sentence\" : 0 , \"text\" : \"2004\" } ] }, { \"@id\" : \"3a495e7fda460928c1f56a022d519d54\" , \"@type\" : \"schema:PropertyValue\" , \"name\" : \"TimePoint\" , \"value\" : \"2011\" , \"matches\" : [ { \"@id\" : \"1fd2bcd8b1f2b96fc4c15e7447e6c634\" , \"start\" : 50 , \"end\" : 67 , \"sentence\" : 1 , \"text\" : \"seven years later\" , \"derived\" : true , \"referenceTimePoint\" : \"008743bc89848b9d52537c9b52362e27\" , \"referenceTimeSpan\" : \"11620a7b84820db2e05f8824fd4b10cd\" } ] }, { \"@id\" : \"42850d057130d3162cf6f8f0707bcc3e\" , \"@type\" : \"schema:PropertyValue\" , \"name\" : \"TimeInterval\" , \"value\" : \"2013-02/2014-11\" , \"matches\" : [ { \"@id\" : \"d745613c4b3b182f73cc0173e26879fd\" , \"start\" : 104 , \"end\" : 139 , \"sentence\" : 2 , \"text\" : \"from February 2013 to November 2014\" } ] }, { \"@id\" : \"e7d7ffa0181dba4317d6077d873e7675\" , \"@type\" : \"schema:PropertyValue\" , \"name\" : \"TimePoint\" , \"value\" : \"2013-02\" , \"matches\" : [ { \"@id\" : \"766fe2f27808dccabba5f7fb065248d8\" , \"start\" : 104 , \"end\" : 122 , \"sentence\" : 2 , \"text\" : \"from February 2013\" } ] }, { \"@id\" : \"6a5825809662d2ca7d7f851f95349cac\" , \"@type\" : \"schema:PropertyValue\" , \"name\" : \"TimePoint\" , \"value\" : \"2014-11\" , \"matches\" : [ { \"@id\" : \"f9cdca3b7b7e5e0614d717a2e03dc2e7\" , \"start\" : 123 , \"end\" : 139 , \"sentence\" : 2 , \"text\" : \"to November 2014\" } ] }, { \"@id\" : \"a5f53924b0dc85e3f44a02dbf2f2ddc1\" , \"@type\" : \"schema:PropertyValue\" , \"name\" : \"TimeSpan\" , \"value\" : \"+ 7 year\" , \"matches\" : [ { \"@id\" : \"11620a7b84820db2e05f8824fd4b10cd\" , \"start\" : 50 , \"end\" : 67 , \"sentence\" : 1 , \"text\" : \"seven years later\" } ] } ] } } } script-version is the version of the scripting engine used by the detector, normally you can ignore it. JSON-LD is a JSON-LD object. For the description of the JSON-LD format refer to the official documentation . The @graph property of the JSON-LD object has two properties: @id : object identifier, is always TemporalInformation items : array containing the temporal information","title":"extraData object"},{"location":"reference/output/detection/temporal-information/#the-information-items","text":"Detected temporal information is stored in the items array. Each member of the array represents all the detected instances of a temporal information. Item properties are: @id : unique identifier @type : schema.org type name : type of time expression, which is: TimePoint for time points TimeSpan for time spans TimeInterval for time intervals value : normalized information value matches : occurrences of the information in the text. Each array item corresponds to a part of the text and has these properties: @id : unique identifier start : zero-based position of the first character of the portion of text expressing the information end : zero-based position of the character after the last of the portion of text expressing the information sentence : index inside the sentences array of the sentence containing the occurrence text : portion of text expressing the information (Optional, for time points) derived : when this property is present, its value is always true , indicating the match refers to a time point that has been derived from other time expressions (Optional, for time spans) indefiniteQuantity : when present, its value is the portion of text that expresses the indefinite quantity, for example the word some (Optional, for time spans) duration : when present, its value is the portion of text indicating the time span represents a duration, for example the word for in for seven years (Optional, for derived time points ) referenceTimePoint : the @id value of the items array member with name equal to TimePoint acting as the base time point from which the match was derived (Optional, for derived time points ) referenceTimeSpan : the @id value of the items array member with name equal to TimeSpan acting as the time span added to\u2014or subtracted\u2014from the base time point to derive the match","title":"The information items"},{"location":"reference/output/detection/writeprint/","text":"Writeprint detector output Introduction The Writeprint detector API resource returns a JSON object with this format: { \"success\": Boolean success flag , \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , \"knowledge\": [], \"paragraphs\": [], \"sentences\": [], \"phrases\": [], \"tokens\": [], \"entities\": [], \"extractions\": [], \"extraData\": {} } } For the description of the contents , language and version properties, see the API resources output overview . You can ignore all the arrays because they are simply functional to the production of the fundamental output that is inside the extradata object. If you are still interested, consider that the output arrays are the product of other API capabilities, so: For knowledge see the description of full analysis output . For: paragraphs sentences phrases tokens see the description of deep linguistic analysis output . For entities see the description of named entity recognition output . extraData object The extraData object only property is JSON-LD , for example: \"extraData\" : { \"JSON-LD\" : { \"@context\" : { ... }, \"@graph\" : [ { \"@type\" : \"https://schema.org/Thing\" , \"readabilityIndexes\" : [ { \"@type\" : \"https://schema.org/PropertyValue\" , \"name\" : \"Coleman-Liau\" , \"value\" : 14.58 , \"readabilityLevel\" : \"Very Difficult\" }, { \"@type\" : \"https://schema.org/PropertyValue\" , \"name\" : \"Gulpease\" , \"value\" : 50.57 , \"readabilityLevel\" : \"Medium\" }, { \"@type\" : \"https://schema.org/PropertyValue\" , \"name\" : \"Automated Readability Index\" , \"value\" : 13.55 , \"readabilityLevel\" : \"Very Difficult\" } ], \"structureIndexes\" : { \"sentences\" : { \"total\" : 11 }, \"tokens\" : { \"total\" : 199 }, \"tokenLengthPerSentence\" : { \"mean\" : 5.69 , \"standardDeviation\" : 3.52 , \"meanAbsoluteDeviation\" : 2.85 }, \"verbTypes\" : { \"total\" : 9 }, \"charactersPerSentence\" : { \"mean\" : 120.09 , \"standardDeviation\" : 50.35 , \"meanAbsoluteDeviation\" : 45.38 , \"total\" : 1321 }, \"atomsPerSentence\" : { \"mean\" : 20.63 , \"standardDeviation\" : 8.94 , \"meanAbsoluteDeviation\" : 7.9 , \"total\" : 227 }, \"tokensPerSentence\" : { \"mean\" : 18.09 , \"standardDeviation\" : 7.05 , \"meanAbsoluteDeviation\" : 6.29 , \"total\" : 199 }, \"phrasesPerSentence\" : { \"mean\" : 10.72 , \"standardDeviation\" : 4.2 , \"meanAbsoluteDeviation\" : 3.93 , \"total\" : 118 }, \"verbTypesPerSentence\" : { \"mean\" : 2.36 , \"standardDeviation\" : 1.14 , \"meanAbsoluteDeviation\" : 0.89 , \"total\" : 9 }, \"smallFirstLetterSentences\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"capitalFirstLetterSentences\" : { \"mean\" : 1 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 11 }, \"adjectivesPerSentence\" : { \"mean\" : 1.54 , \"standardDeviation\" : 1.23 , \"meanAbsoluteDeviation\" : 1.04 , \"total\" : 17 }, \"adverbsPerSentence\" : { \"mean\" : 0.54 , \"standardDeviation\" : 0.89 , \"meanAbsoluteDeviation\" : 0.69 , \"total\" : 6 }, \"articlesPerSentence\" : { \"mean\" : 1.18 , \"standardDeviation\" : 1.26 , \"meanAbsoluteDeviation\" : 0.99 , \"total\" : 13 }, \"auxiliariesPerSentence\" : { \"mean\" : 0.9 , \"standardDeviation\" : 0.79 , \"meanAbsoluteDeviation\" : 0.66 , \"total\" : 10 }, \"conjunctionsPerSentence\" : { \"mean\" : 1.09 , \"standardDeviation\" : 0.99 , \"meanAbsoluteDeviation\" : 0.84 , \"total\" : 12 }, \"nounsPerSentence\" : { \"mean\" : 5.36 , \"standardDeviation\" : 1.96 , \"meanAbsoluteDeviation\" : 1.66 , \"total\" : 59 }, \"properNounsPerSentence\" : { \"mean\" : 0.72 , \"standardDeviation\" : 0.86 , \"meanAbsoluteDeviation\" : 0.66 , \"total\" : 8 }, \"punctuationPerSentence\" : { \"mean\" : 1.9 , \"standardDeviation\" : 0.9 , \"meanAbsoluteDeviation\" : 0.66 , \"total\" : 21 }, \"prepositionsPerSentence\" : { \"mean\" : 1.72 , \"standardDeviation\" : 1.13 , \"meanAbsoluteDeviation\" : 0.89 , \"total\" : 19 }, \"pronounsPerSentence\" : { \"mean\" : 0.36 , \"standardDeviation\" : 0.64 , \"meanAbsoluteDeviation\" : 0.52 , \"total\" : 4 }, \"particlesPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"verbsPerSentence\" : { \"mean\" : 2.72 , \"standardDeviation\" : 1.05 , \"meanAbsoluteDeviation\" : 0.92 , \"total\" : 30 }, \"namedEntitiesPerSentence\" : { \"mean\" : 0.81 , \"standardDeviation\" : 1.11 , \"meanAbsoluteDeviation\" : 0.74 , \"total\" : 9 }, \"adjectivePhrasesPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"conjunctionPhrasesPerSentence\" : { \"mean\" : 1 , \"standardDeviation\" : 0.85 , \"meanAbsoluteDeviation\" : 0.72 , \"total\" : 11 }, \"adverbPhrasesPerSentence\" : { \"mean\" : 0.18 , \"standardDeviation\" : 0.38 , \"meanAbsoluteDeviation\" : 0.29 , \"total\" : 2 }, \"nounPhrasesPerSentence\" : { \"mean\" : 3.09 , \"standardDeviation\" : 1.23 , \"meanAbsoluteDeviation\" : 1.02 , \"total\" : 34 }, \"nominalPredicatesPerSentence\" : { \"mean\" : 0.18 , \"standardDeviation\" : 0.38 , \"meanAbsoluteDeviation\" : 0.29 , \"total\" : 2 }, \"prepositionPhrasesPerSentence\" : { \"mean\" : 1.9 , \"standardDeviation\" : 1.08 , \"meanAbsoluteDeviation\" : 0.84 , \"total\" : 21 }, \"relativePhrasesPerSentence\" : { \"mean\" : 0.18 , \"standardDeviation\" : 0.38 , \"meanAbsoluteDeviation\" : 0.29 , \"total\" : 2 }, \"verbPhrasesPerSentence\" : { \"mean\" : 2.54 , \"standardDeviation\" : 1.07 , \"meanAbsoluteDeviation\" : 0.87 , \"total\" : 28 }, \"unknownConceptsPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"emoticonsPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"academicLanguageWordsPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"businessLanguageWordsPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"crimeLanguageWordsPerSentence\" : { \"mean\" : 0.36 , \"standardDeviation\" : 0.48 , \"meanAbsoluteDeviation\" : 0.46 , \"total\" : 4 }, \"laymanLanguageWordsPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"legalLanguageWordsPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"militaryLanguageWordsPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"politicalLanguageWordsPerSentence\" : { \"mean\" : 1.27 , \"standardDeviation\" : 1.6 , \"meanAbsoluteDeviation\" : 1.3 , \"total\" : 14 }, \"socialMediaLanguageWordsPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"colonsPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"commasPerSentence\" : { \"mean\" : 0.63 , \"standardDeviation\" : 0.88 , \"meanAbsoluteDeviation\" : 0.69 , \"total\" : 7 }, \"dotsPerSentence\" : { \"mean\" : 1 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 11 }, \"doubleQuotationMarksPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"exclamationMarksPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"exclamationMarkQuestionMarkSequencesPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"multipleDotsPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"multipleExclamationMarksPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"multipleQuestionMarksPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"questionMarksPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"semicolonsPerSentence\" : { \"mean\" : 0.09 , \"standardDeviation\" : 0.28 , \"meanAbsoluteDeviation\" : 0.16 , \"total\" : 1 }, \"singleQuotationMarksPerSentence\" : { \"mean\" : 0.18 , \"standardDeviation\" : 0.38 , \"meanAbsoluteDeviation\" : 0.29 , \"total\" : 2 }, \"commonlyMisspelledWordsPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"functionWordsPerSentence\" : { \"mean\" : 6.45 , \"standardDeviation\" : 3.75 , \"meanAbsoluteDeviation\" : 2.85 , \"total\" : 71 }, \"mostCommonWordsPerSentence\" : { \"mean\" : 8.36 , \"standardDeviation\" : 4.05 , \"meanAbsoluteDeviation\" : 3.07 , \"total\" : 92 } } } ] } } Writeprint information is contained in the JSON-LD property which, as the name suggests, is a JSON-LD object. For the description of the JSON-LD format refer to the official documentation . The @graph property of the JSON-LD object is an array which only item is an object containing the actual Writeprint information. The readabilityIndexes property is an array containing one item for each of the available readability indexes. The structureIndexes property is an object which properties represent the other 57 indexes. The readabilityIndexes array readabilityIndexes array items have these properties: name : the readability index name, for example Coleman-Liau value : the index value, for example 14.58 readabilityLevel : the degree of difficulty, with these possible values: Very Easy Easy Medium Difficult Very Difficult The structureIndexes object All the properties of the structureIndexes object whose name ends with \"PerSentence\" except tokenLengthPerSentence are objects with these properties: mean : the mean of all the values for all the sentences standardDeviation : the standard deviation . meanAbsoluteDeviation : the mean absolute deviation total : the sum of the number of occurrences of items measured by the index for all the sentences in the document The tokenLengthPerSentence object doesn't have the total property. The remaining properties of the structureIndexes object, that is: sentences tokens verbTypes have only the total property. Find the description of the indexes in the guide section .","title":"Writeprint"},{"location":"reference/output/detection/writeprint/#writeprint-detector-output","text":"","title":"Writeprint detector output"},{"location":"reference/output/detection/writeprint/#introduction","text":"The Writeprint detector API resource returns a JSON object with this format: { \"success\": Boolean success flag , \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , \"knowledge\": [], \"paragraphs\": [], \"sentences\": [], \"phrases\": [], \"tokens\": [], \"entities\": [], \"extractions\": [], \"extraData\": {} } } For the description of the contents , language and version properties, see the API resources output overview . You can ignore all the arrays because they are simply functional to the production of the fundamental output that is inside the extradata object. If you are still interested, consider that the output arrays are the product of other API capabilities, so: For knowledge see the description of full analysis output . For: paragraphs sentences phrases tokens see the description of deep linguistic analysis output . For entities see the description of named entity recognition output .","title":"Introduction"},{"location":"reference/output/detection/writeprint/#extradata-object","text":"The extraData object only property is JSON-LD , for example: \"extraData\" : { \"JSON-LD\" : { \"@context\" : { ... }, \"@graph\" : [ { \"@type\" : \"https://schema.org/Thing\" , \"readabilityIndexes\" : [ { \"@type\" : \"https://schema.org/PropertyValue\" , \"name\" : \"Coleman-Liau\" , \"value\" : 14.58 , \"readabilityLevel\" : \"Very Difficult\" }, { \"@type\" : \"https://schema.org/PropertyValue\" , \"name\" : \"Gulpease\" , \"value\" : 50.57 , \"readabilityLevel\" : \"Medium\" }, { \"@type\" : \"https://schema.org/PropertyValue\" , \"name\" : \"Automated Readability Index\" , \"value\" : 13.55 , \"readabilityLevel\" : \"Very Difficult\" } ], \"structureIndexes\" : { \"sentences\" : { \"total\" : 11 }, \"tokens\" : { \"total\" : 199 }, \"tokenLengthPerSentence\" : { \"mean\" : 5.69 , \"standardDeviation\" : 3.52 , \"meanAbsoluteDeviation\" : 2.85 }, \"verbTypes\" : { \"total\" : 9 }, \"charactersPerSentence\" : { \"mean\" : 120.09 , \"standardDeviation\" : 50.35 , \"meanAbsoluteDeviation\" : 45.38 , \"total\" : 1321 }, \"atomsPerSentence\" : { \"mean\" : 20.63 , \"standardDeviation\" : 8.94 , \"meanAbsoluteDeviation\" : 7.9 , \"total\" : 227 }, \"tokensPerSentence\" : { \"mean\" : 18.09 , \"standardDeviation\" : 7.05 , \"meanAbsoluteDeviation\" : 6.29 , \"total\" : 199 }, \"phrasesPerSentence\" : { \"mean\" : 10.72 , \"standardDeviation\" : 4.2 , \"meanAbsoluteDeviation\" : 3.93 , \"total\" : 118 }, \"verbTypesPerSentence\" : { \"mean\" : 2.36 , \"standardDeviation\" : 1.14 , \"meanAbsoluteDeviation\" : 0.89 , \"total\" : 9 }, \"smallFirstLetterSentences\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"capitalFirstLetterSentences\" : { \"mean\" : 1 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 11 }, \"adjectivesPerSentence\" : { \"mean\" : 1.54 , \"standardDeviation\" : 1.23 , \"meanAbsoluteDeviation\" : 1.04 , \"total\" : 17 }, \"adverbsPerSentence\" : { \"mean\" : 0.54 , \"standardDeviation\" : 0.89 , \"meanAbsoluteDeviation\" : 0.69 , \"total\" : 6 }, \"articlesPerSentence\" : { \"mean\" : 1.18 , \"standardDeviation\" : 1.26 , \"meanAbsoluteDeviation\" : 0.99 , \"total\" : 13 }, \"auxiliariesPerSentence\" : { \"mean\" : 0.9 , \"standardDeviation\" : 0.79 , \"meanAbsoluteDeviation\" : 0.66 , \"total\" : 10 }, \"conjunctionsPerSentence\" : { \"mean\" : 1.09 , \"standardDeviation\" : 0.99 , \"meanAbsoluteDeviation\" : 0.84 , \"total\" : 12 }, \"nounsPerSentence\" : { \"mean\" : 5.36 , \"standardDeviation\" : 1.96 , \"meanAbsoluteDeviation\" : 1.66 , \"total\" : 59 }, \"properNounsPerSentence\" : { \"mean\" : 0.72 , \"standardDeviation\" : 0.86 , \"meanAbsoluteDeviation\" : 0.66 , \"total\" : 8 }, \"punctuationPerSentence\" : { \"mean\" : 1.9 , \"standardDeviation\" : 0.9 , \"meanAbsoluteDeviation\" : 0.66 , \"total\" : 21 }, \"prepositionsPerSentence\" : { \"mean\" : 1.72 , \"standardDeviation\" : 1.13 , \"meanAbsoluteDeviation\" : 0.89 , \"total\" : 19 }, \"pronounsPerSentence\" : { \"mean\" : 0.36 , \"standardDeviation\" : 0.64 , \"meanAbsoluteDeviation\" : 0.52 , \"total\" : 4 }, \"particlesPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"verbsPerSentence\" : { \"mean\" : 2.72 , \"standardDeviation\" : 1.05 , \"meanAbsoluteDeviation\" : 0.92 , \"total\" : 30 }, \"namedEntitiesPerSentence\" : { \"mean\" : 0.81 , \"standardDeviation\" : 1.11 , \"meanAbsoluteDeviation\" : 0.74 , \"total\" : 9 }, \"adjectivePhrasesPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"conjunctionPhrasesPerSentence\" : { \"mean\" : 1 , \"standardDeviation\" : 0.85 , \"meanAbsoluteDeviation\" : 0.72 , \"total\" : 11 }, \"adverbPhrasesPerSentence\" : { \"mean\" : 0.18 , \"standardDeviation\" : 0.38 , \"meanAbsoluteDeviation\" : 0.29 , \"total\" : 2 }, \"nounPhrasesPerSentence\" : { \"mean\" : 3.09 , \"standardDeviation\" : 1.23 , \"meanAbsoluteDeviation\" : 1.02 , \"total\" : 34 }, \"nominalPredicatesPerSentence\" : { \"mean\" : 0.18 , \"standardDeviation\" : 0.38 , \"meanAbsoluteDeviation\" : 0.29 , \"total\" : 2 }, \"prepositionPhrasesPerSentence\" : { \"mean\" : 1.9 , \"standardDeviation\" : 1.08 , \"meanAbsoluteDeviation\" : 0.84 , \"total\" : 21 }, \"relativePhrasesPerSentence\" : { \"mean\" : 0.18 , \"standardDeviation\" : 0.38 , \"meanAbsoluteDeviation\" : 0.29 , \"total\" : 2 }, \"verbPhrasesPerSentence\" : { \"mean\" : 2.54 , \"standardDeviation\" : 1.07 , \"meanAbsoluteDeviation\" : 0.87 , \"total\" : 28 }, \"unknownConceptsPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"emoticonsPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"academicLanguageWordsPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"businessLanguageWordsPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"crimeLanguageWordsPerSentence\" : { \"mean\" : 0.36 , \"standardDeviation\" : 0.48 , \"meanAbsoluteDeviation\" : 0.46 , \"total\" : 4 }, \"laymanLanguageWordsPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"legalLanguageWordsPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"militaryLanguageWordsPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"politicalLanguageWordsPerSentence\" : { \"mean\" : 1.27 , \"standardDeviation\" : 1.6 , \"meanAbsoluteDeviation\" : 1.3 , \"total\" : 14 }, \"socialMediaLanguageWordsPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"colonsPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"commasPerSentence\" : { \"mean\" : 0.63 , \"standardDeviation\" : 0.88 , \"meanAbsoluteDeviation\" : 0.69 , \"total\" : 7 }, \"dotsPerSentence\" : { \"mean\" : 1 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 11 }, \"doubleQuotationMarksPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"exclamationMarksPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"exclamationMarkQuestionMarkSequencesPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"multipleDotsPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"multipleExclamationMarksPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"multipleQuestionMarksPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"questionMarksPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"semicolonsPerSentence\" : { \"mean\" : 0.09 , \"standardDeviation\" : 0.28 , \"meanAbsoluteDeviation\" : 0.16 , \"total\" : 1 }, \"singleQuotationMarksPerSentence\" : { \"mean\" : 0.18 , \"standardDeviation\" : 0.38 , \"meanAbsoluteDeviation\" : 0.29 , \"total\" : 2 }, \"commonlyMisspelledWordsPerSentence\" : { \"mean\" : 0 , \"standardDeviation\" : 0 , \"meanAbsoluteDeviation\" : 0 , \"total\" : 0 }, \"functionWordsPerSentence\" : { \"mean\" : 6.45 , \"standardDeviation\" : 3.75 , \"meanAbsoluteDeviation\" : 2.85 , \"total\" : 71 }, \"mostCommonWordsPerSentence\" : { \"mean\" : 8.36 , \"standardDeviation\" : 4.05 , \"meanAbsoluteDeviation\" : 3.07 , \"total\" : 92 } } } ] } } Writeprint information is contained in the JSON-LD property which, as the name suggests, is a JSON-LD object. For the description of the JSON-LD format refer to the official documentation . The @graph property of the JSON-LD object is an array which only item is an object containing the actual Writeprint information. The readabilityIndexes property is an array containing one item for each of the available readability indexes. The structureIndexes property is an object which properties represent the other 57 indexes.","title":"extraData object"},{"location":"reference/output/detection/writeprint/#the-readabilityindexes-array","text":"readabilityIndexes array items have these properties: name : the readability index name, for example Coleman-Liau value : the index value, for example 14.58 readabilityLevel : the degree of difficulty, with these possible values: Very Easy Easy Medium Difficult Very Difficult","title":"The readabilityIndexes array"},{"location":"reference/output/detection/writeprint/#the-structureindexes-object","text":"All the properties of the structureIndexes object whose name ends with \"PerSentence\" except tokenLengthPerSentence are objects with these properties: mean : the mean of all the values for all the sentences standardDeviation : the standard deviation . meanAbsoluteDeviation : the mean absolute deviation total : the sum of the number of occurrences of items measured by the index for all the sentences in the document The tokenLengthPerSentence object doesn't have the total property. The remaining properties of the structureIndexes object, that is: sentences tokens verbTypes have only the total property. Find the description of the indexes in the guide section .","title":"The structureIndexes object"},{"location":"reference/output/entity-recognition/","text":"Named entity recognition output The API resource performing named entity recognition returns a JSON object with this format: { \"success\": Boolean success flag , \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , \"knowledge\": [], \"entities\": [] } } For the description of the contents , language and version properties, see the API resources output overview . entities Each item of the entities array represents a named entity, for example: { \"lemma\" : \"National Basketball Association\" , \"positions\" : [ { \"end\" : 139 , \"start\" : 136 } ], \"relevance\" : 10 , \"syncon\" : 206693 , \"type\" : \"ORG\" , \"attributes\" : [ { \"attribute\" : \"role\" , \"lemma\" : \"league\" , \"syncon\" : 36253 , \"type\" : \"org\" } ] } type identifies the kind of entity. The possible values for type are listed in the reference section . positions is an array containing the positions of the entity's mentions in the text. The syncon and the lemma properties are the outcome of semantic analysis and lemmatization respectively. These are exactly the same processes carried out during deep linguistic analysis . Value -1 for syncon means the concept doesn't have a correspondent in the expert.ai Knowledge Graph . This can happen with entities that are recognized through heuristics (e.g. John Smith ). relevance is an indicator of the importance of the entity in text. It's values ranges from 1 to 15. attributes The attributes array contains information about the entities that is inferred by semantic analysis based on: Information available in the Knowledge Graph Semantic features of the entity's name The context in which the entity is cited The attribute property indicates the type of attribute. Possible values are: Value Description age Age of a person birthdate Birth date of a person birthplace Birth place of a person deathdate Death date of a person deathplace Death date of a person gender Gender of a person humanspec Specification of a person nationality Nationality of a person orgspec Specification of an organization placespec Specification of a place prodspec Specification of a product qualifyingadj Qualifying adjective qualifyingadv Qualifying adverb qualifyingnoun Qualifying noun role Role of an entity; if referred to a person can also be a title or a profession timerangespec Interval of time specification timespec Time specification Attributes can be nested, i.e. an attribute can have other attributes that further specify it. For example from the text: Saudi King Salman called on governments around the world these attributes are inferred for entity Salman : \"attributes\" : [ { \"attribute\" : \"gender\" , \"lemma\" : \"male\" , \"syncon\" : -1 , \"type\" : \"\" }, { \"attribute\" : \"role\" , \"lemma\" : \"King\" , \"syncon\" : 43350 , \"type\" : \"nph\" , \"attributes\" : [ { \"attribute\" : \"placespec\" , \"lemma\" : \"Saudi Arabia\" , \"syncon\" : 38596 , \"type\" : \"GEO\" } ], } ] The nested attribute, in this case, specifies the place of which entity Salman is the king, as if it were the answer to the question: \"king of what?\". For the syncon and lemma properties see above: they are the result of deep linguistic analysis. If the attribute is a generic or named entity, type identifies the kind of entity. Possible values can be uppercase or lowercase. Uppercase corresponds to named entities, lowercase to generic entities. knowledge The knowledge array contains Knowledge Graph data about the syncons associated with the entities. Its contents are described in the article about the output of full analysis .","title":"Named entity recognition"},{"location":"reference/output/entity-recognition/#named-entity-recognition-output","text":"The API resource performing named entity recognition returns a JSON object with this format: { \"success\": Boolean success flag , \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , \"knowledge\": [], \"entities\": [] } } For the description of the contents , language and version properties, see the API resources output overview .","title":"Named entity recognition output"},{"location":"reference/output/entity-recognition/#entities","text":"Each item of the entities array represents a named entity, for example: { \"lemma\" : \"National Basketball Association\" , \"positions\" : [ { \"end\" : 139 , \"start\" : 136 } ], \"relevance\" : 10 , \"syncon\" : 206693 , \"type\" : \"ORG\" , \"attributes\" : [ { \"attribute\" : \"role\" , \"lemma\" : \"league\" , \"syncon\" : 36253 , \"type\" : \"org\" } ] } type identifies the kind of entity. The possible values for type are listed in the reference section . positions is an array containing the positions of the entity's mentions in the text. The syncon and the lemma properties are the outcome of semantic analysis and lemmatization respectively. These are exactly the same processes carried out during deep linguistic analysis . Value -1 for syncon means the concept doesn't have a correspondent in the expert.ai Knowledge Graph . This can happen with entities that are recognized through heuristics (e.g. John Smith ). relevance is an indicator of the importance of the entity in text. It's values ranges from 1 to 15.","title":"entities"},{"location":"reference/output/entity-recognition/#attributes","text":"The attributes array contains information about the entities that is inferred by semantic analysis based on: Information available in the Knowledge Graph Semantic features of the entity's name The context in which the entity is cited The attribute property indicates the type of attribute. Possible values are: Value Description age Age of a person birthdate Birth date of a person birthplace Birth place of a person deathdate Death date of a person deathplace Death date of a person gender Gender of a person humanspec Specification of a person nationality Nationality of a person orgspec Specification of an organization placespec Specification of a place prodspec Specification of a product qualifyingadj Qualifying adjective qualifyingadv Qualifying adverb qualifyingnoun Qualifying noun role Role of an entity; if referred to a person can also be a title or a profession timerangespec Interval of time specification timespec Time specification Attributes can be nested, i.e. an attribute can have other attributes that further specify it. For example from the text: Saudi King Salman called on governments around the world these attributes are inferred for entity Salman : \"attributes\" : [ { \"attribute\" : \"gender\" , \"lemma\" : \"male\" , \"syncon\" : -1 , \"type\" : \"\" }, { \"attribute\" : \"role\" , \"lemma\" : \"King\" , \"syncon\" : 43350 , \"type\" : \"nph\" , \"attributes\" : [ { \"attribute\" : \"placespec\" , \"lemma\" : \"Saudi Arabia\" , \"syncon\" : 38596 , \"type\" : \"GEO\" } ], } ] The nested attribute, in this case, specifies the place of which entity Salman is the king, as if it were the answer to the question: \"king of what?\". For the syncon and lemma properties see above: they are the result of deep linguistic analysis. If the attribute is a generic or named entity, type identifies the kind of entity. Possible values can be uppercase or lowercase. Uppercase corresponds to named entities, lowercase to generic entities.","title":"attributes"},{"location":"reference/output/entity-recognition/#knowledge","text":"The knowledge array contains Knowledge Graph data about the syncons associated with the entities. Its contents are described in the article about the output of full analysis .","title":"knowledge"},{"location":"reference/output/full-analysis/","text":"Full document analysis output The resource performing full analysis returns a JSON object with this format: { \"success\": Boolean success flag , \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , \"knowledge\": [], \"paragraphs\": [], \"sentences\": [], \"phrases\": [], \"tokens\": [], \"mainSentences\": [], \"mainPhrases\": [], \"mainLemmas\": [], \"mainSyncons\": [], \"topics\": [], \"entities\": [], \"relations\": [], \"sentiment\": {} } } For the description of the contents , language and version properties, see the API resources output overview . Components arrays and objects have the same structure they have in the response of the resource that performs the corresponding process, so: For: paragraphs sentences phrases tokens see the format of deep linguistic analysis output . For: mainSentences mainPhrases mainLemmas mainSyncons topics see the format of keyphrase extraction output . For: entities see the format of named entity recognition output . For: relations see the format of relation extraction output . For: sentiment object see the format of sentiment analysis output . knowledge The knowledge array contains Knowledge Graph data information about syncons. Items in these arrays: tokens manSyncons entities relations items (in the sentiment object) can have a syncon property: the link between those items and the corresponding items in the knowledge array is thus represented by the value of the syncon property both items have in common. For example, if this is an item of the tokens array: { \"atoms\" : [ { \"end\" : 45 , \"lemma\" : \"basketball\" , \"start\" : 35 , \"type\" : \"NOU\" }, { \"end\" : 53 , \"lemma\" : \"player\" , \"start\" : 46 , \"type\" : \"NOU\" } ], \"dependency\" : { \"head\" : 2 , \"id\" : 6 , \"label\" : \"nmod\" }, \"end\" : 53 , \"lemma\" : \"basketball player\" , \"morphology\" : \"Number=Plur\" , \"paragraph\" : 0 , \"phrase\" : 2 , \"pos\" : \"NOUN\" , \"sentence\" : 0 , \"start\" : 35 , \"syncon\" : 41583 , \"type\" : \"NOU\" } the corresponding entry in the knowledge array can be: { \"label\" : \"person.athlete.basketball_player\" , \"properties\" : [ { \"type\" : \"WikiDataId\" , \"value\" : \"Q3665646\" } ], \"syncon\" : 41583 } It can be a \"many-to-one\" relationship since more than one item in the tokens , relations and sentiment items arrays can have the same syncon ID, but there's always one entry in the knowledge array for a given syncon, so the knowledge array is a reference table. For example, if a text contains several occurrences of basketball player , each occurrence corresponds to a separate item in the tokens array, but all tokens \"point\" to the same entry in the knowledge array. Items with the syncon property set to -1 have no corresponding entry in the knowledge array. This is because they are concepts recognized through heuristics and are not present in the context's Knowledge Graph. Each entry in the array has a format like this: { \"label\" : \"person\" , \"properties\" : [ { \"type\" : \"WikiDataId\" , \"value\" : \"Q215627\" } ], \"syncon\" : 73282 } The label property is a textual rendering of the general conceptual category for the syncon in the Knowledge Graph. The properties array contains the outcome of knowledge linking. Each item has two properties, type and value . type specifies the knowledge base, value is the property value. Possible knowledge bases and interpretations of the value property follow. type value Coordinate Latitude and longitude WikiDataId Wikipedia article ID DBpediaId URL of the DBPedia content GeoNamesId ID of the record in the GeoNames database","title":"Full analysis"},{"location":"reference/output/full-analysis/#full-document-analysis-output","text":"The resource performing full analysis returns a JSON object with this format: { \"success\": Boolean success flag , \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , \"knowledge\": [], \"paragraphs\": [], \"sentences\": [], \"phrases\": [], \"tokens\": [], \"mainSentences\": [], \"mainPhrases\": [], \"mainLemmas\": [], \"mainSyncons\": [], \"topics\": [], \"entities\": [], \"relations\": [], \"sentiment\": {} } } For the description of the contents , language and version properties, see the API resources output overview . Components arrays and objects have the same structure they have in the response of the resource that performs the corresponding process, so: For: paragraphs sentences phrases tokens see the format of deep linguistic analysis output . For: mainSentences mainPhrases mainLemmas mainSyncons topics see the format of keyphrase extraction output . For: entities see the format of named entity recognition output . For: relations see the format of relation extraction output . For: sentiment object see the format of sentiment analysis output .","title":"Full document analysis output"},{"location":"reference/output/full-analysis/#knowledge","text":"The knowledge array contains Knowledge Graph data information about syncons. Items in these arrays: tokens manSyncons entities relations items (in the sentiment object) can have a syncon property: the link between those items and the corresponding items in the knowledge array is thus represented by the value of the syncon property both items have in common. For example, if this is an item of the tokens array: { \"atoms\" : [ { \"end\" : 45 , \"lemma\" : \"basketball\" , \"start\" : 35 , \"type\" : \"NOU\" }, { \"end\" : 53 , \"lemma\" : \"player\" , \"start\" : 46 , \"type\" : \"NOU\" } ], \"dependency\" : { \"head\" : 2 , \"id\" : 6 , \"label\" : \"nmod\" }, \"end\" : 53 , \"lemma\" : \"basketball player\" , \"morphology\" : \"Number=Plur\" , \"paragraph\" : 0 , \"phrase\" : 2 , \"pos\" : \"NOUN\" , \"sentence\" : 0 , \"start\" : 35 , \"syncon\" : 41583 , \"type\" : \"NOU\" } the corresponding entry in the knowledge array can be: { \"label\" : \"person.athlete.basketball_player\" , \"properties\" : [ { \"type\" : \"WikiDataId\" , \"value\" : \"Q3665646\" } ], \"syncon\" : 41583 } It can be a \"many-to-one\" relationship since more than one item in the tokens , relations and sentiment items arrays can have the same syncon ID, but there's always one entry in the knowledge array for a given syncon, so the knowledge array is a reference table. For example, if a text contains several occurrences of basketball player , each occurrence corresponds to a separate item in the tokens array, but all tokens \"point\" to the same entry in the knowledge array. Items with the syncon property set to -1 have no corresponding entry in the knowledge array. This is because they are concepts recognized through heuristics and are not present in the context's Knowledge Graph. Each entry in the array has a format like this: { \"label\" : \"person\" , \"properties\" : [ { \"type\" : \"WikiDataId\" , \"value\" : \"Q215627\" } ], \"syncon\" : 73282 } The label property is a textual rendering of the general conceptual category for the syncon in the Knowledge Graph. The properties array contains the outcome of knowledge linking. Each item has two properties, type and value . type specifies the knowledge base, value is the property value. Possible knowledge bases and interpretations of the value property follow. type value Coordinate Latitude and longitude WikiDataId Wikipedia article ID DBpediaId URL of the DBPedia content GeoNamesId ID of the record in the GeoNames database","title":"knowledge"},{"location":"reference/output/keyphrase-extraction/","text":"Keyphrase extraction output The relevants resource that performs keyphrase extraction returns a JSON object with this structure: { \"success\": Boolean success flag , \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , \"knowledge\": [], \"topics\": [], \"mainSentences\": [], \"mainPhrases\": [], \"mainSyncons\": [], \"mainLemmas\": [] } } For the description of the contents , language and version properties see the API resources output overview . Common properties Items that can be directly mapped to the text have properties indicating their position . Items that occur only once, such as a sentence, have a start and end properties while items that can occur multiple times, such as a main lemma, have a positions array containing the start and end positions of all the occurrences. Items also have a score property which provides a measure of their relevance. topics The topics array contains references to Knowledge Graph topics pertinent with the text. Each array item corresponds to a topic, for example: { \"id\" : 223 , \"label\" : \"mechanics\" , \"score\" : 3.5 , \"winner\" : true } Possible topics are listed in the reference section . id is the identification number, winner is a Boolean flag set to true if the topic is considered particularly relevant. mainSentences The mainSentences array contains info about relevant sentences. Each array item represents a sentence, for example: { \"value\" : \"The machine is held until ready to start by a sort of trap to be sprung when all is ready; then with a tremendous flapping and snapping of the four-cylinder engine, the huge machine springs aloft.\" , \"score\" : 13.3 , \"start\" : 740 , \"end\" : 936 } mainPhrases The mainPhrases array contains info about the phrases deemed particularly representative during the analysis. Each array item represents a phrase, for example: { \"value\" : \"four-cylinder engine\" , \"score\" : 8 , \"positions\" : [ { \"start\" : 883 , \"end\" : 903 } ] } mainSyncons The mainSyncons array contains references to Knowledge Graph syncons corresponding to the concepts that were considered relevant. Each array item represents a syncon, for example: { \"lemma\" : \"experiment\" , \"positions\" : [ { \"end\" : 224 , \"start\" : 213 }, { \"end\" : 2830 , \"start\" : 2820 } ], \"score\" : 5.8 , \"syncon\" : 2496 } The syncon and the lemma properties are the outcome of semantic analysis and lemmatization respectively. These are exactly the same processes carried out during deep linguistic analysis . syncon can be interpreted as a pointer to the knowledge array entry having its syncon property set to the same value. mainLemmas The mainLemmas array contains relevant lemmas. Each array item represents a lemma, for example: { \"value\" : \"locomotive\" , \"score\" : 6.5 , \"positions\" : [ { \"start\" : 1152 , \"end\" : 1162 }, { \"start\" : 1163 , \"end\" : 1167 }, { \"start\" : 1239 , \"end\" : 1249 }, { \"start\" : 1335 , \"end\" : 1345 }, { \"start\" : 1394 , \"end\" : 1404 } ] } knowledge The knowledge array contains Knowledge Graph data for the items of the mainSyncons array. Its contents are described in the article about the output of full analysis .","title":"Keyphrase extraction"},{"location":"reference/output/keyphrase-extraction/#keyphrase-extraction-output","text":"The relevants resource that performs keyphrase extraction returns a JSON object with this structure: { \"success\": Boolean success flag , \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , \"knowledge\": [], \"topics\": [], \"mainSentences\": [], \"mainPhrases\": [], \"mainSyncons\": [], \"mainLemmas\": [] } } For the description of the contents , language and version properties see the API resources output overview .","title":"Keyphrase extraction output"},{"location":"reference/output/keyphrase-extraction/#common-properties","text":"Items that can be directly mapped to the text have properties indicating their position . Items that occur only once, such as a sentence, have a start and end properties while items that can occur multiple times, such as a main lemma, have a positions array containing the start and end positions of all the occurrences. Items also have a score property which provides a measure of their relevance.","title":"Common properties"},{"location":"reference/output/keyphrase-extraction/#topics","text":"The topics array contains references to Knowledge Graph topics pertinent with the text. Each array item corresponds to a topic, for example: { \"id\" : 223 , \"label\" : \"mechanics\" , \"score\" : 3.5 , \"winner\" : true } Possible topics are listed in the reference section . id is the identification number, winner is a Boolean flag set to true if the topic is considered particularly relevant.","title":"topics"},{"location":"reference/output/keyphrase-extraction/#mainsentences","text":"The mainSentences array contains info about relevant sentences. Each array item represents a sentence, for example: { \"value\" : \"The machine is held until ready to start by a sort of trap to be sprung when all is ready; then with a tremendous flapping and snapping of the four-cylinder engine, the huge machine springs aloft.\" , \"score\" : 13.3 , \"start\" : 740 , \"end\" : 936 }","title":"mainSentences"},{"location":"reference/output/keyphrase-extraction/#mainphrases","text":"The mainPhrases array contains info about the phrases deemed particularly representative during the analysis. Each array item represents a phrase, for example: { \"value\" : \"four-cylinder engine\" , \"score\" : 8 , \"positions\" : [ { \"start\" : 883 , \"end\" : 903 } ] }","title":"mainPhrases"},{"location":"reference/output/keyphrase-extraction/#mainsyncons","text":"The mainSyncons array contains references to Knowledge Graph syncons corresponding to the concepts that were considered relevant. Each array item represents a syncon, for example: { \"lemma\" : \"experiment\" , \"positions\" : [ { \"end\" : 224 , \"start\" : 213 }, { \"end\" : 2830 , \"start\" : 2820 } ], \"score\" : 5.8 , \"syncon\" : 2496 } The syncon and the lemma properties are the outcome of semantic analysis and lemmatization respectively. These are exactly the same processes carried out during deep linguistic analysis . syncon can be interpreted as a pointer to the knowledge array entry having its syncon property set to the same value.","title":"mainSyncons"},{"location":"reference/output/keyphrase-extraction/#mainlemmas","text":"The mainLemmas array contains relevant lemmas. Each array item represents a lemma, for example: { \"value\" : \"locomotive\" , \"score\" : 6.5 , \"positions\" : [ { \"start\" : 1152 , \"end\" : 1162 }, { \"start\" : 1163 , \"end\" : 1167 }, { \"start\" : 1239 , \"end\" : 1249 }, { \"start\" : 1335 , \"end\" : 1345 }, { \"start\" : 1394 , \"end\" : 1404 } ] }","title":"mainLemmas"},{"location":"reference/output/keyphrase-extraction/#knowledge","text":"The knowledge array contains Knowledge Graph data for the items of the mainSyncons array. Its contents are described in the article about the output of full analysis .","title":"knowledge"},{"location":"reference/output/linguistic-analysis/","text":"Deep linguistic analysis output The deep linguistic analysis resource returns a JSON object with this format: { \"success\": Boolean success flag , \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , \"knowledge\": [], \"tokens\": [], \"phrases\": [], \"sentences\": [], \"paragraphs\": [] } } For the description of the contents , language and version properties, see the API resources output overview . The knowledge array contains Knowledge Graph data as a result of the semantic analysis process. Its contents are described in the article about the output of full analysis . The paragraphs , sentences , phrases and tokens arrays are produced by the text subdivision process . The items of the tokens array are then enriched by the other deep linguistic analysis processes: part-of-speech tagging , morphological analysis , lemmatization , syntactic analysis and semantic analysis . The contents of these arrays are described below. tokens The tokens array contains an item for every token detected. Each item has a format like this: { \"syncon\" : 62653 , \"start\" : 74 , \"end\" : 83 , \"type\" : \"NOU\" , \"lemma\" : \"long time\" , \"pos\" : \"NOUN\" , \"dependency\" : { \"id\" : 11 , \"head\" : 7 , \"label\" : \"nmod\" }, \"morphology\" : \"Number=Sing\" , \"paragraph\" : 0 , \"sentence\" : 0 , \"phrase\" : 4 , \"atoms\" : [ { \"start\" : 74 , \"end\" : 78 , \"type\" : \"ADJ\" , \"lemma\" : \"long\" }, { \"start\" : 79 , \"end\" : 83 , \"type\" : \"NOU\" , \"lemma\" : \"time\" } ] } The syncon property is the outcome of the semantic analysis process. Its value is the ID of the corresponding syncon in the Knowledge Graph. The -1 value is attributed to tokens that do not have a corresponding syncon. A positive value has a match in the value of the syncon property of an entry in the knowledge array. type is the result of custom part-of-speech tagging . lemma is the result of lemmatization . pos is the result of standard part-of-speech tagging . dependency is the result of syntactic analysis . id represents the index of the token in the text. dep specifies the dependency relation with another token according to the Universal Dependencies conventions . head identifies the token that receives the relation the relation. Its value corresponds to the value of the id property of another token, the only exception being the root token \u2014the one with the dep property set to root \u2014for which head and id have the same value. morphology is the result of morphological analysis . start , end , phrase , sentence and paragraph are the result of text subdivision process . start and end are the positions of the token text in the analyzed text, which is the value of the content property of the outer data object. phrase is the phrase containing the token; it's the zero-based index of the phrase in the phrases array. sentence is the sentence containing the token; it's the zero-based index of the sentence in the sentences array. paragraph is the paragraph containing the token; it's the zero-based index of the paragraph in the paragraphs array. In the case of collocations, the token object can contain the atoms array. There's an item for every word of the collocation in the atoms array and in each item of the atoms array: start and end are the result of text subdivision process . They represent the positions of the atom text in the analyzed text, which is the value of the content property of the outer data object type is the the result of custom part-of-speech tagging . lemma property is the result of lemmatization for to the word. Sometimes the semantic analysis process determines that a token is a named entity\u2014for example: a person's name\u2014even if there is no corresponding concept in the Knowledge Graph. In this case the syncon property is set to -1, but the token has an additional vsyn property. For example: { \"syncon\" : -1 , \"vsyn\" : { \"id\" : -436106 , \"parent\" : 73303 }, \"start\" : 0 , \"end\" : 19 , \"type\" : \"NPR.NPH\" , \"lemma\" : \"Mauricio Pochettino\" , ... This property, whose name means \"virtual syncon\", is an object with two properties: id is a negative number generated by the semantic analysis process and assigned to all tokens considered as occurrences of the same entity. It is not the ID of a Knowledge Graph syncon. parent is the number of a Knowledge Graph syncon which, conceptually, is the parent of the concept expressed by the token. For example, if the token has been recognized as a person's name, its parent is the concept of person . The parent syncon data is located in the knowledge array. phrases The phrases array is created and populated by the text subdivision process. It contains an item for every phrase detected. For example, the phrase: Michael Jordan was one of the best basketball players of all time . corresponds to an array item like this: { \"tokens\" : [ 7 , 8 , 9 ], \"type\" : \"PP\" , \"start\" : 54 , \"end\" : 65 } The tokens array contains the zero-based indexes of the constituent tokens. For example, token 7 is the 8th token. type specifies the phrase type . start and end are the positions of the phrase in the analyzed text, which is the value of the content property of the outer data object. sentences The sentences array is created and populated by the text subdivision process. It contains an item for every sentence detected. For example, this sentence: Michael Jordan was one of the best basketball players of all time. corresponds to an array item like this: { \"phrases\" : [ 0 , 1 , 2 , 3 , 4 , 5 ], \"start\" : 0 , \"end\" : 66 } The phrases array contains the zero-based indexes of the constituent phrases. start and end are the positions of the sentence in the analyzed text, which is the value of the content property of the outer data object. paragraphs The paragraphs array is created and populated by the text subdivision process. It contains an item for every paragraph detected. For example this text: Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half. Michael Jordan was also a baseball player and an actor. contains two paragraphs and the corresponding array is something like: \"paragraphs\" : [ { \"sentences\" : [ 0 , 1 ], \"start\" : 0 , \"end\" : 176 }, { \"sentences\" : [ 2 ], \"start\" : 177 , \"end\" : 232 } ] The sentences array in each item contains the zero-based indexes of the constituent sentences. start and end are the positions of the paragraph in the analyzed text, which is the value of the content property of the outer data object. knowledge The knowledge array contains Knowledge Graph data for the items of the tokens array. Its contents are described in the article about the output of full analysis .","title":"Deep linguistic analysis"},{"location":"reference/output/linguistic-analysis/#deep-linguistic-analysis-output","text":"The deep linguistic analysis resource returns a JSON object with this format: { \"success\": Boolean success flag , \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , \"knowledge\": [], \"tokens\": [], \"phrases\": [], \"sentences\": [], \"paragraphs\": [] } } For the description of the contents , language and version properties, see the API resources output overview . The knowledge array contains Knowledge Graph data as a result of the semantic analysis process. Its contents are described in the article about the output of full analysis . The paragraphs , sentences , phrases and tokens arrays are produced by the text subdivision process . The items of the tokens array are then enriched by the other deep linguistic analysis processes: part-of-speech tagging , morphological analysis , lemmatization , syntactic analysis and semantic analysis . The contents of these arrays are described below.","title":"Deep linguistic analysis output"},{"location":"reference/output/linguistic-analysis/#tokens","text":"The tokens array contains an item for every token detected. Each item has a format like this: { \"syncon\" : 62653 , \"start\" : 74 , \"end\" : 83 , \"type\" : \"NOU\" , \"lemma\" : \"long time\" , \"pos\" : \"NOUN\" , \"dependency\" : { \"id\" : 11 , \"head\" : 7 , \"label\" : \"nmod\" }, \"morphology\" : \"Number=Sing\" , \"paragraph\" : 0 , \"sentence\" : 0 , \"phrase\" : 4 , \"atoms\" : [ { \"start\" : 74 , \"end\" : 78 , \"type\" : \"ADJ\" , \"lemma\" : \"long\" }, { \"start\" : 79 , \"end\" : 83 , \"type\" : \"NOU\" , \"lemma\" : \"time\" } ] } The syncon property is the outcome of the semantic analysis process. Its value is the ID of the corresponding syncon in the Knowledge Graph. The -1 value is attributed to tokens that do not have a corresponding syncon. A positive value has a match in the value of the syncon property of an entry in the knowledge array. type is the result of custom part-of-speech tagging . lemma is the result of lemmatization . pos is the result of standard part-of-speech tagging . dependency is the result of syntactic analysis . id represents the index of the token in the text. dep specifies the dependency relation with another token according to the Universal Dependencies conventions . head identifies the token that receives the relation the relation. Its value corresponds to the value of the id property of another token, the only exception being the root token \u2014the one with the dep property set to root \u2014for which head and id have the same value. morphology is the result of morphological analysis . start , end , phrase , sentence and paragraph are the result of text subdivision process . start and end are the positions of the token text in the analyzed text, which is the value of the content property of the outer data object. phrase is the phrase containing the token; it's the zero-based index of the phrase in the phrases array. sentence is the sentence containing the token; it's the zero-based index of the sentence in the sentences array. paragraph is the paragraph containing the token; it's the zero-based index of the paragraph in the paragraphs array. In the case of collocations, the token object can contain the atoms array. There's an item for every word of the collocation in the atoms array and in each item of the atoms array: start and end are the result of text subdivision process . They represent the positions of the atom text in the analyzed text, which is the value of the content property of the outer data object type is the the result of custom part-of-speech tagging . lemma property is the result of lemmatization for to the word. Sometimes the semantic analysis process determines that a token is a named entity\u2014for example: a person's name\u2014even if there is no corresponding concept in the Knowledge Graph. In this case the syncon property is set to -1, but the token has an additional vsyn property. For example: { \"syncon\" : -1 , \"vsyn\" : { \"id\" : -436106 , \"parent\" : 73303 }, \"start\" : 0 , \"end\" : 19 , \"type\" : \"NPR.NPH\" , \"lemma\" : \"Mauricio Pochettino\" , ... This property, whose name means \"virtual syncon\", is an object with two properties: id is a negative number generated by the semantic analysis process and assigned to all tokens considered as occurrences of the same entity. It is not the ID of a Knowledge Graph syncon. parent is the number of a Knowledge Graph syncon which, conceptually, is the parent of the concept expressed by the token. For example, if the token has been recognized as a person's name, its parent is the concept of person . The parent syncon data is located in the knowledge array.","title":"tokens"},{"location":"reference/output/linguistic-analysis/#phrases","text":"The phrases array is created and populated by the text subdivision process. It contains an item for every phrase detected. For example, the phrase: Michael Jordan was one of the best basketball players of all time . corresponds to an array item like this: { \"tokens\" : [ 7 , 8 , 9 ], \"type\" : \"PP\" , \"start\" : 54 , \"end\" : 65 } The tokens array contains the zero-based indexes of the constituent tokens. For example, token 7 is the 8th token. type specifies the phrase type . start and end are the positions of the phrase in the analyzed text, which is the value of the content property of the outer data object.","title":"phrases"},{"location":"reference/output/linguistic-analysis/#sentences","text":"The sentences array is created and populated by the text subdivision process. It contains an item for every sentence detected. For example, this sentence: Michael Jordan was one of the best basketball players of all time. corresponds to an array item like this: { \"phrases\" : [ 0 , 1 , 2 , 3 , 4 , 5 ], \"start\" : 0 , \"end\" : 66 } The phrases array contains the zero-based indexes of the constituent phrases. start and end are the positions of the sentence in the analyzed text, which is the value of the content property of the outer data object.","title":"sentences"},{"location":"reference/output/linguistic-analysis/#paragraphs","text":"The paragraphs array is created and populated by the text subdivision process. It contains an item for every paragraph detected. For example this text: Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half. Michael Jordan was also a baseball player and an actor. contains two paragraphs and the corresponding array is something like: \"paragraphs\" : [ { \"sentences\" : [ 0 , 1 ], \"start\" : 0 , \"end\" : 176 }, { \"sentences\" : [ 2 ], \"start\" : 177 , \"end\" : 232 } ] The sentences array in each item contains the zero-based indexes of the constituent sentences. start and end are the positions of the paragraph in the analyzed text, which is the value of the content property of the outer data object.","title":"paragraphs"},{"location":"reference/output/linguistic-analysis/#knowledge","text":"The knowledge array contains Knowledge Graph data for the items of the tokens array. Its contents are described in the article about the output of full analysis .","title":"knowledge"},{"location":"reference/output/managed-errors/","text":"Manged errors In case of manged errors, the API resources return the HTTP 200 OK status and a JSON response with this format: { \"success\": false \"errors\": [ { \"code\": error code , \"message\": error message } ], } The Boolean property success indicates that processing was not successful while the error object contains the error details. An example of this is submitting an analysis or classification resource a JSON object without the text property: { \"errors\" : [ { \"code\" : \"PREPARE_DOCUMENT_FAILED\" , \"message\" : \"missing layout key in json\" } ], \"success\" : false }","title":"Managed errors"},{"location":"reference/output/managed-errors/#manged-errors","text":"In case of manged errors, the API resources return the HTTP 200 OK status and a JSON response with this format: { \"success\": false \"errors\": [ { \"code\": error code , \"message\": error message } ], } The Boolean property success indicates that processing was not successful while the error object contains the error details. An example of this is submitting an analysis or classification resource a JSON object without the text property: { \"errors\" : [ { \"code\" : \"PREPARE_DOCUMENT_FAILED\" , \"message\" : \"missing layout key in json\" } ], \"success\" : false }","title":"Manged errors"},{"location":"reference/output/relation-extraction/","text":"Relation extraction output The API resource performing relation extraction returns a JSON object with this format: { \"success\": Boolean success flag , \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , \"paragraphs\": [], \"sentences\": [], \"phrases\": [], \"tokens\": [], \"knowledge\": [], \"relations\": [] } } For the description of the contents , language and version properties, see the API resources output overview . The paragraphs , sentences , phrases and tokens arrays are produced by the text subdivision process . These sections are described in the article about the output of the deep linguistic analysis . The knowledge array contains Knowledge Graph data about the relations elements, as a result of the semantic analysis process. Its contents are described in the article about the output of full analysis . relations Each item of the relations array represents a verb plus the text elements that are in a semantic relation with it. These elements may specify arguments, adjuncts or subordinate clauses. For example, given this input text: John sent a letter to Mary. the relations array can contain an item like this: { \"verb\" : { \"text\" : \"sent\" , \"lemma\" : \"send\" , \"syncon\" : 68296 , \"phrase\" : 1 , \"type\" : \"\" , \"relevance\" : 15 }, \"related\" : [ { \"relation\" : \"sbj_who\" , \"text\" : \"John\" , \"lemma\" : \"John\" , \"syncon\" : -1 , \"type\" : \"NPH\" , \"phrase\" : 0 , \"relevance\" : 15 }, { \"relation\" : \"obj_what\" , \"text\" : \"a letter\" , \"lemma\" : \"letter\" , \"syncon\" : 29131 , \"type\" : \"wrk\" , \"phrase\" : 2 , \"relevance\" : 10 }, { \"relation\" : \"to_who\" , \"text\" : \"to Mary\" , \"lemma\" : \"Mary\" , \"syncon\" : -1 , \"type\" : \"NPH\" , \"phrase\" : 3 , \"relevance\" : 10 } ] } Common properties The verb object and the items of the related array share some properties. text if the portion of text corresponding to the element. phrase is the index of the phrase containing the element. The value must be interpreted as a pointer to an item of the phrases array, where the positions of the first and the last character of the phrase can be found. This information can be used for text highlighting. From the phrase it is possible to go back to the sentence it belongs to\u2014using the sentences array\u2014and from the sentence to the paragraph\u2014using the paragraphs array\u2014,or, going in the opposite direction, to find the tokens contained in the phrase \u2014using the tokens array. Subordinate clauses\u2014related items having the relation property set to sub \u2014do not have a one-to-one correspondence with a phrase. In that case, phrase has the conventional value -1. The syncon and lemma properties are the outcome of semantic analysis and lemmatization respectively. These are exactly the same processes carried out during deep linguistic analysis . Value -1 for syncon means the concept doesn't have a correspondent in the expert.ai Knowledge Graph . This can happen with: entities having a proper noun that are recognized through heuristics (e.g. John Smith ) parts-of-speech that ar not mapped in the Knowledge Graph like pronouns (e.g., them ) subordinate clauses like quotes (e.g., John said: \" I will do it! \" ) In cases 1 and 2 lemma is an empty string. relevance is an indicator of the importance of the element in text. Its values ranges from 1 to 15. When element importance cannot be determined, relevance has the conventional value -1. verb The verb object is always present and it represents the verb. type is the verb type. When set, it can be one of the following: Verb type Description CPL to be used as a connection as in John is a smart guy MOV Verb of movement like to go SAY Verb of communication like to say related The items of the related array represent text elements that are related to the verb. relation is the type of relation and can be one of the following: Possible values of relation sbj_who sbj_what obj_who obj_what is_who is_what to_who to_what using_what preposition* + _what preposition* + _who sub ** when where to_where from_where in_where which_way how of_age limited_to * Prepositions are expressed in the language specified for the analysis, so, for example, a possible value in the case of German language could be auf_what . Multi-word prepositional expressions like according to , in front of , ecc., are written in compact form ( accordingto , infrontof ). ** The sub relation type is used for subordinate clauses. type identifies the kind of element. Possible values can be uppercase or lowercase. Uppercase corresponds to named entities, lowercase to generic entities. Relations can be recursive: a related item can be related to another item and so on. In this case an item of the related array can contain a related array. For example, given this input text: Mireille placed the plant pot on the landing at the top of the stairs. relations can be like this: \"relations\" : [ { \"related\" : [ { \"lemma\" : \"Mireille\" , \"phrase\" : 0 , \"relation\" : \"sbj_who\" , \"relevance\" : 14 , \"syncon\" : -1 , \"text\" : \"Mireille\" , \"type\" : \"NPH\" }, { \"lemma\" : \"pot\" , \"phrase\" : 2 , \"relation\" : \"obj_what\" , \"relevance\" : 15 , \"syncon\" : 18506 , \"text\" : \"the plant pot\" , \"type\" : \"prd\" }, { \"lemma\" : \"landing\" , \"phrase\" : 3 , \"relation\" : \"on_what\" , \"relevance\" : 5 , \"syncon\" : 16859 , \"text\" : \"on the landing\" , \"type\" : \"bld\" }, { \"lemma\" : \"top\" , \"phrase\" : 4 , \"related\" : [ { \"lemma\" : \"stairs\" , \"phrase\" : 5 , \"relation\" : \"of_what\" , \"relevance\" : 1 , \"syncon\" : 20016 , \"text\" : \"of the stairs\" , \"type\" : \"bld\" } ], \"relation\" : \"at_what\" , \"relevance\" : -1 , \"syncon\" : 37732 , \"text\" : \"at the top\" , \"type\" : \"\" } ], \"verb\" : { \"lemma\" : \"place\" , \"phrase\" : 1 , \"relevance\" : 15 , \"syncon\" : 68498 , \"text\" : \"placed\" , \"type\" : \"\" } } ] knowledge The knowledge array contains Knowledge Graph data for the items of the relations array. Its contents are described in the article about the output of full analysis .","title":"Relation extraction"},{"location":"reference/output/relation-extraction/#relation-extraction-output","text":"The API resource performing relation extraction returns a JSON object with this format: { \"success\": Boolean success flag , \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , \"paragraphs\": [], \"sentences\": [], \"phrases\": [], \"tokens\": [], \"knowledge\": [], \"relations\": [] } } For the description of the contents , language and version properties, see the API resources output overview . The paragraphs , sentences , phrases and tokens arrays are produced by the text subdivision process . These sections are described in the article about the output of the deep linguistic analysis . The knowledge array contains Knowledge Graph data about the relations elements, as a result of the semantic analysis process. Its contents are described in the article about the output of full analysis .","title":"Relation extraction output"},{"location":"reference/output/relation-extraction/#relations","text":"Each item of the relations array represents a verb plus the text elements that are in a semantic relation with it. These elements may specify arguments, adjuncts or subordinate clauses. For example, given this input text: John sent a letter to Mary. the relations array can contain an item like this: { \"verb\" : { \"text\" : \"sent\" , \"lemma\" : \"send\" , \"syncon\" : 68296 , \"phrase\" : 1 , \"type\" : \"\" , \"relevance\" : 15 }, \"related\" : [ { \"relation\" : \"sbj_who\" , \"text\" : \"John\" , \"lemma\" : \"John\" , \"syncon\" : -1 , \"type\" : \"NPH\" , \"phrase\" : 0 , \"relevance\" : 15 }, { \"relation\" : \"obj_what\" , \"text\" : \"a letter\" , \"lemma\" : \"letter\" , \"syncon\" : 29131 , \"type\" : \"wrk\" , \"phrase\" : 2 , \"relevance\" : 10 }, { \"relation\" : \"to_who\" , \"text\" : \"to Mary\" , \"lemma\" : \"Mary\" , \"syncon\" : -1 , \"type\" : \"NPH\" , \"phrase\" : 3 , \"relevance\" : 10 } ] }","title":"relations"},{"location":"reference/output/relation-extraction/#common-properties","text":"The verb object and the items of the related array share some properties. text if the portion of text corresponding to the element. phrase is the index of the phrase containing the element. The value must be interpreted as a pointer to an item of the phrases array, where the positions of the first and the last character of the phrase can be found. This information can be used for text highlighting. From the phrase it is possible to go back to the sentence it belongs to\u2014using the sentences array\u2014and from the sentence to the paragraph\u2014using the paragraphs array\u2014,or, going in the opposite direction, to find the tokens contained in the phrase \u2014using the tokens array. Subordinate clauses\u2014related items having the relation property set to sub \u2014do not have a one-to-one correspondence with a phrase. In that case, phrase has the conventional value -1. The syncon and lemma properties are the outcome of semantic analysis and lemmatization respectively. These are exactly the same processes carried out during deep linguistic analysis . Value -1 for syncon means the concept doesn't have a correspondent in the expert.ai Knowledge Graph . This can happen with: entities having a proper noun that are recognized through heuristics (e.g. John Smith ) parts-of-speech that ar not mapped in the Knowledge Graph like pronouns (e.g., them ) subordinate clauses like quotes (e.g., John said: \" I will do it! \" ) In cases 1 and 2 lemma is an empty string. relevance is an indicator of the importance of the element in text. Its values ranges from 1 to 15. When element importance cannot be determined, relevance has the conventional value -1.","title":"Common properties"},{"location":"reference/output/relation-extraction/#verb","text":"The verb object is always present and it represents the verb. type is the verb type. When set, it can be one of the following: Verb type Description CPL to be used as a connection as in John is a smart guy MOV Verb of movement like to go SAY Verb of communication like to say","title":"verb"},{"location":"reference/output/relation-extraction/#related","text":"The items of the related array represent text elements that are related to the verb. relation is the type of relation and can be one of the following: Possible values of relation sbj_who sbj_what obj_who obj_what is_who is_what to_who to_what using_what preposition* + _what preposition* + _who sub ** when where to_where from_where in_where which_way how of_age limited_to * Prepositions are expressed in the language specified for the analysis, so, for example, a possible value in the case of German language could be auf_what . Multi-word prepositional expressions like according to , in front of , ecc., are written in compact form ( accordingto , infrontof ). ** The sub relation type is used for subordinate clauses. type identifies the kind of element. Possible values can be uppercase or lowercase. Uppercase corresponds to named entities, lowercase to generic entities. Relations can be recursive: a related item can be related to another item and so on. In this case an item of the related array can contain a related array. For example, given this input text: Mireille placed the plant pot on the landing at the top of the stairs. relations can be like this: \"relations\" : [ { \"related\" : [ { \"lemma\" : \"Mireille\" , \"phrase\" : 0 , \"relation\" : \"sbj_who\" , \"relevance\" : 14 , \"syncon\" : -1 , \"text\" : \"Mireille\" , \"type\" : \"NPH\" }, { \"lemma\" : \"pot\" , \"phrase\" : 2 , \"relation\" : \"obj_what\" , \"relevance\" : 15 , \"syncon\" : 18506 , \"text\" : \"the plant pot\" , \"type\" : \"prd\" }, { \"lemma\" : \"landing\" , \"phrase\" : 3 , \"relation\" : \"on_what\" , \"relevance\" : 5 , \"syncon\" : 16859 , \"text\" : \"on the landing\" , \"type\" : \"bld\" }, { \"lemma\" : \"top\" , \"phrase\" : 4 , \"related\" : [ { \"lemma\" : \"stairs\" , \"phrase\" : 5 , \"relation\" : \"of_what\" , \"relevance\" : 1 , \"syncon\" : 20016 , \"text\" : \"of the stairs\" , \"type\" : \"bld\" } ], \"relation\" : \"at_what\" , \"relevance\" : -1 , \"syncon\" : 37732 , \"text\" : \"at the top\" , \"type\" : \"\" } ], \"verb\" : { \"lemma\" : \"place\" , \"phrase\" : 1 , \"relevance\" : 15 , \"syncon\" : 68498 , \"text\" : \"placed\" , \"type\" : \"\" } } ]","title":"related"},{"location":"reference/output/relation-extraction/#knowledge","text":"The knowledge array contains Knowledge Graph data for the items of the relations array. Its contents are described in the article about the output of full analysis .","title":"knowledge"},{"location":"reference/output/self-documentation/","text":"Self-documentation resources output Overview contexts , taxonomies , detectors and the child resources of taxonomies are API self-documentation resources they return information on the contexts available for document analysis, the taxonomies available for document classification and the detectors available for information detection. See the format of the resources' endpoints in the reference page . contexts The contexts resource returns a JSON object like this: { \"contexts\" : [ { \"description\" : \"Standard context\" , \"languages\" : [ { \"analyses\" : [ \"disambiguation\" , \"relevants\" , \"entities\" , \"sentiment\" , \"relations\" ], \"code\" : \"en\" , \"name\" : \"English\" }, { \"analyses\" : [ \"disambiguation\" , \"relevants\" , \"entities\" , \"relations\" ], \"code\" : \"es\" , \"name\" : \"Spanish\" }, { \"analyses\" : [ \"disambiguation\" , \"relevants\" , \"entities\" , \"relations\" ], \"code\" : \"fr\" , \"name\" : \"French\" }, { \"analyses\" : [ \"disambiguation\" , \"relevants\" , \"entities\" , \"relations\" ], \"code\" : \"de\" , \"name\" : \"German\" }, { \"analyses\" : [ \"disambiguation\" , \"relevants\" , \"entities\" , \"relations\" ], \"code\" : \"it\" , \"name\" : \"Italian\" } ], \"name\" : \"standard\" } ] } The contexts array contains one object for each context. The name property is the exact name that must be used in the endpoint of analysis resources. The languages array documents the context's capabilities in terms of the analyses available for each of the supported languages. code id the ISO 639-1 language code , analyses is an array listing available analyses for the language. The value of the analyses are the exact names that must be used in the endpoint of partial analysis resources. taxonomies The taxonomies resource returns a JSON object like this: { \"taxonomies\" : [ { \"description\" : \"The iptc document classification resources classify texts based on the IPTC Media Topics taxonomy\" , \"languages\" : [ { \"code\" : \"en\" , \"name\" : \"English\" }, { \"code\" : \"es\" , \"name\" : \"Spanish\" }, { \"code\" : \"fr\" , \"name\" : \"French\" }, { \"code\" : \"de\" , \"name\" : \"German\" }, { \"code\" : \"it\" , \"name\" : \"Italian\" } ], \"name\" : \"iptc\" }, { \"contract\" : \"https://github.com/therealexpertai/nlapi-openapi-specification/blob/master/geotax-w-geojson.yaml\" , \"description\" : \"The geotax document classification resources recognize geographic places cited in the text and return corresponding countries' names. In addition, when requested with a specific query-string parameter, they return extra-data containing equivalent GeoJSON objects. Refer to the specific OpenAPI document (https://github.com/therealexpertai/nlapi-openapi-specification/blob/master/geotax-w-geojson.yaml) for this special use of the API resources.\" , \"languages\" : [ { \"code\" : \"en\" , \"name\" : \"English\" }, { \"code\" : \"es\" , \"name\" : \"Spanish\" }, { \"code\" : \"fr\" , \"name\" : \"French\" }, { \"code\" : \"de\" , \"name\" : \"German\" }, { \"code\" : \"it\" , \"name\" : \"Italian\" } ], \"name\" : \"geotax\" }, { \"description\" : \"The emotional-traits document classification resources classify documents in terms of feelings like joy, surprise, irritation, etc. expressed in the text. In addition, when requested with a specific query-string parameter, they return extra-data containing the main groups to which the emotional traits belong. Especially with longer texts, main groups are an useful abstract of the detailed classification. Refer to the specific OpenAPI document (https://github.com/therealexpertai/nlapi-openapi-specification/blob/master/emotional-traits-w-main-groups.yaml) for this special use of the API resources.\" , \"languages\" : [ { \"code\" : \"en\" , \"name\" : \"English\" } ], \"name\" : \"emotional-traits\" }, { \"description\" : \"The behavioral-traits document classification resources classify document in terms of personality traits like curiosity, honesty, negativity, etc. the text deals with.\" , \"languages\" : [ { \"code\" : \"en\" , \"name\" : \"English\" } ], \"name\" : \"behavioral-traits\" } ] } The taxonomies array contains one object for each taxonomy. The name property is the exact name that must be used in the endpoint of classification resources and also in the endpoint of taxonomy child resources. The contract property, which is optional, contains the URL of the OpenAPI document describing a special use of the taxonomy. For example, in the case of the geotax taxonomy, the OpenAPI document defines the way to use the classification resource to obtain a GeoJSON output . The languages array lists the languages for which the taxonomy is available. code id the ISO 639-1 language code . taxonomies child resources taxonomies/{taxonomy}/{language} resources return the category tree \u200bfor a given taxonomy in a given language. Their output is a JSON object like this: { \"success\" : true , \"data\" : [ { \"namespace\" : \"cat-geo_en_1.0\" , \"taxonomy\" : [ { \"id\" : \"GEO_TAX\" , \"label\" : \"Geo Taxonomy\" , \"categories\" : [ { \"id\" : \"001.\" , \"label\" : \"Afghanistan\" }, { \"id\" : \"002.\" , \"label\" : \"Albania\" }, { \"id\" : \"003.\" , \"label\" : \"Algeria\" }, { \"id\" : \"004.\" , \"label\" : \"Andorra\" }, { \"id\" : \"005.\" , \"label\" : \"Angola\" }, { \"id\" : \"006.\" , \"label\" : \"Antigua and Barbuda\" }, { \"id\" : \"007.\" , \"label\" : \"Argentina\" }, { \"id\" : \"008.\" , \"label\" : \"Armenia\" }, { \"id\" : \"009.\" , \"label\" : \"Australia\" }, { \"id\" : \"010.\" , \"label\" : \"Austria\" }, { \"id\" : \"011.\" , \"label\" : \"Azerbaijan\" }, { \"id\" : \"012.\" , \"label\" : \"Bahamas\" }, { \"id\" : \"013.\" , \"label\" : \"Bahrain\" }, { \"id\" : \"014.\" , \"label\" : \"Bangladesh\" }, { \"id\" : \"015.\" , \"label\" : \"Barbados\" }, { \"id\" : \"016.\" , \"label\" : \"Belarus\" }, { \"id\" : \"017.\" , \"label\" : \"Belgium\" }, { \"id\" : \"018.\" , \"label\" : \"Belize\" }, { \"id\" : \"019.\" , \"label\" : \"Benin\" }, { \"id\" : \"020.\" , \"label\" : \"Bhutan\" }, { \"id\" : \"021.\" , \"label\" : \"Bolivia\" }, { \"id\" : \"022.\" , \"label\" : \"Bosnia-Herzegovina\" }, { \"id\" : \"023.\" , \"label\" : \"Botswana\" }, { \"id\" : \"024.\" , \"label\" : \"Brazil\" }, { \"id\" : \"025.\" , \"label\" : \"Brunei\" }, { \"id\" : \"026.\" , \"label\" : \"Bulgaria\" }, { \"id\" : \"027.\" , \"label\" : \"Burkina Faso\" }, { \"id\" : \"028.\" , \"label\" : \"Burundi\" }, { \"id\" : \"029.\" , \"label\" : \"Cambodia\" }, { \"id\" : \"030.\" , \"label\" : \"Cameroon\" }, { \"id\" : \"031.\" , \"label\" : \"Canada\" }, { \"id\" : \"032.\" , \"label\" : \"Cape Verde\" }, { \"id\" : \"033.\" , \"label\" : \"Central African Republic\" }, { \"id\" : \"034.\" , \"label\" : \"Chad\" }, { \"id\" : \"035.\" , \"label\" : \"Chile\" }, { \"id\" : \"036.\" , \"label\" : \"China\" }, { \"id\" : \"037.\" , \"label\" : \"Colombia\" }, { \"id\" : \"038.\" , \"label\" : \"Comoros\" }, { \"id\" : \"039.\" , \"label\" : \"Costa Rica\" }, { \"id\" : \"040.\" , \"label\" : \"C\u00f4te d'Ivoire\" }, { \"id\" : \"041.\" , \"label\" : \"Croatia\" }, { \"id\" : \"042.\" , \"label\" : \"Cuba\" }, { \"id\" : \"043.\" , \"label\" : \"Cyprus\" }, { \"id\" : \"044.\" , \"label\" : \"Czech Republic\" }, { \"id\" : \"045.\" , \"label\" : \"Democratic Republic of Congo\" }, { \"id\" : \"046.\" , \"label\" : \"Denmark\" }, { \"id\" : \"047.\" , \"label\" : \"Djibouti\" }, { \"id\" : \"048.\" , \"label\" : \"Dominica\" }, { \"id\" : \"049.\" , \"label\" : \"Dominican Republic\" }, { \"id\" : \"050.\" , \"label\" : \"East Timor\" }, { \"id\" : \"051.\" , \"label\" : \"Ecuador\" }, { \"id\" : \"052.\" , \"label\" : \"Egypt\" }, { \"id\" : \"053.\" , \"label\" : \"El Salvador\" }, { \"id\" : \"054.\" , \"label\" : \"Equatorial Guinea\" }, { \"id\" : \"055.\" , \"label\" : \"Eritrea\" }, { \"id\" : \"056.\" , \"label\" : \"Estonia\" }, { \"id\" : \"057.\" , \"label\" : \"Ethiopia\" }, { \"id\" : \"058.\" , \"label\" : \"Fiji\" }, { \"id\" : \"059.\" , \"label\" : \"Finland\" }, { \"id\" : \"060.\" , \"label\" : \"France\" }, { \"id\" : \"061.\" , \"label\" : \"Gabon\" }, { \"id\" : \"062.\" , \"label\" : \"Gambia\" }, { \"id\" : \"063.\" , \"label\" : \"Georgia\" }, { \"id\" : \"064.\" , \"label\" : \"Germany\" }, { \"id\" : \"065.\" , \"label\" : \"Ghana\" }, { \"id\" : \"066.\" , \"label\" : \"Greece\" }, { \"id\" : \"067.\" , \"label\" : \"Grenada\" }, { \"id\" : \"068.\" , \"label\" : \"Guatemala\" }, { \"id\" : \"069.\" , \"label\" : \"Guinea\" }, { \"id\" : \"070.\" , \"label\" : \"Guinea-Bissau\" }, { \"id\" : \"071.\" , \"label\" : \"Guyana\" }, { \"id\" : \"072.\" , \"label\" : \"Haiti\" }, { \"id\" : \"073.\" , \"label\" : \"Honduras\" }, { \"id\" : \"074.\" , \"label\" : \"Hungary\" }, { \"id\" : \"075.\" , \"label\" : \"Iceland\" }, { \"id\" : \"076.\" , \"label\" : \"India\" }, { \"id\" : \"077.\" , \"label\" : \"Indonesia\" }, { \"id\" : \"078.\" , \"label\" : \"Iran\" }, { \"id\" : \"079.\" , \"label\" : \"Iraq\" }, { \"id\" : \"080.\" , \"label\" : \"Ireland\" }, { \"id\" : \"081.\" , \"label\" : \"Israel\" }, { \"id\" : \"082.\" , \"label\" : \"Italy\" }, { \"id\" : \"083.\" , \"label\" : \"Jamaica\" }, { \"id\" : \"084.\" , \"label\" : \"Japan\" }, { \"id\" : \"085.\" , \"label\" : \"Jordan\" }, { \"id\" : \"086.\" , \"label\" : \"Kazakhstan\" }, { \"id\" : \"087.\" , \"label\" : \"Kenya\" }, { \"id\" : \"088.\" , \"label\" : \"Kiribati\" }, { \"id\" : \"089.\" , \"label\" : \"Kosovo\" }, { \"id\" : \"090.\" , \"label\" : \"Kuwait\" }, { \"id\" : \"091.\" , \"label\" : \"Kyrgyzstan\" }, { \"id\" : \"092.\" , \"label\" : \"Laos\" }, { \"id\" : \"093.\" , \"label\" : \"Latvia\" }, { \"id\" : \"094.\" , \"label\" : \"Lebanon\" }, { \"id\" : \"095.\" , \"label\" : \"Lesotho\" }, { \"id\" : \"096.\" , \"label\" : \"Liberia\" }, { \"id\" : \"097.\" , \"label\" : \"Libya\" }, { \"id\" : \"098.\" , \"label\" : \"Liechtenstein\" }, { \"id\" : \"099.\" , \"label\" : \"Lithuania\" }, { \"id\" : \"100.\" , \"label\" : \"Luxemburg\" }, { \"id\" : \"101.\" , \"label\" : \"Macedonia\" }, { \"id\" : \"102.\" , \"label\" : \"Madagascar\" }, { \"id\" : \"103.\" , \"label\" : \"Malawi\" }, { \"id\" : \"104.\" , \"label\" : \"Malaysia\" }, { \"id\" : \"105.\" , \"label\" : \"Maldives\" }, { \"id\" : \"106.\" , \"label\" : \"Mali\" }, { \"id\" : \"107.\" , \"label\" : \"Malta\" }, { \"id\" : \"108.\" , \"label\" : \"Marshall Islands\" }, { \"id\" : \"109.\" , \"label\" : \"Mauritania\" }, { \"id\" : \"110.\" , \"label\" : \"Mauritius\" }, { \"id\" : \"111.\" , \"label\" : \"Mexico\" }, { \"id\" : \"112.\" , \"label\" : \"Micronesia\" }, { \"id\" : \"113.\" , \"label\" : \"Moldova\" }, { \"id\" : \"114.\" , \"label\" : \"Monaco\" }, { \"id\" : \"115.\" , \"label\" : \"Mongolia\" }, { \"id\" : \"116.\" , \"label\" : \"Montenegro\" }, { \"id\" : \"117.\" , \"label\" : \"Morocco\" }, { \"id\" : \"118.\" , \"label\" : \"Mozambique\" }, { \"id\" : \"119.\" , \"label\" : \"Myanmar\" }, { \"id\" : \"120.\" , \"label\" : \"Namibia\" }, { \"id\" : \"121.\" , \"label\" : \"Nauru\" }, { \"id\" : \"122.\" , \"label\" : \"Nepal\" }, { \"id\" : \"123.\" , \"label\" : \"Netherlands\" }, { \"id\" : \"124.\" , \"label\" : \"New Zealand\" }, { \"id\" : \"125.\" , \"label\" : \"Nicaragua\" }, { \"id\" : \"126.\" , \"label\" : \"Niger\" }, { \"id\" : \"127.\" , \"label\" : \"Nigeria\" }, { \"id\" : \"128.\" , \"label\" : \"North Korea\" }, { \"id\" : \"129.\" , \"label\" : \"Norway\" }, { \"id\" : \"130.\" , \"label\" : \"Oman\" }, { \"id\" : \"131.\" , \"label\" : \"Pakistan\" }, { \"id\" : \"132.\" , \"label\" : \"Palau\" }, { \"id\" : \"133.\" , \"label\" : \"Panama\" }, { \"id\" : \"134.\" , \"label\" : \"Papua New Guinea\" }, { \"id\" : \"135.\" , \"label\" : \"Paraguay\" }, { \"id\" : \"136.\" , \"label\" : \"Peru\" }, { \"id\" : \"137.\" , \"label\" : \"Philippines\" }, { \"id\" : \"138.\" , \"label\" : \"Poland\" }, { \"id\" : \"139.\" , \"label\" : \"Portugal\" }, { \"id\" : \"140.\" , \"label\" : \"Qatar\" }, { \"id\" : \"141.\" , \"label\" : \"Republic of Congo\" }, { \"id\" : \"142.\" , \"label\" : \"Romania\" }, { \"id\" : \"143.\" , \"label\" : \"Russia\" }, { \"id\" : \"144.\" , \"label\" : \"Rwanda\" }, { \"id\" : \"145.\" , \"label\" : \"Saint Kitts and Nevis\" }, { \"id\" : \"146.\" , \"label\" : \"Saint Lucia\" }, { \"id\" : \"147.\" , \"label\" : \"Saint Vincent and the Grenadines\" }, { \"id\" : \"148.\" , \"label\" : \"Samoa\" }, { \"id\" : \"149.\" , \"label\" : \"San Marino\" }, { \"id\" : \"150.\" , \"label\" : \"S\u00e3o Tom\u00e9 and Pr\u00edncipe\" }, { \"id\" : \"151.\" , \"label\" : \"Saudi Arabia\" }, { \"id\" : \"152.\" , \"label\" : \"Senegal\" }, { \"id\" : \"153.\" , \"label\" : \"Serbia\" }, { \"id\" : \"154.\" , \"label\" : \"Seychelles\" }, { \"id\" : \"155.\" , \"label\" : \"Sierra Leone\" }, { \"id\" : \"156.\" , \"label\" : \"Singapore\" }, { \"id\" : \"157.\" , \"label\" : \"Slovakia\" }, { \"id\" : \"158.\" , \"label\" : \"Slovenia\" }, { \"id\" : \"159.\" , \"label\" : \"Solomon Islands\" }, { \"id\" : \"160.\" , \"label\" : \"Somalia\" }, { \"id\" : \"161.\" , \"label\" : \"South Africa\" }, { \"id\" : \"162.\" , \"label\" : \"South Korea\" }, { \"id\" : \"163.\" , \"label\" : \"Spain\" }, { \"id\" : \"164.\" , \"label\" : \"Sri Lanka\" }, { \"id\" : \"165.\" , \"label\" : \"Sudan\" }, { \"id\" : \"166.\" , \"label\" : \"Suriname\" }, { \"id\" : \"167.\" , \"label\" : \"Swaziland\" }, { \"id\" : \"168.\" , \"label\" : \"Sweden\" }, { \"id\" : \"169.\" , \"label\" : \"Switzerland\" }, { \"id\" : \"170.\" , \"label\" : \"Syria\" }, { \"id\" : \"171.\" , \"label\" : \"Tajikistan\" }, { \"id\" : \"172.\" , \"label\" : \"Tanzania\" }, { \"id\" : \"173.\" , \"label\" : \"Thailand\" }, { \"id\" : \"174.\" , \"label\" : \"Togo\" }, { \"id\" : \"175.\" , \"label\" : \"Tonga\" }, { \"id\" : \"176.\" , \"label\" : \"Trinidad and Tobago\" }, { \"id\" : \"177.\" , \"label\" : \"Tunisia\" }, { \"id\" : \"178.\" , \"label\" : \"Turkey\" }, { \"id\" : \"179.\" , \"label\" : \"Turkmenistan\" }, { \"id\" : \"180.\" , \"label\" : \"Tuvalu\" }, { \"id\" : \"181.\" , \"label\" : \"Uganda\" }, { \"id\" : \"182.\" , \"label\" : \"Ukraine\" }, { \"id\" : \"183.\" , \"label\" : \"United Arab Emirates\" }, { \"id\" : \"184.\" , \"label\" : \"United Kingdom\" , \"categories\" : [ { \"id\" : \"18401.\" , \"label\" : \"England\" }, { \"id\" : \"18402.\" , \"label\" : \"Northern Ireland\" }, { \"id\" : \"18403.\" , \"label\" : \"Scotland\" }, { \"id\" : \"18404.\" , \"label\" : \"Wales\" } ] }, { \"id\" : \"185.\" , \"label\" : \"United States of America\" , \"categories\" : [ { \"id\" : \"18501.\" , \"label\" : \"Alabama\" }, { \"id\" : \"18502.\" , \"label\" : \"Alaska\" }, { \"id\" : \"18503.\" , \"label\" : \"Arizona\" }, { \"id\" : \"18504.\" , \"label\" : \"Arkansas\" }, { \"id\" : \"18505.\" , \"label\" : \"California\" }, { \"id\" : \"18506.\" , \"label\" : \"Colorado\" }, { \"id\" : \"18507.\" , \"label\" : \"Connecticut\" }, { \"id\" : \"18508.\" , \"label\" : \"Delaware\" }, { \"id\" : \"18509.\" , \"label\" : \"District of Columbia\" }, { \"id\" : \"18510.\" , \"label\" : \"Florida\" }, { \"id\" : \"18511.\" , \"label\" : \"Georgia\" }, { \"id\" : \"18512.\" , \"label\" : \"Hawaii\" }, { \"id\" : \"18513.\" , \"label\" : \"Idaho\" }, { \"id\" : \"18514.\" , \"label\" : \"Illinois\" }, { \"id\" : \"18515.\" , \"label\" : \"Indiana\" }, { \"id\" : \"18516.\" , \"label\" : \"Iowa\" }, { \"id\" : \"18517.\" , \"label\" : \"Kansas\" }, { \"id\" : \"18518.\" , \"label\" : \"Kentucky\" }, { \"id\" : \"18519.\" , \"label\" : \"Louisiana\" }, { \"id\" : \"18520.\" , \"label\" : \"Maine\" }, { \"id\" : \"18521.\" , \"label\" : \"Maryland\" }, { \"id\" : \"18522.\" , \"label\" : \"Massachusetts\" }, { \"id\" : \"18523.\" , \"label\" : \"Michigan\" }, { \"id\" : \"18524.\" , \"label\" : \"Minnesota\" }, { \"id\" : \"18525.\" , \"label\" : \"Mississippi\" }, { \"id\" : \"18526.\" , \"label\" : \"Missouri\" }, { \"id\" : \"18527.\" , \"label\" : \"Montana\" }, { \"id\" : \"18528.\" , \"label\" : \"Nebraska\" }, { \"id\" : \"18529.\" , \"label\" : \"Nevada\" }, { \"id\" : \"18530.\" , \"label\" : \"New Hampshire\" }, { \"id\" : \"18531.\" , \"label\" : \"New Jersey\" }, { \"id\" : \"18532.\" , \"label\" : \"New Mexico\" }, { \"id\" : \"18533.\" , \"label\" : \"New York State\" }, { \"id\" : \"18534.\" , \"label\" : \"North Carolina\" }, { \"id\" : \"18535.\" , \"label\" : \"North Dakota\" }, { \"id\" : \"18536.\" , \"label\" : \"Ohio\" }, { \"id\" : \"18537.\" , \"label\" : \"Oklahoma\" }, { \"id\" : \"18538.\" , \"label\" : \"Oregon\" }, { \"id\" : \"18539.\" , \"label\" : \"Pennsylvania\" }, { \"id\" : \"18540.\" , \"label\" : \"Rhode Island\" }, { \"id\" : \"18541.\" , \"label\" : \"South Carolina\" }, { \"id\" : \"18542.\" , \"label\" : \"South Dakota\" }, { \"id\" : \"18543.\" , \"label\" : \"Tennessee\" }, { \"id\" : \"18544.\" , \"label\" : \"Texas\" }, { \"id\" : \"18545.\" , \"label\" : \"Utah\" }, { \"id\" : \"18546.\" , \"label\" : \"Vermont\" }, { \"id\" : \"18547.\" , \"label\" : \"Virginia\" }, { \"id\" : \"18548.\" , \"label\" : \"Washington State\" }, { \"id\" : \"18549.\" , \"label\" : \"West Virginia\" }, { \"id\" : \"18550.\" , \"label\" : \"Wisconsin\" }, { \"id\" : \"18551.\" , \"label\" : \"Wyoming\" } ] }, { \"id\" : \"186.\" , \"label\" : \"Uruguay\" }, { \"id\" : \"187.\" , \"label\" : \"Uzbekistan\" }, { \"id\" : \"188.\" , \"label\" : \"Vanuatu\" }, { \"id\" : \"189.\" , \"label\" : \"Vatican City\" }, { \"id\" : \"190.\" , \"label\" : \"Venezuela\" }, { \"id\" : \"191.\" , \"label\" : \"Vietnam\" }, { \"id\" : \"192.\" , \"label\" : \"Yemen\" }, { \"id\" : \"193.\" , \"label\" : \"Zambia\" }, { \"id\" : \"194.\" , \"label\" : \"Zimbabwe\" } ] } ] } ] } The Boolean property success indicates that processing was successful while the data array contains the results. data contains one object for each categories' tree because, in theory, a taxonomy can have more than one. namespace is the name of the software module containing the taxonomy, while taxonomy is an array of possibly nested categories. Each category is an object with these properties: id : category identifier label : category label categories : sub-categories There can be several categories with the same value for id in the categories' tree, but there cannot be two or more categories with the same id having the same parent category. detectors The detectors resource returns a JSON object like this: { \"detectors\" : [ { \"contract\" : \"https://github.com/therealexpertai/nlapi-openapi-specification/blob/master/pii.yaml\" , \"description\" : \"Personally Identifiable Information (PII) detector\" , \"languages\" : [ { \"code\" : \"en\" , \"name\" : \"English\" }, { \"code\" : \"it\" , \"name\" : \"Italian\" } ], \"name\" : \"pii\" }, { \"contract\" : \"https://github.com/therealexpertai/nlapi-openapi-specification/blob/master/writeprint.yaml\" , \"description\" : \"Writeprint detector\" , \"languages\" : [ { \"code\" : \"en\" , \"name\" : \"English\" }, { \"code\" : \"it\" , \"name\" : \"Italian\" }, { \"code\" : \"es\" , \"name\" : \"Spanish\" }, { \"code\" : \"fr\" , \"name\" : \"French\" }, { \"code\" : \"de\" , \"name\" : \"German\" } ], \"name\" : \"writeprint\" } ] } The detectors array contains one object for each detector. The name property is the exact name that must be used in the endpoint of information detection resources. The languages array lists the languages for which the taxonomy is available. code id the ISO 639-1 language code . Each detector has a specific output which is formally described in an OpenAPI document. The contract property contains the URL of that document.","title":"Self-documentation resources"},{"location":"reference/output/self-documentation/#self-documentation-resources-output","text":"","title":"Self-documentation resources output"},{"location":"reference/output/self-documentation/#overview","text":"contexts , taxonomies , detectors and the child resources of taxonomies are API self-documentation resources they return information on the contexts available for document analysis, the taxonomies available for document classification and the detectors available for information detection. See the format of the resources' endpoints in the reference page .","title":"Overview"},{"location":"reference/output/self-documentation/#contexts","text":"The contexts resource returns a JSON object like this: { \"contexts\" : [ { \"description\" : \"Standard context\" , \"languages\" : [ { \"analyses\" : [ \"disambiguation\" , \"relevants\" , \"entities\" , \"sentiment\" , \"relations\" ], \"code\" : \"en\" , \"name\" : \"English\" }, { \"analyses\" : [ \"disambiguation\" , \"relevants\" , \"entities\" , \"relations\" ], \"code\" : \"es\" , \"name\" : \"Spanish\" }, { \"analyses\" : [ \"disambiguation\" , \"relevants\" , \"entities\" , \"relations\" ], \"code\" : \"fr\" , \"name\" : \"French\" }, { \"analyses\" : [ \"disambiguation\" , \"relevants\" , \"entities\" , \"relations\" ], \"code\" : \"de\" , \"name\" : \"German\" }, { \"analyses\" : [ \"disambiguation\" , \"relevants\" , \"entities\" , \"relations\" ], \"code\" : \"it\" , \"name\" : \"Italian\" } ], \"name\" : \"standard\" } ] } The contexts array contains one object for each context. The name property is the exact name that must be used in the endpoint of analysis resources. The languages array documents the context's capabilities in terms of the analyses available for each of the supported languages. code id the ISO 639-1 language code , analyses is an array listing available analyses for the language. The value of the analyses are the exact names that must be used in the endpoint of partial analysis resources.","title":"contexts"},{"location":"reference/output/self-documentation/#taxonomies","text":"The taxonomies resource returns a JSON object like this: { \"taxonomies\" : [ { \"description\" : \"The iptc document classification resources classify texts based on the IPTC Media Topics taxonomy\" , \"languages\" : [ { \"code\" : \"en\" , \"name\" : \"English\" }, { \"code\" : \"es\" , \"name\" : \"Spanish\" }, { \"code\" : \"fr\" , \"name\" : \"French\" }, { \"code\" : \"de\" , \"name\" : \"German\" }, { \"code\" : \"it\" , \"name\" : \"Italian\" } ], \"name\" : \"iptc\" }, { \"contract\" : \"https://github.com/therealexpertai/nlapi-openapi-specification/blob/master/geotax-w-geojson.yaml\" , \"description\" : \"The geotax document classification resources recognize geographic places cited in the text and return corresponding countries' names. In addition, when requested with a specific query-string parameter, they return extra-data containing equivalent GeoJSON objects. Refer to the specific OpenAPI document (https://github.com/therealexpertai/nlapi-openapi-specification/blob/master/geotax-w-geojson.yaml) for this special use of the API resources.\" , \"languages\" : [ { \"code\" : \"en\" , \"name\" : \"English\" }, { \"code\" : \"es\" , \"name\" : \"Spanish\" }, { \"code\" : \"fr\" , \"name\" : \"French\" }, { \"code\" : \"de\" , \"name\" : \"German\" }, { \"code\" : \"it\" , \"name\" : \"Italian\" } ], \"name\" : \"geotax\" }, { \"description\" : \"The emotional-traits document classification resources classify documents in terms of feelings like joy, surprise, irritation, etc. expressed in the text. In addition, when requested with a specific query-string parameter, they return extra-data containing the main groups to which the emotional traits belong. Especially with longer texts, main groups are an useful abstract of the detailed classification. Refer to the specific OpenAPI document (https://github.com/therealexpertai/nlapi-openapi-specification/blob/master/emotional-traits-w-main-groups.yaml) for this special use of the API resources.\" , \"languages\" : [ { \"code\" : \"en\" , \"name\" : \"English\" } ], \"name\" : \"emotional-traits\" }, { \"description\" : \"The behavioral-traits document classification resources classify document in terms of personality traits like curiosity, honesty, negativity, etc. the text deals with.\" , \"languages\" : [ { \"code\" : \"en\" , \"name\" : \"English\" } ], \"name\" : \"behavioral-traits\" } ] } The taxonomies array contains one object for each taxonomy. The name property is the exact name that must be used in the endpoint of classification resources and also in the endpoint of taxonomy child resources. The contract property, which is optional, contains the URL of the OpenAPI document describing a special use of the taxonomy. For example, in the case of the geotax taxonomy, the OpenAPI document defines the way to use the classification resource to obtain a GeoJSON output . The languages array lists the languages for which the taxonomy is available. code id the ISO 639-1 language code .","title":"taxonomies"},{"location":"reference/output/self-documentation/#taxonomies-child-resources","text":"taxonomies/{taxonomy}/{language} resources return the category tree \u200bfor a given taxonomy in a given language. Their output is a JSON object like this: { \"success\" : true , \"data\" : [ { \"namespace\" : \"cat-geo_en_1.0\" , \"taxonomy\" : [ { \"id\" : \"GEO_TAX\" , \"label\" : \"Geo Taxonomy\" , \"categories\" : [ { \"id\" : \"001.\" , \"label\" : \"Afghanistan\" }, { \"id\" : \"002.\" , \"label\" : \"Albania\" }, { \"id\" : \"003.\" , \"label\" : \"Algeria\" }, { \"id\" : \"004.\" , \"label\" : \"Andorra\" }, { \"id\" : \"005.\" , \"label\" : \"Angola\" }, { \"id\" : \"006.\" , \"label\" : \"Antigua and Barbuda\" }, { \"id\" : \"007.\" , \"label\" : \"Argentina\" }, { \"id\" : \"008.\" , \"label\" : \"Armenia\" }, { \"id\" : \"009.\" , \"label\" : \"Australia\" }, { \"id\" : \"010.\" , \"label\" : \"Austria\" }, { \"id\" : \"011.\" , \"label\" : \"Azerbaijan\" }, { \"id\" : \"012.\" , \"label\" : \"Bahamas\" }, { \"id\" : \"013.\" , \"label\" : \"Bahrain\" }, { \"id\" : \"014.\" , \"label\" : \"Bangladesh\" }, { \"id\" : \"015.\" , \"label\" : \"Barbados\" }, { \"id\" : \"016.\" , \"label\" : \"Belarus\" }, { \"id\" : \"017.\" , \"label\" : \"Belgium\" }, { \"id\" : \"018.\" , \"label\" : \"Belize\" }, { \"id\" : \"019.\" , \"label\" : \"Benin\" }, { \"id\" : \"020.\" , \"label\" : \"Bhutan\" }, { \"id\" : \"021.\" , \"label\" : \"Bolivia\" }, { \"id\" : \"022.\" , \"label\" : \"Bosnia-Herzegovina\" }, { \"id\" : \"023.\" , \"label\" : \"Botswana\" }, { \"id\" : \"024.\" , \"label\" : \"Brazil\" }, { \"id\" : \"025.\" , \"label\" : \"Brunei\" }, { \"id\" : \"026.\" , \"label\" : \"Bulgaria\" }, { \"id\" : \"027.\" , \"label\" : \"Burkina Faso\" }, { \"id\" : \"028.\" , \"label\" : \"Burundi\" }, { \"id\" : \"029.\" , \"label\" : \"Cambodia\" }, { \"id\" : \"030.\" , \"label\" : \"Cameroon\" }, { \"id\" : \"031.\" , \"label\" : \"Canada\" }, { \"id\" : \"032.\" , \"label\" : \"Cape Verde\" }, { \"id\" : \"033.\" , \"label\" : \"Central African Republic\" }, { \"id\" : \"034.\" , \"label\" : \"Chad\" }, { \"id\" : \"035.\" , \"label\" : \"Chile\" }, { \"id\" : \"036.\" , \"label\" : \"China\" }, { \"id\" : \"037.\" , \"label\" : \"Colombia\" }, { \"id\" : \"038.\" , \"label\" : \"Comoros\" }, { \"id\" : \"039.\" , \"label\" : \"Costa Rica\" }, { \"id\" : \"040.\" , \"label\" : \"C\u00f4te d'Ivoire\" }, { \"id\" : \"041.\" , \"label\" : \"Croatia\" }, { \"id\" : \"042.\" , \"label\" : \"Cuba\" }, { \"id\" : \"043.\" , \"label\" : \"Cyprus\" }, { \"id\" : \"044.\" , \"label\" : \"Czech Republic\" }, { \"id\" : \"045.\" , \"label\" : \"Democratic Republic of Congo\" }, { \"id\" : \"046.\" , \"label\" : \"Denmark\" }, { \"id\" : \"047.\" , \"label\" : \"Djibouti\" }, { \"id\" : \"048.\" , \"label\" : \"Dominica\" }, { \"id\" : \"049.\" , \"label\" : \"Dominican Republic\" }, { \"id\" : \"050.\" , \"label\" : \"East Timor\" }, { \"id\" : \"051.\" , \"label\" : \"Ecuador\" }, { \"id\" : \"052.\" , \"label\" : \"Egypt\" }, { \"id\" : \"053.\" , \"label\" : \"El Salvador\" }, { \"id\" : \"054.\" , \"label\" : \"Equatorial Guinea\" }, { \"id\" : \"055.\" , \"label\" : \"Eritrea\" }, { \"id\" : \"056.\" , \"label\" : \"Estonia\" }, { \"id\" : \"057.\" , \"label\" : \"Ethiopia\" }, { \"id\" : \"058.\" , \"label\" : \"Fiji\" }, { \"id\" : \"059.\" , \"label\" : \"Finland\" }, { \"id\" : \"060.\" , \"label\" : \"France\" }, { \"id\" : \"061.\" , \"label\" : \"Gabon\" }, { \"id\" : \"062.\" , \"label\" : \"Gambia\" }, { \"id\" : \"063.\" , \"label\" : \"Georgia\" }, { \"id\" : \"064.\" , \"label\" : \"Germany\" }, { \"id\" : \"065.\" , \"label\" : \"Ghana\" }, { \"id\" : \"066.\" , \"label\" : \"Greece\" }, { \"id\" : \"067.\" , \"label\" : \"Grenada\" }, { \"id\" : \"068.\" , \"label\" : \"Guatemala\" }, { \"id\" : \"069.\" , \"label\" : \"Guinea\" }, { \"id\" : \"070.\" , \"label\" : \"Guinea-Bissau\" }, { \"id\" : \"071.\" , \"label\" : \"Guyana\" }, { \"id\" : \"072.\" , \"label\" : \"Haiti\" }, { \"id\" : \"073.\" , \"label\" : \"Honduras\" }, { \"id\" : \"074.\" , \"label\" : \"Hungary\" }, { \"id\" : \"075.\" , \"label\" : \"Iceland\" }, { \"id\" : \"076.\" , \"label\" : \"India\" }, { \"id\" : \"077.\" , \"label\" : \"Indonesia\" }, { \"id\" : \"078.\" , \"label\" : \"Iran\" }, { \"id\" : \"079.\" , \"label\" : \"Iraq\" }, { \"id\" : \"080.\" , \"label\" : \"Ireland\" }, { \"id\" : \"081.\" , \"label\" : \"Israel\" }, { \"id\" : \"082.\" , \"label\" : \"Italy\" }, { \"id\" : \"083.\" , \"label\" : \"Jamaica\" }, { \"id\" : \"084.\" , \"label\" : \"Japan\" }, { \"id\" : \"085.\" , \"label\" : \"Jordan\" }, { \"id\" : \"086.\" , \"label\" : \"Kazakhstan\" }, { \"id\" : \"087.\" , \"label\" : \"Kenya\" }, { \"id\" : \"088.\" , \"label\" : \"Kiribati\" }, { \"id\" : \"089.\" , \"label\" : \"Kosovo\" }, { \"id\" : \"090.\" , \"label\" : \"Kuwait\" }, { \"id\" : \"091.\" , \"label\" : \"Kyrgyzstan\" }, { \"id\" : \"092.\" , \"label\" : \"Laos\" }, { \"id\" : \"093.\" , \"label\" : \"Latvia\" }, { \"id\" : \"094.\" , \"label\" : \"Lebanon\" }, { \"id\" : \"095.\" , \"label\" : \"Lesotho\" }, { \"id\" : \"096.\" , \"label\" : \"Liberia\" }, { \"id\" : \"097.\" , \"label\" : \"Libya\" }, { \"id\" : \"098.\" , \"label\" : \"Liechtenstein\" }, { \"id\" : \"099.\" , \"label\" : \"Lithuania\" }, { \"id\" : \"100.\" , \"label\" : \"Luxemburg\" }, { \"id\" : \"101.\" , \"label\" : \"Macedonia\" }, { \"id\" : \"102.\" , \"label\" : \"Madagascar\" }, { \"id\" : \"103.\" , \"label\" : \"Malawi\" }, { \"id\" : \"104.\" , \"label\" : \"Malaysia\" }, { \"id\" : \"105.\" , \"label\" : \"Maldives\" }, { \"id\" : \"106.\" , \"label\" : \"Mali\" }, { \"id\" : \"107.\" , \"label\" : \"Malta\" }, { \"id\" : \"108.\" , \"label\" : \"Marshall Islands\" }, { \"id\" : \"109.\" , \"label\" : \"Mauritania\" }, { \"id\" : \"110.\" , \"label\" : \"Mauritius\" }, { \"id\" : \"111.\" , \"label\" : \"Mexico\" }, { \"id\" : \"112.\" , \"label\" : \"Micronesia\" }, { \"id\" : \"113.\" , \"label\" : \"Moldova\" }, { \"id\" : \"114.\" , \"label\" : \"Monaco\" }, { \"id\" : \"115.\" , \"label\" : \"Mongolia\" }, { \"id\" : \"116.\" , \"label\" : \"Montenegro\" }, { \"id\" : \"117.\" , \"label\" : \"Morocco\" }, { \"id\" : \"118.\" , \"label\" : \"Mozambique\" }, { \"id\" : \"119.\" , \"label\" : \"Myanmar\" }, { \"id\" : \"120.\" , \"label\" : \"Namibia\" }, { \"id\" : \"121.\" , \"label\" : \"Nauru\" }, { \"id\" : \"122.\" , \"label\" : \"Nepal\" }, { \"id\" : \"123.\" , \"label\" : \"Netherlands\" }, { \"id\" : \"124.\" , \"label\" : \"New Zealand\" }, { \"id\" : \"125.\" , \"label\" : \"Nicaragua\" }, { \"id\" : \"126.\" , \"label\" : \"Niger\" }, { \"id\" : \"127.\" , \"label\" : \"Nigeria\" }, { \"id\" : \"128.\" , \"label\" : \"North Korea\" }, { \"id\" : \"129.\" , \"label\" : \"Norway\" }, { \"id\" : \"130.\" , \"label\" : \"Oman\" }, { \"id\" : \"131.\" , \"label\" : \"Pakistan\" }, { \"id\" : \"132.\" , \"label\" : \"Palau\" }, { \"id\" : \"133.\" , \"label\" : \"Panama\" }, { \"id\" : \"134.\" , \"label\" : \"Papua New Guinea\" }, { \"id\" : \"135.\" , \"label\" : \"Paraguay\" }, { \"id\" : \"136.\" , \"label\" : \"Peru\" }, { \"id\" : \"137.\" , \"label\" : \"Philippines\" }, { \"id\" : \"138.\" , \"label\" : \"Poland\" }, { \"id\" : \"139.\" , \"label\" : \"Portugal\" }, { \"id\" : \"140.\" , \"label\" : \"Qatar\" }, { \"id\" : \"141.\" , \"label\" : \"Republic of Congo\" }, { \"id\" : \"142.\" , \"label\" : \"Romania\" }, { \"id\" : \"143.\" , \"label\" : \"Russia\" }, { \"id\" : \"144.\" , \"label\" : \"Rwanda\" }, { \"id\" : \"145.\" , \"label\" : \"Saint Kitts and Nevis\" }, { \"id\" : \"146.\" , \"label\" : \"Saint Lucia\" }, { \"id\" : \"147.\" , \"label\" : \"Saint Vincent and the Grenadines\" }, { \"id\" : \"148.\" , \"label\" : \"Samoa\" }, { \"id\" : \"149.\" , \"label\" : \"San Marino\" }, { \"id\" : \"150.\" , \"label\" : \"S\u00e3o Tom\u00e9 and Pr\u00edncipe\" }, { \"id\" : \"151.\" , \"label\" : \"Saudi Arabia\" }, { \"id\" : \"152.\" , \"label\" : \"Senegal\" }, { \"id\" : \"153.\" , \"label\" : \"Serbia\" }, { \"id\" : \"154.\" , \"label\" : \"Seychelles\" }, { \"id\" : \"155.\" , \"label\" : \"Sierra Leone\" }, { \"id\" : \"156.\" , \"label\" : \"Singapore\" }, { \"id\" : \"157.\" , \"label\" : \"Slovakia\" }, { \"id\" : \"158.\" , \"label\" : \"Slovenia\" }, { \"id\" : \"159.\" , \"label\" : \"Solomon Islands\" }, { \"id\" : \"160.\" , \"label\" : \"Somalia\" }, { \"id\" : \"161.\" , \"label\" : \"South Africa\" }, { \"id\" : \"162.\" , \"label\" : \"South Korea\" }, { \"id\" : \"163.\" , \"label\" : \"Spain\" }, { \"id\" : \"164.\" , \"label\" : \"Sri Lanka\" }, { \"id\" : \"165.\" , \"label\" : \"Sudan\" }, { \"id\" : \"166.\" , \"label\" : \"Suriname\" }, { \"id\" : \"167.\" , \"label\" : \"Swaziland\" }, { \"id\" : \"168.\" , \"label\" : \"Sweden\" }, { \"id\" : \"169.\" , \"label\" : \"Switzerland\" }, { \"id\" : \"170.\" , \"label\" : \"Syria\" }, { \"id\" : \"171.\" , \"label\" : \"Tajikistan\" }, { \"id\" : \"172.\" , \"label\" : \"Tanzania\" }, { \"id\" : \"173.\" , \"label\" : \"Thailand\" }, { \"id\" : \"174.\" , \"label\" : \"Togo\" }, { \"id\" : \"175.\" , \"label\" : \"Tonga\" }, { \"id\" : \"176.\" , \"label\" : \"Trinidad and Tobago\" }, { \"id\" : \"177.\" , \"label\" : \"Tunisia\" }, { \"id\" : \"178.\" , \"label\" : \"Turkey\" }, { \"id\" : \"179.\" , \"label\" : \"Turkmenistan\" }, { \"id\" : \"180.\" , \"label\" : \"Tuvalu\" }, { \"id\" : \"181.\" , \"label\" : \"Uganda\" }, { \"id\" : \"182.\" , \"label\" : \"Ukraine\" }, { \"id\" : \"183.\" , \"label\" : \"United Arab Emirates\" }, { \"id\" : \"184.\" , \"label\" : \"United Kingdom\" , \"categories\" : [ { \"id\" : \"18401.\" , \"label\" : \"England\" }, { \"id\" : \"18402.\" , \"label\" : \"Northern Ireland\" }, { \"id\" : \"18403.\" , \"label\" : \"Scotland\" }, { \"id\" : \"18404.\" , \"label\" : \"Wales\" } ] }, { \"id\" : \"185.\" , \"label\" : \"United States of America\" , \"categories\" : [ { \"id\" : \"18501.\" , \"label\" : \"Alabama\" }, { \"id\" : \"18502.\" , \"label\" : \"Alaska\" }, { \"id\" : \"18503.\" , \"label\" : \"Arizona\" }, { \"id\" : \"18504.\" , \"label\" : \"Arkansas\" }, { \"id\" : \"18505.\" , \"label\" : \"California\" }, { \"id\" : \"18506.\" , \"label\" : \"Colorado\" }, { \"id\" : \"18507.\" , \"label\" : \"Connecticut\" }, { \"id\" : \"18508.\" , \"label\" : \"Delaware\" }, { \"id\" : \"18509.\" , \"label\" : \"District of Columbia\" }, { \"id\" : \"18510.\" , \"label\" : \"Florida\" }, { \"id\" : \"18511.\" , \"label\" : \"Georgia\" }, { \"id\" : \"18512.\" , \"label\" : \"Hawaii\" }, { \"id\" : \"18513.\" , \"label\" : \"Idaho\" }, { \"id\" : \"18514.\" , \"label\" : \"Illinois\" }, { \"id\" : \"18515.\" , \"label\" : \"Indiana\" }, { \"id\" : \"18516.\" , \"label\" : \"Iowa\" }, { \"id\" : \"18517.\" , \"label\" : \"Kansas\" }, { \"id\" : \"18518.\" , \"label\" : \"Kentucky\" }, { \"id\" : \"18519.\" , \"label\" : \"Louisiana\" }, { \"id\" : \"18520.\" , \"label\" : \"Maine\" }, { \"id\" : \"18521.\" , \"label\" : \"Maryland\" }, { \"id\" : \"18522.\" , \"label\" : \"Massachusetts\" }, { \"id\" : \"18523.\" , \"label\" : \"Michigan\" }, { \"id\" : \"18524.\" , \"label\" : \"Minnesota\" }, { \"id\" : \"18525.\" , \"label\" : \"Mississippi\" }, { \"id\" : \"18526.\" , \"label\" : \"Missouri\" }, { \"id\" : \"18527.\" , \"label\" : \"Montana\" }, { \"id\" : \"18528.\" , \"label\" : \"Nebraska\" }, { \"id\" : \"18529.\" , \"label\" : \"Nevada\" }, { \"id\" : \"18530.\" , \"label\" : \"New Hampshire\" }, { \"id\" : \"18531.\" , \"label\" : \"New Jersey\" }, { \"id\" : \"18532.\" , \"label\" : \"New Mexico\" }, { \"id\" : \"18533.\" , \"label\" : \"New York State\" }, { \"id\" : \"18534.\" , \"label\" : \"North Carolina\" }, { \"id\" : \"18535.\" , \"label\" : \"North Dakota\" }, { \"id\" : \"18536.\" , \"label\" : \"Ohio\" }, { \"id\" : \"18537.\" , \"label\" : \"Oklahoma\" }, { \"id\" : \"18538.\" , \"label\" : \"Oregon\" }, { \"id\" : \"18539.\" , \"label\" : \"Pennsylvania\" }, { \"id\" : \"18540.\" , \"label\" : \"Rhode Island\" }, { \"id\" : \"18541.\" , \"label\" : \"South Carolina\" }, { \"id\" : \"18542.\" , \"label\" : \"South Dakota\" }, { \"id\" : \"18543.\" , \"label\" : \"Tennessee\" }, { \"id\" : \"18544.\" , \"label\" : \"Texas\" }, { \"id\" : \"18545.\" , \"label\" : \"Utah\" }, { \"id\" : \"18546.\" , \"label\" : \"Vermont\" }, { \"id\" : \"18547.\" , \"label\" : \"Virginia\" }, { \"id\" : \"18548.\" , \"label\" : \"Washington State\" }, { \"id\" : \"18549.\" , \"label\" : \"West Virginia\" }, { \"id\" : \"18550.\" , \"label\" : \"Wisconsin\" }, { \"id\" : \"18551.\" , \"label\" : \"Wyoming\" } ] }, { \"id\" : \"186.\" , \"label\" : \"Uruguay\" }, { \"id\" : \"187.\" , \"label\" : \"Uzbekistan\" }, { \"id\" : \"188.\" , \"label\" : \"Vanuatu\" }, { \"id\" : \"189.\" , \"label\" : \"Vatican City\" }, { \"id\" : \"190.\" , \"label\" : \"Venezuela\" }, { \"id\" : \"191.\" , \"label\" : \"Vietnam\" }, { \"id\" : \"192.\" , \"label\" : \"Yemen\" }, { \"id\" : \"193.\" , \"label\" : \"Zambia\" }, { \"id\" : \"194.\" , \"label\" : \"Zimbabwe\" } ] } ] } ] } The Boolean property success indicates that processing was successful while the data array contains the results. data contains one object for each categories' tree because, in theory, a taxonomy can have more than one. namespace is the name of the software module containing the taxonomy, while taxonomy is an array of possibly nested categories. Each category is an object with these properties: id : category identifier label : category label categories : sub-categories There can be several categories with the same value for id in the categories' tree, but there cannot be two or more categories with the same id having the same parent category.","title":"taxonomies child resources"},{"location":"reference/output/self-documentation/#detectors","text":"The detectors resource returns a JSON object like this: { \"detectors\" : [ { \"contract\" : \"https://github.com/therealexpertai/nlapi-openapi-specification/blob/master/pii.yaml\" , \"description\" : \"Personally Identifiable Information (PII) detector\" , \"languages\" : [ { \"code\" : \"en\" , \"name\" : \"English\" }, { \"code\" : \"it\" , \"name\" : \"Italian\" } ], \"name\" : \"pii\" }, { \"contract\" : \"https://github.com/therealexpertai/nlapi-openapi-specification/blob/master/writeprint.yaml\" , \"description\" : \"Writeprint detector\" , \"languages\" : [ { \"code\" : \"en\" , \"name\" : \"English\" }, { \"code\" : \"it\" , \"name\" : \"Italian\" }, { \"code\" : \"es\" , \"name\" : \"Spanish\" }, { \"code\" : \"fr\" , \"name\" : \"French\" }, { \"code\" : \"de\" , \"name\" : \"German\" } ], \"name\" : \"writeprint\" } ] } The detectors array contains one object for each detector. The name property is the exact name that must be used in the endpoint of information detection resources. The languages array lists the languages for which the taxonomy is available. code id the ISO 639-1 language code . Each detector has a specific output which is formally described in an OpenAPI document. The contract property contains the URL of that document.","title":"detectors"},{"location":"reference/output/sentiment-analysis/","text":"Sentiment analysis output The API resource performing sentiment analysis returns a JSON object with this format: { \"success\": Boolean success flag , \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , \"knowledge\": [], \"sentiment\": {} } } For the description of the contents , language and version properties, see the API resources output overview . The knowledge array contains Knowledge Graph data about items of the sentiment array, as a result of the semantic analysis process. Its contents are described in the article about the output of full analysis . sentiment The sentiment object has three properties indicative of the tone of the whole text: positivity : positivity score negativity : negativity score overall : the overall sentiment score, which is a combination of the scores above All scores are expressed in a -100 (extremely negative) to 100 (extremely positive) range. The sentiment object also contains an items array whose elements, in turn, can contain nested items arrays. These items represent the clusters of text elements that give a positive or negative contribution to sentiment. For example, given this input text: The road was bad. items clusters can be like this: \"items\" : [ { \"lemma\" : \"road\" , \"sentiment\" : -7 , \"syncon\" : 19001 , \"items\" : [ { \"lemma\" : \"bad\" , \"sentiment\" : -7 , \"syncon\" : 81195 } ] } ] sentiment is the sentiment score of the cluster or leaf-item. The sentiment score of a cluster is a function of the child items' scores and the the possible modifiers, which are not returned as separate item, but are nevertheless taken into account. Take, for example, a slight change introduced in the sample text: The road was really bad. the really modifier makes the score get worse: \"items\" : [ { \"lemma\" : \"road\" , \"sentiment\" : -8.8 , \"syncon\" : 19001 , \"items\" : [ { \"lemma\" : \"bad\" , \"sentiment\" : -8.8 , \"syncon\" : 81195 } ] } ] On the other hand, a not before bad can invert the sentiment polarity from negative to positive. The sentiment value can be zero. The syncon and lemma properties are the outcome of semantic analysis and lemmatization respectively. These are the same processes carried out during deep linguistic analysis , but the value of lemma can have some peculiarities. An item having nested items can be an \"unnamed cluster\": in that case the lemma property is an empty string. If the intrinsic item polarity\u2014positive or negative\u2014is opposite of that of the paragraph it belongs to, this marker: [*] is added as a suffix to the the lemma. For example, given this input text: The road was not bad. The lemma for bad is marked with the \"opposite polarity\" sign because it is negated by not : \"items\" : [ { \"items\" : [ { \"lemma\" : \"bad[*]\" , \"sentiment\" : 7 , \"syncon\" : 87597 } ], \"lemma\" : \"road\" , \"sentiment\" : 7 , \"syncon\" : 19001 } ] Another possibility is that a lemma \"attracts\" other words in the same phrase. For example, given the input text: Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half. a value of lemma could be: stand-out;skill In this case the merged terms are separated by a semi-colon ( ; ). Value -1 for syncon means the concept doesn't have a correspondent in the expert.ai Knowledge Graph . knowledge The knowledge array contains Knowledge Graph data for the elements of the items array. Its contents are described in the article about the output of full analysis .","title":"Sentiment analysis"},{"location":"reference/output/sentiment-analysis/#sentiment-analysis-output","text":"The API resource performing sentiment analysis returns a JSON object with this format: { \"success\": Boolean success flag , \"data\": { \"content\": analyzed text , \"language\": language code , \"version\": technology version info , \"knowledge\": [], \"sentiment\": {} } } For the description of the contents , language and version properties, see the API resources output overview . The knowledge array contains Knowledge Graph data about items of the sentiment array, as a result of the semantic analysis process. Its contents are described in the article about the output of full analysis .","title":"Sentiment analysis output"},{"location":"reference/output/sentiment-analysis/#sentiment","text":"The sentiment object has three properties indicative of the tone of the whole text: positivity : positivity score negativity : negativity score overall : the overall sentiment score, which is a combination of the scores above All scores are expressed in a -100 (extremely negative) to 100 (extremely positive) range. The sentiment object also contains an items array whose elements, in turn, can contain nested items arrays. These items represent the clusters of text elements that give a positive or negative contribution to sentiment. For example, given this input text: The road was bad. items clusters can be like this: \"items\" : [ { \"lemma\" : \"road\" , \"sentiment\" : -7 , \"syncon\" : 19001 , \"items\" : [ { \"lemma\" : \"bad\" , \"sentiment\" : -7 , \"syncon\" : 81195 } ] } ] sentiment is the sentiment score of the cluster or leaf-item. The sentiment score of a cluster is a function of the child items' scores and the the possible modifiers, which are not returned as separate item, but are nevertheless taken into account. Take, for example, a slight change introduced in the sample text: The road was really bad. the really modifier makes the score get worse: \"items\" : [ { \"lemma\" : \"road\" , \"sentiment\" : -8.8 , \"syncon\" : 19001 , \"items\" : [ { \"lemma\" : \"bad\" , \"sentiment\" : -8.8 , \"syncon\" : 81195 } ] } ] On the other hand, a not before bad can invert the sentiment polarity from negative to positive. The sentiment value can be zero. The syncon and lemma properties are the outcome of semantic analysis and lemmatization respectively. These are the same processes carried out during deep linguistic analysis , but the value of lemma can have some peculiarities. An item having nested items can be an \"unnamed cluster\": in that case the lemma property is an empty string. If the intrinsic item polarity\u2014positive or negative\u2014is opposite of that of the paragraph it belongs to, this marker: [*] is added as a suffix to the the lemma. For example, given this input text: The road was not bad. The lemma for bad is marked with the \"opposite polarity\" sign because it is negated by not : \"items\" : [ { \"items\" : [ { \"lemma\" : \"bad[*]\" , \"sentiment\" : 7 , \"syncon\" : 87597 } ], \"lemma\" : \"road\" , \"sentiment\" : 7 , \"syncon\" : 19001 } ] Another possibility is that a lemma \"attracts\" other words in the same phrase. For example, given the input text: Michael Jordan was one of the best basketball players of all time. Scoring was Jordan's stand-out skill, but he still holds a defensive NBA record, with eight steals in a half. a value of lemma could be: stand-out;skill In this case the merged terms are separated by a semi-colon ( ; ). Value -1 for syncon means the concept doesn't have a correspondent in the expert.ai Knowledge Graph .","title":"sentiment"},{"location":"reference/output/sentiment-analysis/#knowledge","text":"The knowledge array contains Knowledge Graph data for the elements of the items array. Its contents are described in the article about the output of full analysis .","title":"knowledge"},{"location":"reference/phrase-types/","text":"Phrase types These are the types of phrases recognized during the subdivision process of deep linguistic analysis: Code Description AP Adjective Phrase CP Conjunction Phrase CR Blank lines DP Adverb Phrase NA Not Applicable (usually indicates punctuation) NP Noun Phrase PN Nominal Predicate PP Preposition Phrase RP Relative Phrase VP Verb Phrase","title":"Phrase types"},{"location":"reference/phrase-types/#phrase-types","text":"These are the types of phrases recognized during the subdivision process of deep linguistic analysis: Code Description AP Adjective Phrase CP Conjunction Phrase CR Blank lines DP Adverb Phrase NA Not Applicable (usually indicates punctuation) NP Noun Phrase PN Nominal Predicate PP Preposition Phrase RP Relative Phrase VP Verb Phrase","title":"Phrase types"},{"location":"reference/positions/","text":"Positions Analysis and classification resources return positions. For example: The deep linguistic analysis resource returns the positions of all the subdivisions of the text (paragraphs, sentences, phrases, tokens and atoms). The keyphrase extraction resource returns the positions of main sentences, main phrases, main concepts and main lemmas. The classification resources return the positions of the parts of the text help identify a certain category. All these positions refer to the analyzed text, that is the content property of the data object. The starting position is returned in the start property and the ending position in the end property. The value of the start property is the zero-based index of the first character of the block. For example, if a text begins with: Michael Jordan was one of the best basketball players of all time. the start position of phrase of all time is 54: Michael Jordan was one of the best basketball players of all time . \u2191 012345678901234567890123456789012345678901234567890123 4 5678901234567890 0 1 2 3 4 5 6 7 The value of the end position is the zero-based index of the first character after the text block. In the example case above, the end position of the phrase is 65: Michael Jordan was one of the best basketball players of all time . \u2191 01234567890123456789012345678901234567890123456789012345678901234 5 67890 0 1 2 3 4 5 6 7","title":"Positions"},{"location":"reference/positions/#positions","text":"Analysis and classification resources return positions. For example: The deep linguistic analysis resource returns the positions of all the subdivisions of the text (paragraphs, sentences, phrases, tokens and atoms). The keyphrase extraction resource returns the positions of main sentences, main phrases, main concepts and main lemmas. The classification resources return the positions of the parts of the text help identify a certain category. All these positions refer to the analyzed text, that is the content property of the data object. The starting position is returned in the start property and the ending position in the end property. The value of the start property is the zero-based index of the first character of the block. For example, if a text begins with: Michael Jordan was one of the best basketball players of all time. the start position of phrase of all time is 54: Michael Jordan was one of the best basketball players of all time . \u2191 012345678901234567890123456789012345678901234567890123 4 5678901234567890 0 1 2 3 4 5 6 7 The value of the end position is the zero-based index of the first character after the text block. In the example case above, the end position of the phrase is 65: Michael Jordan was one of the best basketball players of all time . \u2191 01234567890123456789012345678901234567890123456789012345678901234 5 67890 0 1 2 3 4 5 6 7","title":"Positions"},{"location":"reference/request-format/","text":"Request format The document analysis, document classification and information detection capabilities of the expert.ai Natural Language API are provided by REST resources. These resources must be requested with the HTTP POST method. The applicable REST metaphor is that if you post your text to the resource, the response will contain the analysis, the classification or the detected information, based on the type of resource that you request. The text to process must be included in a JSON object with the following format: { \"document\" : { \"text\" : \"Your text here.\" } } The text must be UTF-8 encoded and the Content-Type header of the request must be set to: application/json; charset=utf-8","title":"Request format"},{"location":"reference/request-format/#request-format","text":"The document analysis, document classification and information detection capabilities of the expert.ai Natural Language API are provided by REST resources. These resources must be requested with the HTTP POST method. The applicable REST metaphor is that if you post your text to the resource, the response will contain the analysis, the classification or the detected information, based on the type of resource that you request. The text to process must be included in a JSON object with the following format: { \"document\" : { \"text\" : \"Your text here.\" } } The text must be UTF-8 encoded and the Content-Type header of the request must be set to: application/json; charset=utf-8","title":"Request format"},{"location":"reference/topics/","text":"Standard context topics Here are the topics that keyphrase extraction can detect when using the standard context . The first column shows the identification number of the topic, which is the same for all languages. The other columns show the topic labels, which vary by language. ID English Spanish French German Italian 0 clothing vestuario v\u00eatements Kleidung abbigliamento 1 homeopathy homeopat\u00eda hom\u00e9opathie Hom\u00f6opathie omeopatia 2 clothing accessories accesorios accessoire Kleidungszubeh\u00f6r accessori 3 acoustics ac\u00fastica acoustique Akustik acustica 4 aviation aeron\u00e1utica a\u00e9ronautique Aeronautik aeronautica 5 air force aeron\u00e1utica militar a\u00e9ronautique militaire Luftwaffe aeronautica militare 6 aeronautic technology aerotecnia a\u00e9rotechnique Luftzeugtechnik aerotecnica 7 agriculture agricultura agriculture Landwirtschaft agricoltura 8 food alimentos nourriture Lebensmittel alimenti 9 animal husbandry crianza y ganader\u00eda \u00e9levage Zucht allevamento 10 mountaineering alpinismo alpinisme Bergsteigen alpinismo 11 anatomy anatom\u00eda anatomie Anatomie anatomia 12 antique trade comercio de antig\u00fcedades antiquit\u00e9 Antiquit\u00e4ten antiquariato 13 anthropology antropolog\u00eda anthropologie Anthropologie antropologia 14 free diving apnea apn\u00e9e Atemstillstand apnea 15 heraldry her\u00e1ldica h\u00e9raldique Heraldik araldica 16 archaeology arqueolog\u00eda arch\u00e9ologie Arch\u00e4ologie archeologia 17 architecture arquitectura architecture Architektur architettura 18 archival studies archiv\u00edstica archivage Archivierung archivistica 19 archery arco arc Bogenschie\u00dfen arco 20 arithmetic aritm\u00e9tica arithm\u00e9tique Arithmetik aritmetica 21 weapons armas armes Bewaffnung armi 22 interior design decoraci\u00f3n ameublement Innenausstattung arredamento 23 art arte art Kunst arte 24 martial arts artes marciales arts martiaux Kampfsport arti marziali 25 crafts artesan\u00eda artisanat Handwerk artigianato 26 artistic crafts artesan\u00eda art\u00edstica artisanat d'art Kunsthandwerk artigianato artistico 27 visual arts artes gr\u00e1ficas graphique graphische Kunst arti grafiche 28 artillery artiller\u00eda artillerie Artillerie artiglieria 29 insurance industry seguros assurance Versicherung assicurazioni 30 auction subastas vente Auktion aste 31 astrophysics astrof\u00edsica astrophysique Astrophysik astrofisica 32 astrology astrolog\u00eda astrologie Astrologie astrologia 33 astronautics astron\u00e1utica astronautique Raumfahrt astronautica 34 astronomy astronom\u00eda astronomie Astronomie astronomia 35 culture t\u00e9rminos culturales termes culturels kulturelle Begriffe termini culturali 36 athletics atletismo athl\u00e9tisme Leichtathletik atletica leggera 37 wrestling and weightlifting halterofilia y lucha halt\u00e9rophilie Schwerathletik atletica pesante 38 automation automatizaci\u00f3n automation Automatisierung automazione 39 motor vehicles veh\u00edculos de motor v\u00e9hicule automobile Kraftfahrzeug autoveicoli 40 motor racing automovilismo automobilisme Motorsport automobilismo 41 air travel aviaci\u00f3n aviation Luftfahrt aviazione 42 ballistics bal\u00edstica balistique Ballistik balistica 43 ballet ballet-baile ballet Ballett balletto-danza 44 banking banco banque Bank banca 45 baseball b\u00e9isbol base-ball Baseball baseball 46 The Bible Biblia Bible Bibel Bibbia 47 libraries bibliotecas biblioth\u00e8que Bibliotheken biblioteche 48 billiard sports billar billard Billard biliardo 49 biochemistry bioqu\u00edmica biochimie Biochemie biochimica 50 biophysics biof\u00edsica biophysique Biophysik biofisica 51 biology biolog\u00eda biologie Biologie biologia 52 biotechnology biotecnolog\u00eda biotechnologie Biotechnologie biotecnologie 53 bobsledding bobsleigh bob Bob bob 54 stock exchange bolsa Bourse B\u00f6rse borsa 55 botany bot\u00e1nica botanique Botanik botanica 56 bowling bolos bowling Bowling bowling 57 boxing boxeo boxe Boxen boxe 58 do it yourself bricolaje bricolage do it yourself bricolage 59 bridge (card game) bridge bridge Bridge bridge 60 Buddhism budismo bouddhisme Buddhismus buddismo 61 hunting caza chasse Jagd caccia 62 CAD (computer aided design) cad cad CAD (rechnerunterst\u00fctzte Konstruktion) cad 63 five-a-side football f\u00fatbol sala football \u00e0 cinq Futsal calcetto 64 soccer (US) f\u00fatbol football Fu\u00dfball calcio 65 footwear calzados chaussure Schuhwerk calzature 66 rowing pirag\u00fcismo aviron Rudern canottaggio 67 singing canto chant Gesang canto 68 coal mining carb\u00f3n charbonnage Kohlebergwerk carbone 69 cardiology cardiolog\u00eda cardiologie Kardiologie cardiologia 70 cartography cartograf\u00eda cartographie Kartographie cartografia 71 pottery cer\u00e1mica c\u00e9ramique T\u00f6pferei ceramica 72 chemistry qu\u00edmica chimie Chemie chimica 73 inorganic chemistry qu\u00edmica inorg\u00e1nica chimie inorganique anorganische Chemie chimica inorganica 74 organic chemistry qu\u00edmica org\u00e1nica chimie organique organische Chemie chimica organica 75 surgery cirug\u00eda chirurgie Chirurgie chirurgia 76 cycling ciclismo cyclisme Radfahren ciclismo 77 film industry cine cin\u00e9ma Kino cinema 78 circus circo cirque Zirkus circo 79 track cycling ciclismo en pista cyclisme sur piste Bahnrennen ciclismo su pista 80 cytology citolog\u00eda cytologie Zytologie citologia 81 collecting coleccionismo collection Sammeln collezionismo 82 trade comercio commerce Handel commercio 83 electronic components piezas electr\u00f3nicas composants \u00e9lectroniques elektronischen Komponenten componenti elettronici 84 music composition composici\u00f3n composition Komposition composizione 85 computer art infoarte computer art Computerkunst computer art 86 computer graphics infograf\u00eda infographie Grafik computer grafica 87 media and communication comunicaci\u00f3n communication Massenkommunikation comunicazione 88 leather processing curtidur\u00eda tannerie Gerberei conceria 89 beauty products cosm\u00e9tica cosm\u00e9tologie Kosmetika cosmesi 90 cosmography cosmograf\u00eda cosmographie Kosmographie cosmografia 91 cricket cr\u00edquet cricket Kricket cricket 92 crime criminalidad criminalit\u00e9 Kriminalit\u00e4t criminalit\u00e0 93 Christianity cristianismo Christianisme Christentum cristianesimo 94 news cr\u00f3nica chronique Nachrichten cronaca 95 database base de datos base de donn\u00e9es Datenbank database 96 decoupage decoupage d\u00e9coupage Serviettentechnik decoupage 97 dermatology dermatolog\u00eda dermatologie Dermatologie dermatologia 98 teaching methodology did\u00e1ctica didactique Didaktik didattica 99 dietetics diet\u00e9tica di\u00e9t\u00e9tique Di\u00e4tetik dietetica 100 diplomacy diplomacia diplomatie Diplomatie diplomazia 101 law derecho droit Gesetz diritto 102 non-criminal law derecho civil droit civil Zivilrecht diritto civile 103 business and commercial law derecho mercantil droit commercial Handelsrecht diritto commerciale 104 international law derecho internacional droit international V\u00f6lkerrecht diritto internazionale 105 criminal law derecho penal droit p\u00e9nal Strafrecht diritto penale 106 private law derecho privado droit priv\u00e9 Privatrecht diritto privato 107 administrative law derecho p\u00fablico y administrativo droit public et administratif Staats-und Verwaltungsrecht diritto pubblico e amministrativo 108 electronic storage discos disques Festplatten dischi 109 illustration dibujo dessin Zeichnung disegno 110 Judaism hebra\u00edsmo juda\u00efsme Judentum ebraismo 111 ecology ecolog\u00eda \u00e9cologie \u00d6kologie ecologia 112 e-commerce comercio electr\u00f3nico e-commerce E-Commerce e-commerce 113 the economy econom\u00eda \u00e9conomie Wirtschaft economia 114 construction industry construcci\u00f3n construction Bauindustrie edilizia 115 publishing industria editorial \u00e9dition Verlagswesen editoria 116 electricity electricidad \u00e9lectricit\u00e9 Elektrizit\u00e4t elettricit\u00e0 117 electronics electr\u00f3nica \u00e9lectronique Elektronik elettronica 118 electrotechnics electrot\u00e9cnica \u00e9lectrotechnique Elektrotechnik elettrotecnica 119 e-mail e-mail e-mail E-Mail e-mail 120 embryology embriolog\u00eda embryologie Embryologie embriologia 121 energy energ\u00eda \u00e9nergie Energie energia 122 puzzles enigm\u00edstica jeux d'esprit R\u00e4tsel enigmistica 123 enology enolog\u00eda \u0153nologie \u00d6nologie enologia 124 entomology entomolog\u00eda entomologie Insektenkunde entomologia 125 epigraphy epigraf\u00eda \u00e9pigraphie Epigraphie epigrafia 126 equitation equitaci\u00f3n \u00e9quitation Reitsport equitazione 127 ethics \u00e9tica \u00e9thique Ethik etica 128 Islam islam islam Islam islam 129 ethnology etnolog\u00eda ethnologie Ethnologie etnologia 130 TV broadcasting eventos televisivos \u00e9v\u00e9nements t\u00e9l\u00e9vis\u00e9s Rundfunk Veranstaltungen eventi televisivi 131 evolution evoluci\u00f3n \u00e9volution Evolution evoluzione 132 carpentry carpinter\u00eda menuiserie Zimmerei falegnameria 133 pharmaceuticals farmacia pharmacie Pharmazie farmacia 134 pharmacology farmacolog\u00eda pharmacologie Pharmakologie farmacologia 135 trains ferrocarril chemin de fer Eisenbahn ferrovia 136 philately filatelia philat\u00e9lie Briefmarkensammeln filatelia 137 philology filolog\u00eda philologie Philologie filologia 138 philosophy filosof\u00eda philosophie Philosophie filosofia 139 finance finanzas finance Finanz finanza 140 finance (private) finanzas privadas finances priv\u00e9es Privatfinanz finanza privata 141 public financing finanzas p\u00fablicas finances publiques \u00f6ffentliche Finanz finanza pubblica 142 revenue services fisco fisc Einnahmen fisco 143 physics f\u00edsica physique Physik fisica 144 atomic physics f\u00edsica at\u00f3mica physique atomique Atomphysik fisica atomica 145 physiology fisiolog\u00eda physiologie Physiologie fisiologia 146 phytopathology fitopatolog\u00eda phytopathologie Phytopathologie fitopatologia 147 folklore folclore folklore Volkskunde folclore 148 phonetics fon\u00e9tica phon\u00e9tique Phonetik fonetica 149 phonology fonolog\u00eda phonologie Phonologie fonologia 150 American football f\u00fatbol americano football am\u00e9ricain American Football football americano 151 photography fotograf\u00eda photographie Fotografie fotografia 152 artistic photography fotograf\u00eda art\u00edstica photographie artistique k\u00fcnstlerische Fotografie fotografia artistica 153 free climbing escalada libre free climbing Klettern free climbing 154 fruit growing fruticultura arboriculture fruiti\u00e8re Obstbau frutticoltura 155 cartoons and comic books historieta bande dessin\u00e9e Comics fumetto 156 art galleries galer\u00edas galerie Galerie gallerie 157 gastronomy gastronom\u00eda gastronomie Gastronomie gastronomia 158 genealogy genealog\u00eda g\u00e9n\u00e9alogie Genealogie genealogia 159 genetics gen\u00e9tica g\u00e9n\u00e9tique Genetik genetica 160 geophysics geof\u00edsica g\u00e9ophysique Geophysik geofisica 161 geography geograf\u00eda g\u00e9ographie Geographie geografia 162 geology geolog\u00eda g\u00e9ologie Geologie geologia 163 geometry geometr\u00eda g\u00e9om\u00e9trie Geometrie geometria 164 gardening jardiner\u00eda jardinage G\u00e4rtnerei giardinaggio 165 toys juguetes jouets Spielzeuge giocattoli 166 card games juegos de naipes jeux de cartes Kartenspielen giochi con le carte 167 games and toys juego jeu Spiel gioco 168 jewelry joyer\u00eda bijouterie Schmuck gioielleria 169 the press peri\u00f3dicos journaux Zeitungen giornali 170 journalism periodismo journalisme Journalismus giornalismo 171 go-karting kart go-kart Go-Kart go kart 172 golf golf golf Golf golf 173 graffiti grafiti graffiti Graffito graffiti 174 grammar and syntax gram\u00e1tica grammaire Grammatik grammatica 175 computer hardware hardware mat\u00e9riel Hardware hardware 176 hockey hockey hockey Hockey hockey 177 ice hockey hockey sobre hielo hockey sur glace Eishockey hockey su ghiaccio 178 hydraulics hidr\u00e1ulica hydraulique Hydraulik idraulica 179 immunology inmunolog\u00eda immunologie Immunologie immunologia 180 business empresa entreprise Firma impresa 181 engraving grabado gravure Gravierung incisione 182 industry industria industrie Industrie industria 183 aeronautics industry industria aeron\u00e1utica industrie a\u00e9ronautique Luftfahrtindustrie industria aeronautica 184 food industry industria alimentaria industrie alimentaire Lebensmittelindustrie industria alimentare 185 automotive industry industria automov\u00edlistica industrie automobile Autoindustrie industria automobilistica 186 arms industry industria b\u00e9lica industrie de guerre Kriegsindustrie industria bellica 187 footwear industry industria del calzado industrie de la chaussure Schuhindustrie industria calzaturiera 188 ceramic industry industria cer\u00e1mica industrie c\u00e9ramique Keramikindustrie industria ceramica 189 chemical industry industria qu\u00edmica industrie chimique Chemieindustrie industria chimica 190 rail transport industria del ferrocarril industrie de chemin de fer Eisenbahnindustrie industria ferroviaria 191 furniture industry industria mueblera industrie des meubles M\u00f6belindustrie industria mobiliera 192 motorcycle industry industria motociclista industrie de motos Motorrad-Industrie industria motociclistica 193 shipbuilding industria n\u00e1utica industrie naval Schifffahrtsindustrie industria nautica 194 textile industry industria textil industrie textiles Textilindustrie industria tessile 195 glass manufacturing industria del vidrio industrie de verre Glasherstellung industria vetraria 196 industrial design dise\u00f1o industrial dessin industriel Industriedesign industrial design 197 computer science inform\u00e1tica informatique Informatik informatica 198 engineering ingenier\u00eda g\u00e9nie Ingenieurwesen ingegneria 199 aerospace engineering ingenier\u00eda aeroespacial g\u00e9nie a\u00e9rospatial Luft- und Raumfahrttechnik ingegneria aerospaziale 200 Internet internet internet Internet internet 201 intranet intranet intranet Intranet intranet 202 equestrian sports h\u00edpica hippisme Pferderennen ippica 203 institutions instituciones institutions Institute istituzioni 204 education instrucci\u00f3n enseignement Bildung istruzione 205 ichthyology ictiolog\u00eda ichtyologie Fischkunde ittiologia 206 kayaking kayak kayak Kajak kayak 207 needlework trabajos de cosido ouvrage d'aiguilles Nadelarbeit lavori di cucito 208 job market trabajo travail Arbeit lavoro 209 legislation legislaci\u00f3n l\u00e9gislation Gesetzgebung legislazione 210 judo judo judo Judo judo 211 literature literatura litt\u00e9rature Literatur letteratura 212 linguistics ling\u00fc\u00edstica linguistique Linguistik linguistica 213 lithography litograf\u00eda lithographie Lithographie litografia 214 liturgy liturgia liturgie Liturgie liturgia 215 lottery loter\u00eda loterie Lotterie lotterie 216 animal slaughter mataderos abattage Schlachtung macellazione 217 knitting punto tricot Stricken maglia 218 manufacturing manufactura manufacture Herstellung manifattura 219 watercraft and nautical navigation marina marine Flotte marina 220 naval forces marina militar marine militaire Marine marina militare 221 marketing marketing marketing Marketing marketing 222 mathematics matem\u00e1tica math\u00e9matiques Mathematik matematica 223 mechanics mec\u00e1nica m\u00e9canique Mechanik meccanica 224 public administration administraci\u00f3n p\u00fablica administration publique Beh\u00f6rde amministrazione pubblica 225 medicine medicina m\u00e9decine Medizin medicina 226 alternative medicine medicina alternativa m\u00e9decine douce Alternative Medizin medicina alternativa 227 data storage memoria m\u00e9moire Speicher memoria 228 board games juegos de mesa jeux de table Brettspiele giochi da tavolo 229 metallurgy metalurgia m\u00e9tallurgie Metallurgie metallurgia 230 meteorology meteorolog\u00eda m\u00e9t\u00e9orologie Meteorologie meteorologia 231 meter and prosody m\u00e9trica m\u00e9trique Metrik metrica 232 ophthalmology oftalmolog\u00eda ophtalmologie Augenheilkunde oculistica 233 microbiology microbiolog\u00eda microbiologie Mikrobiologie microbiologia 234 armed forces militar militaire Milit\u00e4r militare 235 mineralogy mineralog\u00eda min\u00e9ralogie Mineralogie mineralogia 236 mines minas mines Minen miniere 237 government departments and ministries ministerios Minist\u00e8res Ministerien ministeri 238 rocket science tecnolog\u00eda misil\u00edstica missile Raketen missilistica 239 mythology mitolog\u00eda mythologie Mythologie mitologia 240 furniture muebles meubles M\u00f6blierung mobili 241 fashion moda mode Mode moda 242 computer monitor monitor \u00e9cran Monitor monitor 243 motorcycling motociclismo motocyclisme Motorradfahren motociclismo 244 powertrain design and engineering ingenier\u00eda mec\u00e1nica sports motoris\u00e9s Design und Technik motoristica 245 multimedia multimedia multim\u00e9dia Multimedia multimedia 246 masonry alba\u00f1iler\u00eda ma\u00e7onnerie Mauerwerk muratura 247 museums museos mus\u00e9es Museen musei 248 music m\u00fasica musique Musik musica 249 numismatics numism\u00e1tica numismatique Numismatik numismatica 250 swimming nataci\u00f3n nage Schwimmen nuoto 251 occultism ocultismo occultisme Okkultismus occultismo 252 hydrography hidrograf\u00eda hydrographie Hydrographie idrografia 253 orthodontics odontolog\u00eda odontologie Kieferorthop\u00e4die odontoiatria 254 oncology oncolog\u00eda oncologie Onkologie oncologia 255 opera \u00f3pera op\u00e9ra Oper opera 256 goldsmithing orfebrer\u00eda orf\u00e8vrerie Juwelierskunst oreficeria 257 ornithology ornitolog\u00eda ornithologie Vogelkunde ornitologia 258 watchmaking relojer\u00eda horlogerie Uhrenindustrie orologeria 259 orthopedics ortopedia orthop\u00e9die Orthop\u00e4die ortopedia 260 hospitals hospital h\u00f4pital Krankenhaus ospedale 261 optics \u00f3ptica optique Optik ottica 262 paleography paleograf\u00eda pal\u00e9ographie Pal\u00e4ographie paleografia 263 paleontology paleontolog\u00eda pal\u00e9ontologie Pal\u00e4ontologie paleontologia 264 paleozoology paleozoolog\u00eda pal\u00e9ozoologie Pal\u00e4ozoologie paleozoologia 265 basketball baloncesto basket-ball Basketball pallacanestro 266 handball balonmano handball Handball pallamano 267 water polo polo acu\u00e1tico water-polo Wasserball pallanuoto 268 volleyball voleibol volley-ball Volleyball pallavolo 269 papyrology papirolog\u00eda papyrologie Papyrologie papirologia 270 parapsychology parapsicolog\u00eda parapsychologie Parapsychologie parapsicologia 271 Parliament and legislative bodies parlamento parlement Parlament parlamento 272 skating patinaje patinage Schlittschuhlaufen pattinaggio 273 pedagogy pedagog\u00eda p\u00e9dagogie P\u00e4dagogie pedagogia 274 classical music m\u00fasica cl\u00e1sica musique classique klassische Musik musica classica 275 fishing pesca p\u00eache Angeln pesca 276 sport fishing pesca deportiva p\u00eache sportive Sportfischen pesca sportiva 277 spearfishing pesca submarina p\u00eache sous-marine Unterwasserfischen pesca subacquea 278 petrochemistry petroqu\u00edmica p\u00e9trochimie Petrochemie petrolchimica 279 petrochemicals petr\u00f3leo p\u00e9trole \u00d6l petrolio 280 table tennis ping-pong ping-pong Tischtennis ping-pong 281 painting pintura peinture Malerei pittura 282 glass painting pintura sobre vidrio peinture sur verre Glasmalerei pittura vetraria 283 poetry poes\u00eda po\u00e9sie Poesie poesia 284 poker p\u00f3quer poker Poker poker 285 politics pol\u00edtica politique Politik politica 286 police polic\u00eda police Polizei polizia 287 postal service correo poste Post posta 288 prehistory prehistoria pr\u00e9histoire Vorgeschichte preistoria 289 welfare previsi\u00f3n \u00e9tat providence Wohlfahrt previdenza 290 computer processors procesadores processeurs Prozessoren processori 291 perfumery perfumer\u00eda parfumerie Parf\u00fcmerie profumeria 292 computer programming programaci\u00f3n programmation Programmierung programmazione 293 office productivity software programas para la oficina programme de bureau Business-Programme programmi per ufficio 294 psychoanalysis psicoan\u00e1lisis psychanalyse Psychoanalyse psicanalisi 295 psychiatry psiquiatr\u00eda psychiatrie Psychiatrie psichiatria 296 psychology psicolog\u00eda psychologie Psychologie psicologia 297 advertising publicidad publicit\u00e9 Werbung pubblicit\u00e0 298 radio broadcasting radio radio Radio radio 299 radiology radiolog\u00eda radiologie Radiologie radiologia 300 kitchen tools utensilios de cocina ustensiles de cuisine K\u00fcchenutensilien strumenti da cucina 301 public relations relaciones p\u00fablicas relations publiques Public Relations relazioni pubbliche 302 religion religi\u00f3n religion Verehrung religione 303 information technology redes r\u00e9seaux Netze reti 304 rhetoric ret\u00f3rica rh\u00e9torique Rhetorik retorica 305 lacework bordado broderie Spitze ricamo 306 restaurant industry restauraci\u00f3n restauration Catering ristorazione 307 robotics rob\u00f3tica robotique Robotik robotica 308 rugby rugby rugby Rugby rugby 309 health care sanidad sant\u00e9 Gesundheitsamt sanit\u00e0 310 tailoring sastrer\u00eda haute couture Schneiderei sartoria 311 chess ajedrez \u00e9checs Schach scacchi 312 scenography escenograf\u00eda sc\u00e9nographie Szenografie scenografia 313 graphics card tarjetas gr\u00e1ficas cartes graphiques Grafikkarten schede grafiche 314 fencing esgrima escrime Fechten scherma 315 skiing esqu\u00ed ski Skifahren sci 316 downhill skiing esqu\u00ed alpino ski alpin Abfahrtsski sci alpino 317 cross-country skiing esqu\u00ed de fondo ski de fond Langlauf sci di fondo 318 water skiing esqu\u00ed n\u00e1utico ski nautique Wasserski sci nautico 319 STEM ciencias puras science pure exakte Wissenschaft scienze pure 320 social science ciencias sociales sciences sociales Sozialwissenschaft scienze sociali 321 sculpture escultura sculpture Skulptur scultura 322 school systems escuela \u00e9cole Schule scuola 323 forestry silvicultura sylviculture Waldbau selvicoltura 324 social services servicios sociales services sociaux Sozialdienst servizi sociali 325 sexology sexolog\u00eda sexologie Sexologie sessuologia 326 sects sectas sectes Sekten sette 327 security and public safety seguridad social s\u00e9curit\u00e9 sociale Sozialdienstleistungen sicurezza sociale 328 iron metallurgy siderurgia sid\u00e9rurgie Eisen- und Stahlindustrie siderurgia 329 unions sindicatos syndicats Gewerkschaften sindacati 330 seismology sismolog\u00eda sismologie Seismologie sismologia 331 operating systems sistemas operativos syst\u00e8me d'exploitation Betriebssysteme sistemi operativi 332 mining industry industria de la extracci\u00f3n industrie extractive Bergbau industria estrattiva 333 mushing carreras de trineo tra\u00eenage Schlittenhund sleddog 334 sledding trineo luge Schlitten slittino 335 snowboard snowboard snowboard Snowboard snowboard 336 sociology sociolog\u00eda sociologie Soziologie sociologia 337 softball s\u00f3fbol softball Softball softball 338 software software logiciel Software software 339 entertainment espect\u00e1culo spectacle Show spettacolo 340 sports deportes sport Sport sport 341 squash squash squash Squash squash 342 printing imprenta impression Druck stampa 343 printers impresoras imprimantes Druckers stampanti 344 statistics estad\u00edstica statistique Statistik statistica 345 history historia histoire Geschichte storia 346 ancient history historia antigua histoire ancienne alte Geschichte storia antica 347 contemporary history historia contempor\u00e1nea histoire contemporaine Zeitgeschichte storia contemporanea 348 medieval history historia medieval histoire du moyen \u00e2ge mittelalterliche Geschichte storia medievale 349 modern history historia moderna histoire des temps modernes moderne Geschichte storia moderna 350 roads and traffic carretera route Autobahn strada 351 musical instruments instrumentos musicales instruments de musique Musikinstrumente strumenti 352 hospitality facilities instalaciones tur\u00edsticas activit\u00e9s touristiques touristische Anlagen strutture turistiche 353 numismatics (study) estudio de las monedas \u00e9tude de monnaie M\u00fcnzkunde studio delle monete 354 scuba diving actividades submarinas plong\u00e9e Tauchen subacquea 355 surfing surf surf Surf surf 356 theatre teatro th\u00e9\u00e2tre Theater teatro 357 technical drawing t\u00e9cnica technicisme mechanische Zeichnung tecnica 358 technology tecnolog\u00eda technologie Technologie tecnologia 359 telecommunications telecomunicaciones t\u00e9l\u00e9communication Telekommunikation telecomunicazioni 360 telephony telefon\u00eda t\u00e9l\u00e9phonie Telefonie telefonia 361 telegraph tel\u00e9grafo t\u00e9l\u00e9graphe Telegraph telegrafo 362 television televisi\u00f3n t\u00e9l\u00e9vision Fernsehen televisione 363 tennis tenis tennis Tennis tennis 364 theology teolog\u00eda th\u00e9ologie Theologie teologia 365 thermohydraulics termohidr\u00e1ulica thermohydraulique Thermohydraulik termo-idraulica 366 thermodynamics termodin\u00e1mica thermodynamique Thermodynamik termodinamica 367 textiles tejidos tissus Stoff tessuti 368 target-shooting tiro al blanco tir \u00e0 la cible Scheibenschie\u00dfen tiro a segno 369 skeet shooting tiro al plato tir au pigeon Tontaubenschie\u00dfen tiro al piattello 370 topography topograf\u00eda topographie Topographie topografia 371 transportation transportes transports Transporte trasporti 372 trekking senderismo trekking Trekking trekking 373 diving saltos plongeon Wasserspringen tuffi 374 tourism turismo tourisme Tourismus turismo 375 crochet ganchillo crochet H\u00e4kelei uncinetto 376 university universidad universit\u00e9 Universit\u00e4t universit\u00e0 377 city planning urbanismo urbanisme Stadtplanung urbanistica 378 sailing vela voile Segeln vela 379 veterinary science veterinaria m\u00e9decine v\u00e9t\u00e9rinaire Tiermedizin veterinaria 380 glassware vidrer\u00eda verrerie Glasgeschirr vetreria 381 video games videojuegos jeux vid\u00e9o Videospiele videogiochi 382 wine industry viticultura viticulture Weinanbau viticoltura 383 volcanology vulcanolog\u00eda volcanologie Vulkanologie vulcanologia 384 windsurfing windsurf planche \u00e0 voile Windsurf windsurf 385 zoology zoolog\u00eda zoologie Zoologie zoologia 386 zootechnics zootecnia zootechnie Tierzucht zootecnia 387 bureaucratic terminology t\u00e9rminos burocr\u00e1ticos termes bureaucratiques b\u00fcrokratische Begriffe termini burocratici 388 scientific terms t\u00e9rminos cient\u00edficos termes scientifiques wissenschaftliche Begriffe termini scientifici 389 technical terminology t\u00e9rminos t\u00e9cnicos termes techniques Fachbegriffe termini tecnici 390 cosmetic industry industria cosm\u00e9tica industrie de la cosm\u00e9tique Kosmetikindustrie industria cosmetica 391 gymnastics gimnasia gymnastique Gymnastik ginnastica 392 rhythmic gymnastics gimnasia r\u00edtmica gymnastique rythmique Calisthenics ginnastica ritmica 393 artistic gymnastics gimnasia art\u00edstica gymnastique sportive Kunstturnen ginnastica artistica 394 lawn bowls juego de las bochas jeu de boules Kugelsport gioco delle bocce 395 checkers damas dames Damespiel dama 396 science fiction ciencia ficci\u00f3n science-fiction Science-Fiction fantascienza 397 accounting contabilidad comptabilit\u00e9 Buchhaltung contabilit\u00e0 398 marine biology biolog\u00eda marina biologie marine Meeresbiologie biologia marina 399 parachuting paracaidismo parachutisme Fallschirmspringen paracadutismo 400 gambling juego de azar jeux de hasard Gl\u00fccksspiel gioco d'azzardo 401 karate k\u00e1rate karat\u00e9 Karate karate 402 typewriting dactilograf\u00eda dactylographie Maschinenschreiben dattilografia 403 shorthand estenograf\u00eda st\u00e9nographie Kurzschrift stenografia 404 Hinduism hinduismo hindouisme Hinduismus induismo 405 polo polo polo Polo polo 406 pornography pornograf\u00eda pornographie Pornographie pornografia","title":"Standard context topics"},{"location":"reference/topics/#standard-context-topics","text":"Here are the topics that keyphrase extraction can detect when using the standard context . The first column shows the identification number of the topic, which is the same for all languages. The other columns show the topic labels, which vary by language. ID English Spanish French German Italian 0 clothing vestuario v\u00eatements Kleidung abbigliamento 1 homeopathy homeopat\u00eda hom\u00e9opathie Hom\u00f6opathie omeopatia 2 clothing accessories accesorios accessoire Kleidungszubeh\u00f6r accessori 3 acoustics ac\u00fastica acoustique Akustik acustica 4 aviation aeron\u00e1utica a\u00e9ronautique Aeronautik aeronautica 5 air force aeron\u00e1utica militar a\u00e9ronautique militaire Luftwaffe aeronautica militare 6 aeronautic technology aerotecnia a\u00e9rotechnique Luftzeugtechnik aerotecnica 7 agriculture agricultura agriculture Landwirtschaft agricoltura 8 food alimentos nourriture Lebensmittel alimenti 9 animal husbandry crianza y ganader\u00eda \u00e9levage Zucht allevamento 10 mountaineering alpinismo alpinisme Bergsteigen alpinismo 11 anatomy anatom\u00eda anatomie Anatomie anatomia 12 antique trade comercio de antig\u00fcedades antiquit\u00e9 Antiquit\u00e4ten antiquariato 13 anthropology antropolog\u00eda anthropologie Anthropologie antropologia 14 free diving apnea apn\u00e9e Atemstillstand apnea 15 heraldry her\u00e1ldica h\u00e9raldique Heraldik araldica 16 archaeology arqueolog\u00eda arch\u00e9ologie Arch\u00e4ologie archeologia 17 architecture arquitectura architecture Architektur architettura 18 archival studies archiv\u00edstica archivage Archivierung archivistica 19 archery arco arc Bogenschie\u00dfen arco 20 arithmetic aritm\u00e9tica arithm\u00e9tique Arithmetik aritmetica 21 weapons armas armes Bewaffnung armi 22 interior design decoraci\u00f3n ameublement Innenausstattung arredamento 23 art arte art Kunst arte 24 martial arts artes marciales arts martiaux Kampfsport arti marziali 25 crafts artesan\u00eda artisanat Handwerk artigianato 26 artistic crafts artesan\u00eda art\u00edstica artisanat d'art Kunsthandwerk artigianato artistico 27 visual arts artes gr\u00e1ficas graphique graphische Kunst arti grafiche 28 artillery artiller\u00eda artillerie Artillerie artiglieria 29 insurance industry seguros assurance Versicherung assicurazioni 30 auction subastas vente Auktion aste 31 astrophysics astrof\u00edsica astrophysique Astrophysik astrofisica 32 astrology astrolog\u00eda astrologie Astrologie astrologia 33 astronautics astron\u00e1utica astronautique Raumfahrt astronautica 34 astronomy astronom\u00eda astronomie Astronomie astronomia 35 culture t\u00e9rminos culturales termes culturels kulturelle Begriffe termini culturali 36 athletics atletismo athl\u00e9tisme Leichtathletik atletica leggera 37 wrestling and weightlifting halterofilia y lucha halt\u00e9rophilie Schwerathletik atletica pesante 38 automation automatizaci\u00f3n automation Automatisierung automazione 39 motor vehicles veh\u00edculos de motor v\u00e9hicule automobile Kraftfahrzeug autoveicoli 40 motor racing automovilismo automobilisme Motorsport automobilismo 41 air travel aviaci\u00f3n aviation Luftfahrt aviazione 42 ballistics bal\u00edstica balistique Ballistik balistica 43 ballet ballet-baile ballet Ballett balletto-danza 44 banking banco banque Bank banca 45 baseball b\u00e9isbol base-ball Baseball baseball 46 The Bible Biblia Bible Bibel Bibbia 47 libraries bibliotecas biblioth\u00e8que Bibliotheken biblioteche 48 billiard sports billar billard Billard biliardo 49 biochemistry bioqu\u00edmica biochimie Biochemie biochimica 50 biophysics biof\u00edsica biophysique Biophysik biofisica 51 biology biolog\u00eda biologie Biologie biologia 52 biotechnology biotecnolog\u00eda biotechnologie Biotechnologie biotecnologie 53 bobsledding bobsleigh bob Bob bob 54 stock exchange bolsa Bourse B\u00f6rse borsa 55 botany bot\u00e1nica botanique Botanik botanica 56 bowling bolos bowling Bowling bowling 57 boxing boxeo boxe Boxen boxe 58 do it yourself bricolaje bricolage do it yourself bricolage 59 bridge (card game) bridge bridge Bridge bridge 60 Buddhism budismo bouddhisme Buddhismus buddismo 61 hunting caza chasse Jagd caccia 62 CAD (computer aided design) cad cad CAD (rechnerunterst\u00fctzte Konstruktion) cad 63 five-a-side football f\u00fatbol sala football \u00e0 cinq Futsal calcetto 64 soccer (US) f\u00fatbol football Fu\u00dfball calcio 65 footwear calzados chaussure Schuhwerk calzature 66 rowing pirag\u00fcismo aviron Rudern canottaggio 67 singing canto chant Gesang canto 68 coal mining carb\u00f3n charbonnage Kohlebergwerk carbone 69 cardiology cardiolog\u00eda cardiologie Kardiologie cardiologia 70 cartography cartograf\u00eda cartographie Kartographie cartografia 71 pottery cer\u00e1mica c\u00e9ramique T\u00f6pferei ceramica 72 chemistry qu\u00edmica chimie Chemie chimica 73 inorganic chemistry qu\u00edmica inorg\u00e1nica chimie inorganique anorganische Chemie chimica inorganica 74 organic chemistry qu\u00edmica org\u00e1nica chimie organique organische Chemie chimica organica 75 surgery cirug\u00eda chirurgie Chirurgie chirurgia 76 cycling ciclismo cyclisme Radfahren ciclismo 77 film industry cine cin\u00e9ma Kino cinema 78 circus circo cirque Zirkus circo 79 track cycling ciclismo en pista cyclisme sur piste Bahnrennen ciclismo su pista 80 cytology citolog\u00eda cytologie Zytologie citologia 81 collecting coleccionismo collection Sammeln collezionismo 82 trade comercio commerce Handel commercio 83 electronic components piezas electr\u00f3nicas composants \u00e9lectroniques elektronischen Komponenten componenti elettronici 84 music composition composici\u00f3n composition Komposition composizione 85 computer art infoarte computer art Computerkunst computer art 86 computer graphics infograf\u00eda infographie Grafik computer grafica 87 media and communication comunicaci\u00f3n communication Massenkommunikation comunicazione 88 leather processing curtidur\u00eda tannerie Gerberei conceria 89 beauty products cosm\u00e9tica cosm\u00e9tologie Kosmetika cosmesi 90 cosmography cosmograf\u00eda cosmographie Kosmographie cosmografia 91 cricket cr\u00edquet cricket Kricket cricket 92 crime criminalidad criminalit\u00e9 Kriminalit\u00e4t criminalit\u00e0 93 Christianity cristianismo Christianisme Christentum cristianesimo 94 news cr\u00f3nica chronique Nachrichten cronaca 95 database base de datos base de donn\u00e9es Datenbank database 96 decoupage decoupage d\u00e9coupage Serviettentechnik decoupage 97 dermatology dermatolog\u00eda dermatologie Dermatologie dermatologia 98 teaching methodology did\u00e1ctica didactique Didaktik didattica 99 dietetics diet\u00e9tica di\u00e9t\u00e9tique Di\u00e4tetik dietetica 100 diplomacy diplomacia diplomatie Diplomatie diplomazia 101 law derecho droit Gesetz diritto 102 non-criminal law derecho civil droit civil Zivilrecht diritto civile 103 business and commercial law derecho mercantil droit commercial Handelsrecht diritto commerciale 104 international law derecho internacional droit international V\u00f6lkerrecht diritto internazionale 105 criminal law derecho penal droit p\u00e9nal Strafrecht diritto penale 106 private law derecho privado droit priv\u00e9 Privatrecht diritto privato 107 administrative law derecho p\u00fablico y administrativo droit public et administratif Staats-und Verwaltungsrecht diritto pubblico e amministrativo 108 electronic storage discos disques Festplatten dischi 109 illustration dibujo dessin Zeichnung disegno 110 Judaism hebra\u00edsmo juda\u00efsme Judentum ebraismo 111 ecology ecolog\u00eda \u00e9cologie \u00d6kologie ecologia 112 e-commerce comercio electr\u00f3nico e-commerce E-Commerce e-commerce 113 the economy econom\u00eda \u00e9conomie Wirtschaft economia 114 construction industry construcci\u00f3n construction Bauindustrie edilizia 115 publishing industria editorial \u00e9dition Verlagswesen editoria 116 electricity electricidad \u00e9lectricit\u00e9 Elektrizit\u00e4t elettricit\u00e0 117 electronics electr\u00f3nica \u00e9lectronique Elektronik elettronica 118 electrotechnics electrot\u00e9cnica \u00e9lectrotechnique Elektrotechnik elettrotecnica 119 e-mail e-mail e-mail E-Mail e-mail 120 embryology embriolog\u00eda embryologie Embryologie embriologia 121 energy energ\u00eda \u00e9nergie Energie energia 122 puzzles enigm\u00edstica jeux d'esprit R\u00e4tsel enigmistica 123 enology enolog\u00eda \u0153nologie \u00d6nologie enologia 124 entomology entomolog\u00eda entomologie Insektenkunde entomologia 125 epigraphy epigraf\u00eda \u00e9pigraphie Epigraphie epigrafia 126 equitation equitaci\u00f3n \u00e9quitation Reitsport equitazione 127 ethics \u00e9tica \u00e9thique Ethik etica 128 Islam islam islam Islam islam 129 ethnology etnolog\u00eda ethnologie Ethnologie etnologia 130 TV broadcasting eventos televisivos \u00e9v\u00e9nements t\u00e9l\u00e9vis\u00e9s Rundfunk Veranstaltungen eventi televisivi 131 evolution evoluci\u00f3n \u00e9volution Evolution evoluzione 132 carpentry carpinter\u00eda menuiserie Zimmerei falegnameria 133 pharmaceuticals farmacia pharmacie Pharmazie farmacia 134 pharmacology farmacolog\u00eda pharmacologie Pharmakologie farmacologia 135 trains ferrocarril chemin de fer Eisenbahn ferrovia 136 philately filatelia philat\u00e9lie Briefmarkensammeln filatelia 137 philology filolog\u00eda philologie Philologie filologia 138 philosophy filosof\u00eda philosophie Philosophie filosofia 139 finance finanzas finance Finanz finanza 140 finance (private) finanzas privadas finances priv\u00e9es Privatfinanz finanza privata 141 public financing finanzas p\u00fablicas finances publiques \u00f6ffentliche Finanz finanza pubblica 142 revenue services fisco fisc Einnahmen fisco 143 physics f\u00edsica physique Physik fisica 144 atomic physics f\u00edsica at\u00f3mica physique atomique Atomphysik fisica atomica 145 physiology fisiolog\u00eda physiologie Physiologie fisiologia 146 phytopathology fitopatolog\u00eda phytopathologie Phytopathologie fitopatologia 147 folklore folclore folklore Volkskunde folclore 148 phonetics fon\u00e9tica phon\u00e9tique Phonetik fonetica 149 phonology fonolog\u00eda phonologie Phonologie fonologia 150 American football f\u00fatbol americano football am\u00e9ricain American Football football americano 151 photography fotograf\u00eda photographie Fotografie fotografia 152 artistic photography fotograf\u00eda art\u00edstica photographie artistique k\u00fcnstlerische Fotografie fotografia artistica 153 free climbing escalada libre free climbing Klettern free climbing 154 fruit growing fruticultura arboriculture fruiti\u00e8re Obstbau frutticoltura 155 cartoons and comic books historieta bande dessin\u00e9e Comics fumetto 156 art galleries galer\u00edas galerie Galerie gallerie 157 gastronomy gastronom\u00eda gastronomie Gastronomie gastronomia 158 genealogy genealog\u00eda g\u00e9n\u00e9alogie Genealogie genealogia 159 genetics gen\u00e9tica g\u00e9n\u00e9tique Genetik genetica 160 geophysics geof\u00edsica g\u00e9ophysique Geophysik geofisica 161 geography geograf\u00eda g\u00e9ographie Geographie geografia 162 geology geolog\u00eda g\u00e9ologie Geologie geologia 163 geometry geometr\u00eda g\u00e9om\u00e9trie Geometrie geometria 164 gardening jardiner\u00eda jardinage G\u00e4rtnerei giardinaggio 165 toys juguetes jouets Spielzeuge giocattoli 166 card games juegos de naipes jeux de cartes Kartenspielen giochi con le carte 167 games and toys juego jeu Spiel gioco 168 jewelry joyer\u00eda bijouterie Schmuck gioielleria 169 the press peri\u00f3dicos journaux Zeitungen giornali 170 journalism periodismo journalisme Journalismus giornalismo 171 go-karting kart go-kart Go-Kart go kart 172 golf golf golf Golf golf 173 graffiti grafiti graffiti Graffito graffiti 174 grammar and syntax gram\u00e1tica grammaire Grammatik grammatica 175 computer hardware hardware mat\u00e9riel Hardware hardware 176 hockey hockey hockey Hockey hockey 177 ice hockey hockey sobre hielo hockey sur glace Eishockey hockey su ghiaccio 178 hydraulics hidr\u00e1ulica hydraulique Hydraulik idraulica 179 immunology inmunolog\u00eda immunologie Immunologie immunologia 180 business empresa entreprise Firma impresa 181 engraving grabado gravure Gravierung incisione 182 industry industria industrie Industrie industria 183 aeronautics industry industria aeron\u00e1utica industrie a\u00e9ronautique Luftfahrtindustrie industria aeronautica 184 food industry industria alimentaria industrie alimentaire Lebensmittelindustrie industria alimentare 185 automotive industry industria automov\u00edlistica industrie automobile Autoindustrie industria automobilistica 186 arms industry industria b\u00e9lica industrie de guerre Kriegsindustrie industria bellica 187 footwear industry industria del calzado industrie de la chaussure Schuhindustrie industria calzaturiera 188 ceramic industry industria cer\u00e1mica industrie c\u00e9ramique Keramikindustrie industria ceramica 189 chemical industry industria qu\u00edmica industrie chimique Chemieindustrie industria chimica 190 rail transport industria del ferrocarril industrie de chemin de fer Eisenbahnindustrie industria ferroviaria 191 furniture industry industria mueblera industrie des meubles M\u00f6belindustrie industria mobiliera 192 motorcycle industry industria motociclista industrie de motos Motorrad-Industrie industria motociclistica 193 shipbuilding industria n\u00e1utica industrie naval Schifffahrtsindustrie industria nautica 194 textile industry industria textil industrie textiles Textilindustrie industria tessile 195 glass manufacturing industria del vidrio industrie de verre Glasherstellung industria vetraria 196 industrial design dise\u00f1o industrial dessin industriel Industriedesign industrial design 197 computer science inform\u00e1tica informatique Informatik informatica 198 engineering ingenier\u00eda g\u00e9nie Ingenieurwesen ingegneria 199 aerospace engineering ingenier\u00eda aeroespacial g\u00e9nie a\u00e9rospatial Luft- und Raumfahrttechnik ingegneria aerospaziale 200 Internet internet internet Internet internet 201 intranet intranet intranet Intranet intranet 202 equestrian sports h\u00edpica hippisme Pferderennen ippica 203 institutions instituciones institutions Institute istituzioni 204 education instrucci\u00f3n enseignement Bildung istruzione 205 ichthyology ictiolog\u00eda ichtyologie Fischkunde ittiologia 206 kayaking kayak kayak Kajak kayak 207 needlework trabajos de cosido ouvrage d'aiguilles Nadelarbeit lavori di cucito 208 job market trabajo travail Arbeit lavoro 209 legislation legislaci\u00f3n l\u00e9gislation Gesetzgebung legislazione 210 judo judo judo Judo judo 211 literature literatura litt\u00e9rature Literatur letteratura 212 linguistics ling\u00fc\u00edstica linguistique Linguistik linguistica 213 lithography litograf\u00eda lithographie Lithographie litografia 214 liturgy liturgia liturgie Liturgie liturgia 215 lottery loter\u00eda loterie Lotterie lotterie 216 animal slaughter mataderos abattage Schlachtung macellazione 217 knitting punto tricot Stricken maglia 218 manufacturing manufactura manufacture Herstellung manifattura 219 watercraft and nautical navigation marina marine Flotte marina 220 naval forces marina militar marine militaire Marine marina militare 221 marketing marketing marketing Marketing marketing 222 mathematics matem\u00e1tica math\u00e9matiques Mathematik matematica 223 mechanics mec\u00e1nica m\u00e9canique Mechanik meccanica 224 public administration administraci\u00f3n p\u00fablica administration publique Beh\u00f6rde amministrazione pubblica 225 medicine medicina m\u00e9decine Medizin medicina 226 alternative medicine medicina alternativa m\u00e9decine douce Alternative Medizin medicina alternativa 227 data storage memoria m\u00e9moire Speicher memoria 228 board games juegos de mesa jeux de table Brettspiele giochi da tavolo 229 metallurgy metalurgia m\u00e9tallurgie Metallurgie metallurgia 230 meteorology meteorolog\u00eda m\u00e9t\u00e9orologie Meteorologie meteorologia 231 meter and prosody m\u00e9trica m\u00e9trique Metrik metrica 232 ophthalmology oftalmolog\u00eda ophtalmologie Augenheilkunde oculistica 233 microbiology microbiolog\u00eda microbiologie Mikrobiologie microbiologia 234 armed forces militar militaire Milit\u00e4r militare 235 mineralogy mineralog\u00eda min\u00e9ralogie Mineralogie mineralogia 236 mines minas mines Minen miniere 237 government departments and ministries ministerios Minist\u00e8res Ministerien ministeri 238 rocket science tecnolog\u00eda misil\u00edstica missile Raketen missilistica 239 mythology mitolog\u00eda mythologie Mythologie mitologia 240 furniture muebles meubles M\u00f6blierung mobili 241 fashion moda mode Mode moda 242 computer monitor monitor \u00e9cran Monitor monitor 243 motorcycling motociclismo motocyclisme Motorradfahren motociclismo 244 powertrain design and engineering ingenier\u00eda mec\u00e1nica sports motoris\u00e9s Design und Technik motoristica 245 multimedia multimedia multim\u00e9dia Multimedia multimedia 246 masonry alba\u00f1iler\u00eda ma\u00e7onnerie Mauerwerk muratura 247 museums museos mus\u00e9es Museen musei 248 music m\u00fasica musique Musik musica 249 numismatics numism\u00e1tica numismatique Numismatik numismatica 250 swimming nataci\u00f3n nage Schwimmen nuoto 251 occultism ocultismo occultisme Okkultismus occultismo 252 hydrography hidrograf\u00eda hydrographie Hydrographie idrografia 253 orthodontics odontolog\u00eda odontologie Kieferorthop\u00e4die odontoiatria 254 oncology oncolog\u00eda oncologie Onkologie oncologia 255 opera \u00f3pera op\u00e9ra Oper opera 256 goldsmithing orfebrer\u00eda orf\u00e8vrerie Juwelierskunst oreficeria 257 ornithology ornitolog\u00eda ornithologie Vogelkunde ornitologia 258 watchmaking relojer\u00eda horlogerie Uhrenindustrie orologeria 259 orthopedics ortopedia orthop\u00e9die Orthop\u00e4die ortopedia 260 hospitals hospital h\u00f4pital Krankenhaus ospedale 261 optics \u00f3ptica optique Optik ottica 262 paleography paleograf\u00eda pal\u00e9ographie Pal\u00e4ographie paleografia 263 paleontology paleontolog\u00eda pal\u00e9ontologie Pal\u00e4ontologie paleontologia 264 paleozoology paleozoolog\u00eda pal\u00e9ozoologie Pal\u00e4ozoologie paleozoologia 265 basketball baloncesto basket-ball Basketball pallacanestro 266 handball balonmano handball Handball pallamano 267 water polo polo acu\u00e1tico water-polo Wasserball pallanuoto 268 volleyball voleibol volley-ball Volleyball pallavolo 269 papyrology papirolog\u00eda papyrologie Papyrologie papirologia 270 parapsychology parapsicolog\u00eda parapsychologie Parapsychologie parapsicologia 271 Parliament and legislative bodies parlamento parlement Parlament parlamento 272 skating patinaje patinage Schlittschuhlaufen pattinaggio 273 pedagogy pedagog\u00eda p\u00e9dagogie P\u00e4dagogie pedagogia 274 classical music m\u00fasica cl\u00e1sica musique classique klassische Musik musica classica 275 fishing pesca p\u00eache Angeln pesca 276 sport fishing pesca deportiva p\u00eache sportive Sportfischen pesca sportiva 277 spearfishing pesca submarina p\u00eache sous-marine Unterwasserfischen pesca subacquea 278 petrochemistry petroqu\u00edmica p\u00e9trochimie Petrochemie petrolchimica 279 petrochemicals petr\u00f3leo p\u00e9trole \u00d6l petrolio 280 table tennis ping-pong ping-pong Tischtennis ping-pong 281 painting pintura peinture Malerei pittura 282 glass painting pintura sobre vidrio peinture sur verre Glasmalerei pittura vetraria 283 poetry poes\u00eda po\u00e9sie Poesie poesia 284 poker p\u00f3quer poker Poker poker 285 politics pol\u00edtica politique Politik politica 286 police polic\u00eda police Polizei polizia 287 postal service correo poste Post posta 288 prehistory prehistoria pr\u00e9histoire Vorgeschichte preistoria 289 welfare previsi\u00f3n \u00e9tat providence Wohlfahrt previdenza 290 computer processors procesadores processeurs Prozessoren processori 291 perfumery perfumer\u00eda parfumerie Parf\u00fcmerie profumeria 292 computer programming programaci\u00f3n programmation Programmierung programmazione 293 office productivity software programas para la oficina programme de bureau Business-Programme programmi per ufficio 294 psychoanalysis psicoan\u00e1lisis psychanalyse Psychoanalyse psicanalisi 295 psychiatry psiquiatr\u00eda psychiatrie Psychiatrie psichiatria 296 psychology psicolog\u00eda psychologie Psychologie psicologia 297 advertising publicidad publicit\u00e9 Werbung pubblicit\u00e0 298 radio broadcasting radio radio Radio radio 299 radiology radiolog\u00eda radiologie Radiologie radiologia 300 kitchen tools utensilios de cocina ustensiles de cuisine K\u00fcchenutensilien strumenti da cucina 301 public relations relaciones p\u00fablicas relations publiques Public Relations relazioni pubbliche 302 religion religi\u00f3n religion Verehrung religione 303 information technology redes r\u00e9seaux Netze reti 304 rhetoric ret\u00f3rica rh\u00e9torique Rhetorik retorica 305 lacework bordado broderie Spitze ricamo 306 restaurant industry restauraci\u00f3n restauration Catering ristorazione 307 robotics rob\u00f3tica robotique Robotik robotica 308 rugby rugby rugby Rugby rugby 309 health care sanidad sant\u00e9 Gesundheitsamt sanit\u00e0 310 tailoring sastrer\u00eda haute couture Schneiderei sartoria 311 chess ajedrez \u00e9checs Schach scacchi 312 scenography escenograf\u00eda sc\u00e9nographie Szenografie scenografia 313 graphics card tarjetas gr\u00e1ficas cartes graphiques Grafikkarten schede grafiche 314 fencing esgrima escrime Fechten scherma 315 skiing esqu\u00ed ski Skifahren sci 316 downhill skiing esqu\u00ed alpino ski alpin Abfahrtsski sci alpino 317 cross-country skiing esqu\u00ed de fondo ski de fond Langlauf sci di fondo 318 water skiing esqu\u00ed n\u00e1utico ski nautique Wasserski sci nautico 319 STEM ciencias puras science pure exakte Wissenschaft scienze pure 320 social science ciencias sociales sciences sociales Sozialwissenschaft scienze sociali 321 sculpture escultura sculpture Skulptur scultura 322 school systems escuela \u00e9cole Schule scuola 323 forestry silvicultura sylviculture Waldbau selvicoltura 324 social services servicios sociales services sociaux Sozialdienst servizi sociali 325 sexology sexolog\u00eda sexologie Sexologie sessuologia 326 sects sectas sectes Sekten sette 327 security and public safety seguridad social s\u00e9curit\u00e9 sociale Sozialdienstleistungen sicurezza sociale 328 iron metallurgy siderurgia sid\u00e9rurgie Eisen- und Stahlindustrie siderurgia 329 unions sindicatos syndicats Gewerkschaften sindacati 330 seismology sismolog\u00eda sismologie Seismologie sismologia 331 operating systems sistemas operativos syst\u00e8me d'exploitation Betriebssysteme sistemi operativi 332 mining industry industria de la extracci\u00f3n industrie extractive Bergbau industria estrattiva 333 mushing carreras de trineo tra\u00eenage Schlittenhund sleddog 334 sledding trineo luge Schlitten slittino 335 snowboard snowboard snowboard Snowboard snowboard 336 sociology sociolog\u00eda sociologie Soziologie sociologia 337 softball s\u00f3fbol softball Softball softball 338 software software logiciel Software software 339 entertainment espect\u00e1culo spectacle Show spettacolo 340 sports deportes sport Sport sport 341 squash squash squash Squash squash 342 printing imprenta impression Druck stampa 343 printers impresoras imprimantes Druckers stampanti 344 statistics estad\u00edstica statistique Statistik statistica 345 history historia histoire Geschichte storia 346 ancient history historia antigua histoire ancienne alte Geschichte storia antica 347 contemporary history historia contempor\u00e1nea histoire contemporaine Zeitgeschichte storia contemporanea 348 medieval history historia medieval histoire du moyen \u00e2ge mittelalterliche Geschichte storia medievale 349 modern history historia moderna histoire des temps modernes moderne Geschichte storia moderna 350 roads and traffic carretera route Autobahn strada 351 musical instruments instrumentos musicales instruments de musique Musikinstrumente strumenti 352 hospitality facilities instalaciones tur\u00edsticas activit\u00e9s touristiques touristische Anlagen strutture turistiche 353 numismatics (study) estudio de las monedas \u00e9tude de monnaie M\u00fcnzkunde studio delle monete 354 scuba diving actividades submarinas plong\u00e9e Tauchen subacquea 355 surfing surf surf Surf surf 356 theatre teatro th\u00e9\u00e2tre Theater teatro 357 technical drawing t\u00e9cnica technicisme mechanische Zeichnung tecnica 358 technology tecnolog\u00eda technologie Technologie tecnologia 359 telecommunications telecomunicaciones t\u00e9l\u00e9communication Telekommunikation telecomunicazioni 360 telephony telefon\u00eda t\u00e9l\u00e9phonie Telefonie telefonia 361 telegraph tel\u00e9grafo t\u00e9l\u00e9graphe Telegraph telegrafo 362 television televisi\u00f3n t\u00e9l\u00e9vision Fernsehen televisione 363 tennis tenis tennis Tennis tennis 364 theology teolog\u00eda th\u00e9ologie Theologie teologia 365 thermohydraulics termohidr\u00e1ulica thermohydraulique Thermohydraulik termo-idraulica 366 thermodynamics termodin\u00e1mica thermodynamique Thermodynamik termodinamica 367 textiles tejidos tissus Stoff tessuti 368 target-shooting tiro al blanco tir \u00e0 la cible Scheibenschie\u00dfen tiro a segno 369 skeet shooting tiro al plato tir au pigeon Tontaubenschie\u00dfen tiro al piattello 370 topography topograf\u00eda topographie Topographie topografia 371 transportation transportes transports Transporte trasporti 372 trekking senderismo trekking Trekking trekking 373 diving saltos plongeon Wasserspringen tuffi 374 tourism turismo tourisme Tourismus turismo 375 crochet ganchillo crochet H\u00e4kelei uncinetto 376 university universidad universit\u00e9 Universit\u00e4t universit\u00e0 377 city planning urbanismo urbanisme Stadtplanung urbanistica 378 sailing vela voile Segeln vela 379 veterinary science veterinaria m\u00e9decine v\u00e9t\u00e9rinaire Tiermedizin veterinaria 380 glassware vidrer\u00eda verrerie Glasgeschirr vetreria 381 video games videojuegos jeux vid\u00e9o Videospiele videogiochi 382 wine industry viticultura viticulture Weinanbau viticoltura 383 volcanology vulcanolog\u00eda volcanologie Vulkanologie vulcanologia 384 windsurfing windsurf planche \u00e0 voile Windsurf windsurf 385 zoology zoolog\u00eda zoologie Zoologie zoologia 386 zootechnics zootecnia zootechnie Tierzucht zootecnia 387 bureaucratic terminology t\u00e9rminos burocr\u00e1ticos termes bureaucratiques b\u00fcrokratische Begriffe termini burocratici 388 scientific terms t\u00e9rminos cient\u00edficos termes scientifiques wissenschaftliche Begriffe termini scientifici 389 technical terminology t\u00e9rminos t\u00e9cnicos termes techniques Fachbegriffe termini tecnici 390 cosmetic industry industria cosm\u00e9tica industrie de la cosm\u00e9tique Kosmetikindustrie industria cosmetica 391 gymnastics gimnasia gymnastique Gymnastik ginnastica 392 rhythmic gymnastics gimnasia r\u00edtmica gymnastique rythmique Calisthenics ginnastica ritmica 393 artistic gymnastics gimnasia art\u00edstica gymnastique sportive Kunstturnen ginnastica artistica 394 lawn bowls juego de las bochas jeu de boules Kugelsport gioco delle bocce 395 checkers damas dames Damespiel dama 396 science fiction ciencia ficci\u00f3n science-fiction Science-Fiction fantascienza 397 accounting contabilidad comptabilit\u00e9 Buchhaltung contabilit\u00e0 398 marine biology biolog\u00eda marina biologie marine Meeresbiologie biologia marina 399 parachuting paracaidismo parachutisme Fallschirmspringen paracadutismo 400 gambling juego de azar jeux de hasard Gl\u00fccksspiel gioco d'azzardo 401 karate k\u00e1rate karat\u00e9 Karate karate 402 typewriting dactilograf\u00eda dactylographie Maschinenschreiben dattilografia 403 shorthand estenograf\u00eda st\u00e9nographie Kurzschrift stenografia 404 Hinduism hinduismo hindouisme Hinduismus induismo 405 polo polo polo Polo polo 406 pornography pornograf\u00eda pornographie Pornographie pornografia","title":"Standard context topics"},{"location":"release-notes/","text":"Release notes This page documents updates to the expert.ai Natural Language API. July 26, 2021 Version 2.3 released. This version introduces: Temporal information detection for English, Spanish, French, German and Italian. Support for the German language in emotional traits and behavioral traits document classification. This new version is backward compatible with v2. Version 1 has been discontinued. May 5, 2021 Version 2.2 released. This version introduces new extensions: Document classification: new emotional traits and behavioral traits taxonomies for English. Information detection: new Writeprint detector for English, Spanish, French, German and Italian. This version is backward compatible with v2. March 15, 2021 Version 2.1 released. This version extends the API introducing information detection capabilities with the Personally Identifiable Information (PII) detector . This version of the API is backward compatible with v2. Version 1 is now deprecated. October 30, 2020 Natural Language API v2 released! It introduces: Relation extraction capability: extracting relationships between verbs and other terms in a sentence to answer questions about the target and actors of actions and events. Sentiment analysis capability: analyzing the overall sentiment of a document based on the sentiment and relevance of associated terms. Entity attributes (e.g. George Washington > President > United States of America) in named entity recognition. Geographic taxonomy Revision of the self-documentation section of the API, both for contexts and taxonomies , new resources documenting taxonomy categories' trees Lemmas for main concepts are now provided by keyphrase extraction. Version 2 RESTful interface is documented in the OpenAPI documents . Version 1 remains available for now but will be deprecated soon. July 27, 2020 Expert.ai Natural Language API is born!","title":"Release notes"},{"location":"release-notes/#release-notes","text":"This page documents updates to the expert.ai Natural Language API.","title":"Release notes"},{"location":"release-notes/#july-26-2021","text":"Version 2.3 released. This version introduces: Temporal information detection for English, Spanish, French, German and Italian. Support for the German language in emotional traits and behavioral traits document classification. This new version is backward compatible with v2. Version 1 has been discontinued.","title":"July 26, 2021"},{"location":"release-notes/#may-5-2021","text":"Version 2.2 released. This version introduces new extensions: Document classification: new emotional traits and behavioral traits taxonomies for English. Information detection: new Writeprint detector for English, Spanish, French, German and Italian. This version is backward compatible with v2.","title":"May 5, 2021"},{"location":"release-notes/#march-15-2021","text":"Version 2.1 released. This version extends the API introducing information detection capabilities with the Personally Identifiable Information (PII) detector . This version of the API is backward compatible with v2. Version 1 is now deprecated.","title":"March 15, 2021"},{"location":"release-notes/#october-30-2020","text":"Natural Language API v2 released! It introduces: Relation extraction capability: extracting relationships between verbs and other terms in a sentence to answer questions about the target and actors of actions and events. Sentiment analysis capability: analyzing the overall sentiment of a document based on the sentiment and relevance of associated terms. Entity attributes (e.g. George Washington > President > United States of America) in named entity recognition. Geographic taxonomy Revision of the self-documentation section of the API, both for contexts and taxonomies , new resources documenting taxonomy categories' trees Lemmas for main concepts are now provided by keyphrase extraction. Version 2 RESTful interface is documented in the OpenAPI documents . Version 1 remains available for now but will be deprecated soon.","title":"October 30, 2020"},{"location":"release-notes/#july-27-2020","text":"Expert.ai Natural Language API is born!","title":"July 27, 2020"}]}
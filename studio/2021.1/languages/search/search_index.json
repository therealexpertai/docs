{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"expert.ai languages The languages Welcome to the expert.ai languages reference guide. Expert.ai languages are the programming languages used to write the \"source code\" for a unique type of software: text intelligence engines . Text intelligence engines can be programmed to perform automatic categorization , automatic extraction or both. There are two expert.ai languages: rules language. scripting language. The core of text intelligence engines is written in the rules language, while scripting is optional and used to control and extend the process workflow. The rules language is a declarative language, because it doesn't implement algorithms in explicit steps. Essentially, the source code is a set of rules . Categorization Document classification is the activity that a human mind or a computer program performs whenever a decision needs to be taken or a task needs to be executed based on the document type or on the topics covered by the document. For example, in order to give a proper answer to a customer's request, the type of request must be determined. Text intelligence engines classify documents with a process called categorization , that strives to determine what a document/text 1 is about . The possible categories are called domains and the entire set of domains the engine is able to recognize is called taxonomy . The core of the categorization process is to compare categorization rules with the text. Each rule is made of a condition and a domain: if the text satisfies the rule's condition, the domain will receive a certain amount of scoring points . At the end of the process, the domains with the received points constitute the categorization outcome. An example of categorization by comparison is the recognition of military aircraft during the Second World War. To determine whether the planes were friends or foes, military personnel had playing cards, tables and posters showing the silhouettes of military planes. Comparing the shapes in their cards with those of the airplanes flying over them, they could \"categorize\" airplanes as either \"good\" or \"bad\" and, in the latter case, raise the alarm. U.S. Air Force photo by Ken LaRock, VIRIN: 170921-F-IO108-001.JPG Extraction Information extraction is the activity that a human mind or a computer program performs to detect needed data in a document. For example, to correctly associate a customer's request with the customer's record, the customer's identification data like first name, last name or customer code must be extracted from the request. Text intelligence engines perform information extraction to retrieve useful data out of a document/text by comparing extraction rules with the text. As for categorization rules, extraction rules contain a condition, but instead of a domain, they are associated with a data template : if the text satisfies a rule's condition, the text that matches the condition as a whole or parts of it will be transferred\u2014verbatim or after a value normalization\u2014in template fields that together constitute an extraction record . At the end of the process, records constitute the extraction outcome. Rules As anticipated above, a rule is a combination of a condition and an action. The action is performed whenever the condition is met: in categorization the action is: \"increase the score of the associated domain by N points\", while in extraction it is: \"fill data template fields with the text matched by the conditions or by the sub-conditions\". A condition can be equated to a rigid or elastic shape . The disambiguator\u2014the text analysis module at the heart of expert.ai technology\u2014transforms the input text into a sequence of tokens , each enriched with all the attributes that the disambiguator has identified during its analyses. The text intelligence engine takes each defined rule and \"superimposes\" its condition (the aforementioned \"shape\") to the token stream. Whenever the \"shape\" fits perfectly a portion of the stream or, less commonly, the entire stream, the condition is satisfied and the rule's action is performed. It can be said that the input tokens trigger or activate the rule. A text intelligence engine's source code can thus be considered as a collection of possibly overlapping shapes which capture clues topics clues or interesting data. Like the World War II \"spotter cards\" mentioned above, or swatches used to match paint colors, the larger the assortment, the greater the ability to recognize interesting cases. It is important, however, to properly size the assortment so that time and money are not wasted on \"silhouettes\" or \"colors\" (shapes, rules) which are impertinent to the needs of the project. The condition of a rule can also be seen as a \"text template\" or \"text model\": if the template/model matches the input text, then the rule will be activated. In general, there is no explicit order of evaluation of the rules of the same type; it is as if all of the defined rules got evaluated simultaneously. The only exceptions to this principle are sub-rules , which, if defined, are necessarily evaluated before rules. Other types of rules and the analysis pipeline A programmer can also define tagging and segmentation rules. Rules of these types do not categorize or extract, but their results\u2014tags and segments\u2014can be exploited in categorization and extraction rules. The rules evaluation order is: Tagging rules Segmentation rules Categorization & extraction rules Rules evaluation is the core of the analysis pipeline (see the picture below) that the text intelligence engine executes for every input document. Rules language features and this book There are language constructs which are specific to the categorization activity and there are those which are specific to extraction activity. However, the rules language also has many features which are common to both activities. This book has been divided into sections according to these commonalities and peculiarities. Segmentation rules are dealt together with the common features, while a specific section is dedicated to tagging rules. Scripting language A script allows the programmer to control and extend the text intelligence engine document analysis pipeline, because it can perform powerful actions after crucial steps of the process, from the preparation of input text to the finalization of results. The expert.ai scripting language conforms to the Standard ECMA-262 5.1 Language Specification so it can be defined as \"JavaScript like\" and every JavaScript programmer can easily use it, but it's not JavaScript and has nothing to do with Web browsers and Internet connections. An entire section of this book is dedicated to scripting. Throughout this guide the terms \"document\" and \"text\" are used interchangeably because document is intended as \"the document's text\". In light of this interpretation, a document can be: Any plain text file. The text of a \"textual\" PDF file. The text taken from an image like a scanned document or from a \"visual\" PDF file via OCR. The caption of a photograph. The transcript of a phone call. A line from a chat. The text of an e-mail message. A \"tweet\". A post in a forum or a comment to a post. The value of a field in a screen form. ... in summary: any string of characters from any source \u21a9","title":"expert.ai languages"},{"location":"#expertai-languages","text":"","title":"expert.ai languages"},{"location":"#the-languages","text":"Welcome to the expert.ai languages reference guide. Expert.ai languages are the programming languages used to write the \"source code\" for a unique type of software: text intelligence engines . Text intelligence engines can be programmed to perform automatic categorization , automatic extraction or both. There are two expert.ai languages: rules language. scripting language. The core of text intelligence engines is written in the rules language, while scripting is optional and used to control and extend the process workflow. The rules language is a declarative language, because it doesn't implement algorithms in explicit steps. Essentially, the source code is a set of rules .","title":"The languages"},{"location":"#categorization","text":"Document classification is the activity that a human mind or a computer program performs whenever a decision needs to be taken or a task needs to be executed based on the document type or on the topics covered by the document. For example, in order to give a proper answer to a customer's request, the type of request must be determined. Text intelligence engines classify documents with a process called categorization , that strives to determine what a document/text 1 is about . The possible categories are called domains and the entire set of domains the engine is able to recognize is called taxonomy . The core of the categorization process is to compare categorization rules with the text. Each rule is made of a condition and a domain: if the text satisfies the rule's condition, the domain will receive a certain amount of scoring points . At the end of the process, the domains with the received points constitute the categorization outcome. An example of categorization by comparison is the recognition of military aircraft during the Second World War. To determine whether the planes were friends or foes, military personnel had playing cards, tables and posters showing the silhouettes of military planes. Comparing the shapes in their cards with those of the airplanes flying over them, they could \"categorize\" airplanes as either \"good\" or \"bad\" and, in the latter case, raise the alarm. U.S. Air Force photo by Ken LaRock, VIRIN: 170921-F-IO108-001.JPG","title":"Categorization"},{"location":"#extraction","text":"Information extraction is the activity that a human mind or a computer program performs to detect needed data in a document. For example, to correctly associate a customer's request with the customer's record, the customer's identification data like first name, last name or customer code must be extracted from the request. Text intelligence engines perform information extraction to retrieve useful data out of a document/text by comparing extraction rules with the text. As for categorization rules, extraction rules contain a condition, but instead of a domain, they are associated with a data template : if the text satisfies a rule's condition, the text that matches the condition as a whole or parts of it will be transferred\u2014verbatim or after a value normalization\u2014in template fields that together constitute an extraction record . At the end of the process, records constitute the extraction outcome.","title":"Extraction"},{"location":"#rules","text":"As anticipated above, a rule is a combination of a condition and an action. The action is performed whenever the condition is met: in categorization the action is: \"increase the score of the associated domain by N points\", while in extraction it is: \"fill data template fields with the text matched by the conditions or by the sub-conditions\". A condition can be equated to a rigid or elastic shape . The disambiguator\u2014the text analysis module at the heart of expert.ai technology\u2014transforms the input text into a sequence of tokens , each enriched with all the attributes that the disambiguator has identified during its analyses. The text intelligence engine takes each defined rule and \"superimposes\" its condition (the aforementioned \"shape\") to the token stream. Whenever the \"shape\" fits perfectly a portion of the stream or, less commonly, the entire stream, the condition is satisfied and the rule's action is performed. It can be said that the input tokens trigger or activate the rule. A text intelligence engine's source code can thus be considered as a collection of possibly overlapping shapes which capture clues topics clues or interesting data. Like the World War II \"spotter cards\" mentioned above, or swatches used to match paint colors, the larger the assortment, the greater the ability to recognize interesting cases. It is important, however, to properly size the assortment so that time and money are not wasted on \"silhouettes\" or \"colors\" (shapes, rules) which are impertinent to the needs of the project. The condition of a rule can also be seen as a \"text template\" or \"text model\": if the template/model matches the input text, then the rule will be activated. In general, there is no explicit order of evaluation of the rules of the same type; it is as if all of the defined rules got evaluated simultaneously. The only exceptions to this principle are sub-rules , which, if defined, are necessarily evaluated before rules.","title":"Rules"},{"location":"#other-types-of-rules-and-the-analysis-pipeline","text":"A programmer can also define tagging and segmentation rules. Rules of these types do not categorize or extract, but their results\u2014tags and segments\u2014can be exploited in categorization and extraction rules. The rules evaluation order is: Tagging rules Segmentation rules Categorization & extraction rules Rules evaluation is the core of the analysis pipeline (see the picture below) that the text intelligence engine executes for every input document.","title":"Other types of rules and the analysis pipeline"},{"location":"#rules-language-features-and-this-book","text":"There are language constructs which are specific to the categorization activity and there are those which are specific to extraction activity. However, the rules language also has many features which are common to both activities. This book has been divided into sections according to these commonalities and peculiarities. Segmentation rules are dealt together with the common features, while a specific section is dedicated to tagging rules.","title":"Rules language features and this book"},{"location":"#scripting-language","text":"A script allows the programmer to control and extend the text intelligence engine document analysis pipeline, because it can perform powerful actions after crucial steps of the process, from the preparation of input text to the finalization of results. The expert.ai scripting language conforms to the Standard ECMA-262 5.1 Language Specification so it can be defined as \"JavaScript like\" and every JavaScript programmer can easily use it, but it's not JavaScript and has nothing to do with Web browsers and Internet connections. An entire section of this book is dedicated to scripting. Throughout this guide the terms \"document\" and \"text\" are used interchangeably because document is intended as \"the document's text\". In light of this interpretation, a document can be: Any plain text file. The text of a \"textual\" PDF file. The text taken from an image like a scanned document or from a \"visual\" PDF file via OCR. The caption of a photograph. The transcript of a phone call. A line from a chat. The text of an e-mail message. A \"tweet\". A post in a forum or a comment to a post. The value of a field in a screen form. ... in summary: any string of characters from any source \u21a9","title":"Scripting language"},{"location":"attributes/","text":"Attributes overview Attributes are the building blocks of categorization and extraction rules. They are operands which are used to match disambiguation output tokens based on the attributes they possess. The syntax of a generic attribute is: attribute_name(value1[, value2, ...]) where attribute_name can be one of the possible attributes listed below. value# refers to the parameter taken by the attribute. For each attribute, it is possible to specify more than one value. This table lists all possible attributes along with their values and a short description. For further details, please see the individual attribute sections. Attribute Values type Description KEYWORD String Matches any token that is exactly equal to the given strings LEMMA Lemma Matches any token that is a possible inflection of a given lemma contained in the Knowledge Graph BLEMMA Lemma Similar to LEMMA , but a match is performed at the sub-token or \"atom\" level of the text SYNCON Syncon Matches any token which corresponds to a given concept (syncon) contained in the Knowledge Graph ANCESTOR Syncon Matches every token which corresponds to a \"descendant concept\" of a given concept contained in the Knowledge Graph LIST Syncon Matches every token corresponding to any lemma of the given Knowledge Graph concepts BLIST Syncon Similar to LIST , but matches are performed at the \"atom level\" of the text analysis output TYPE Type Matches any token of the given types PATTERN Regular expression(s) Matches any token matching the given regular expressions ROLE Role Matches any token having one of the given roles in the sentence analysis of the text POSITION Position Matches any token occupying one of the given positions RELEVANT List Matches any token being in one of the given lists of relevant text elements TAG Tag Matches any token corresponding to one of the given tags As an option, values can be loaded from an external list.","title":"Attributes overview"},{"location":"attributes/#attributes-overview","text":"Attributes are the building blocks of categorization and extraction rules. They are operands which are used to match disambiguation output tokens based on the attributes they possess. The syntax of a generic attribute is: attribute_name(value1[, value2, ...]) where attribute_name can be one of the possible attributes listed below. value# refers to the parameter taken by the attribute. For each attribute, it is possible to specify more than one value. This table lists all possible attributes along with their values and a short description. For further details, please see the individual attribute sections. Attribute Values type Description KEYWORD String Matches any token that is exactly equal to the given strings LEMMA Lemma Matches any token that is a possible inflection of a given lemma contained in the Knowledge Graph BLEMMA Lemma Similar to LEMMA , but a match is performed at the sub-token or \"atom\" level of the text SYNCON Syncon Matches any token which corresponds to a given concept (syncon) contained in the Knowledge Graph ANCESTOR Syncon Matches every token which corresponds to a \"descendant concept\" of a given concept contained in the Knowledge Graph LIST Syncon Matches every token corresponding to any lemma of the given Knowledge Graph concepts BLIST Syncon Similar to LIST , but matches are performed at the \"atom level\" of the text analysis output TYPE Type Matches any token of the given types PATTERN Regular expression(s) Matches any token matching the given regular expressions ROLE Role Matches any token having one of the given roles in the sentence analysis of the text POSITION Position Matches any token occupying one of the given positions RELEVANT List Matches any token being in one of the given lists of relevant text elements TAG Tag Matches any token corresponding to one of the given tags As an option, values can be loaded from an external list.","title":"Attributes overview"},{"location":"attributes/ancestor/","text":"ANCESTOR attribute Overview The ANCESTOR attribute identifies a chain of concepts by specifying the numeric ID of a syncon and considering it as the starting point of the chain. A token will be recognized in a text if, during the disambiguation process, it is associated with one of the syncons in the selected chain. The syntax is: ANCESTOR(id1[, id2, ...]) ANCESTOR must be written in uppercase. id# refers to the unique ID assigned to each and every syncon contained in the Knowledge Graph. It is always a whole number made up of one or several digits. As with the SYNCON attribute, the ANCESTOR attribute allows you to specify the ID of a concept (syncon) contained in the Knowledge Graph. However, where SYNCON considers the specified syncon ID as the only concept to be considered, ANCESTOR considers the ID as the starting point for a chain of concepts. In fact, syncons in the Knowledge Graph are linked to each other through semantic relations called links. When using the ANCESTOR attribute, the selected chain is navigated downwards (never upwards) following the branch structure of the chain from the most general (the syncon ID specified in the rule) to the most specific (the last selected node of the chain) concept. Since a chain consists of several levels, it is possible to limit the number of levels to be navigated. This is done by adding a colon (:) after the syncon ID, followed by the number of levels. ANCESTOR (ID1:level number, ID2:level number, ...) Levels range from 0 to 99, where 0 is the starting point of the chain which only considers the first level and 99 is the default value that considers all levels. If no value is specified, the whole chain is considered (same as declaring level 99). It is also possible to specify the link to be navigated when looking for descendants. This can be done by adding another colon (:) after the level number followed by the name of the link. ANCESTOR (ID1:level number:linkname, ID2:level number:linkname, ...) Valid links are those available in the Knowledge Graph, including any custom link added for a specific project. If no link name is specified and the given ancestor is a noun, the supernomen/subnomen (\"part of\" type of relation) link will be navigated by default. If no link name is specified and the given ancestor is a verb, the superverbum/subverbum (\"way of \" type of relation) link will be navigated by default. Any other links must be specified in order to be considered in the rule. When the ANCESTOR attribute is used in a rule, a token is identified in a document only if it is disambiguated as an instance of one of the syncons that is part of the chain. A syncon can be made up of one or more lemmas representing the same concept (synonyms), therefore all synonyms, variants, abbreviations etc. that are part of the same syncon will be matched if found in a document. Synonyms and variants are also recognized both in their base form and inflected forms. Please be aware that a token in the document that matches one of the lemmas of one of the syncons in the chain is not a sufficient condition for the token to match the ANCESTOR rule. In fact, many lemmas contained in the Knowledge Graph are polysemic, which means that they have several meanings or they represent different concepts. For an ancestor rule to be verified, the disambiguator must associate the token in the text to one of the meanings represented by the syncons chain used in the rules. This means that the ANCESTOR attribute not only considers the form of a word but also its contextual meaning. Consider the following examples: ANCESTOR (17200) The above rule starts at the concept of house meaning a building for human habitation and includes all concepts below it. Since neither a level number nor a link name was specified, this rule considers syncon 17200 and all of its descendants in the supernomen/subnomen chain. Therefore, it will recognize not only the concept of house , but also the concepts for different types of houses. For example, the first level of the supernomen/subnomen chain starting from syncon 17200 contains concepts for apartment building , detached house , duplex house , townhouse etc. Further down the chain, apartment building contains second level concepts such as apartment block and cooperative . Instead, in this example: ANCESTOR (78449:2) syncon 78449 refers to plant with the meaning of any living organism of vegetable origin . The above rule considers only the supernomen/subnomen link up to the second level of descendants. In other words, the concept of plant is recognized in a document along with its direct children ( flower , tree , bush , etc.) which are found in the first level of the hierarchy. The type of flowers and trees are also recognized ( tulip , orchid , shade tree , high-trunk tree , etc.) because they are found in the second level of the hierarchy. The types of roses however are not be considered since they are in the third level of the hierarchy descending from plant. Going further into syntax complexity, consider the following statement: ANCESTOR (12622858:99:syncon/geography) Syncon 12622858 refers to United Kingdom and this statement considers all levels down the chain following the syncon/geography link (99 is the default level number that indicates maximum depth in a concept chain). In other words, the chain linking United Kingdom along with all the administrative places associated with this country (constituent nations like Wales , counties like Derbyshire , cities like Manchester etc.) will be matched if found in a document. Matching a virtual supernomen As a standard, the ANCESTOR attribute will recognize in a text one or several words contained in the Knowledge Graph when the syncon ID of one of their ancestors (father, grandfather, great-grandfather etc.) is specified. However, certain unknown elements can also be matched by rules using the ANCESTOR attribute. In fact, the disambiguator is able to apply a heuristic approach when faced with unknown elements, and guess from the context if an unknown entity can be virtually linked to a concept in the Knowledge Graph. In other words, when the disambiguator comes across an unknown element in a text, it uses the known words surrounding the unknown element to assign it to a virtual supernomen . A typical example of unknown elements which are elevated to the rank of \"entities\" are units of measurements, such as meters. In fact, all possible values indicating a length measure are not contained in the Knowledge Graph; yet it is possible to specify the syncon ID for the concept of \"meter\": ANCESTOR (100011573) to match in a text any value relating to meters, whether it is contained in the Knowledge Graph or not. This is possible because the disambiguator is able to connect an unknown entity to a known concept thus creating a virtual kinship. In the same way, in the following sentence: [..] It is a type of pattern seen in the tiled Islamic mosaics at the Alhambra Palace in Spain and the Darb-i Imam shrine in Iran, but which had never been thought could exist in nature. the disambiguator recognizes that Darb-i Imam is the proper name of a shrine and will be therefore connected to the syncon ID 20444 referring to the concept shrine , which becomes its virtual supernomen. Another example of an unknown entity being linked to a virtual supernomen is in the following sentence: Shechtman was born in Tel Aviv in 1941 and received his PhD from Technion, the Israel Institute of Technology in Haifa, in 1972. Here, Israel Institute of Technology is recognized as a virtual son of the concept of Institute intended as an educational institution (syncon ID 148106). This process can be potentially applied to any unknown element in the text, however the correctness of the output will always be strictly related to the quality and quantity of the contextual information available to the disambiguator. With UNKNOWN It is also possible to use the ANCESTOR attribute with the UNKNOWN value in the place of a syncon ID, as shown below. ANCESTOR (UNKNOWN) In this case, only the elements with no virtual supernomens will be matched in a text. This means that only elements which are not contained in the Knowledge Graph and for which the disambiguator was not able to assign a supernomen will be taken into consideration. The disambiguator searches for the virtual fathers by navigating the supernomen/subnomen link, therefore when the UNKNOWN value is used no other links or levels can be specified. Warning If used by itself, the UNKNOWN value, can be extremely powerful and hyper generative. In fact, it is designed to be used in combination with other attributes. Double link ancestor Another feature regarding the use of the ANCESTOR attribute is the possibility to specify a second link to be navigated. This can be performed by adding another colon (:) after the first link name and then adding the name of the second link. ANCESTOR(ID:levels:linkname:linkname) This syntax performs a complex task of concept identification, the process is comprised of the following steps: Navigate the first type of chain starting from the specified ancestor ID. In the first chain, identify all syncons that are also part of the second type of chain. Navigate the second chain starting from each of the identified syncons. In the text, identify the syncons that are found in any of the chains belonging to the second type of link and starting from any of the syncons found in the first chain. Consider the following sample text, which contains information about natural geographic features related to the United Kingdom: England is a country that is part of the United Kingdom. It shares land borders with Scotland to the north and Wales to the west; the Irish Sea is to the north west, the Celtic Sea to the south west, while the North Sea to the east and the English Channel to the south separate it from continental Europe. Most of England comprises the central and southern part of the island of Great Britain in the North Atlantic. The country also includes over 100 smaller islands such as the Isle of Sheppey and the Isle of Wight. For demonstration purposes, the goal is to categorize any document against a taxonomy of Countries identifying any geographic concept mentioned in the text. Using the \"double link\" ancestor syntax presented above, it is possible to obtain the result. This categorization rule: SCOPE SENTENCE { //United Kingdom: natural elements DOMAIN(dom1:NORMAL) { ANCESTOR(12622858:99:syncon/geography:omninomen/parsnomen)//United Kingdom } } is triggered by the sample text because four concepts related to the Country are found in the text. They are natural elements that are part of the United Kingdom: Irish Sea , English Channel , Isle of Sheppey and Isle of Wight . Starting from the ancestor syncon ID which corresponds to the concept of United Kingdom (12622858, down 99 is the default to indicate maximum depth into the concepts chain), the syncon/geography link navigates down the chain of concepts and links all the administrative places in which the Country is organized (constituent nations, counties, cities...). The second link, omninomen/parsnomen (\"part of\" type of relation), identifies in the text all the natural elements linked to every administrative place which belongs to the first chain. In the sample text above, some of the natural elements are linked to the UK itself (the starting point of the chain) while others are linked to England (child of UK in the administrative chain). In other words, this syntax identifies all the natural elements associated to the territory of the Country using just a single rule. This is just one of the possible applications of this syntax. Another example is to use the supernomen/subnomen link in combination with the omninomen/parsnomen link to identify the components of different types of motor vehicles. Let's consider the rule below: SCOPE SENTENCE { // Motor vehicle part DOMAIN(dom1:NORMAL) { ANCESTOR(78327:99:supernomen/subnomen:omninomen/parsnomen)//motor vehicle } } and a new sample text: Almost all trucks share a common construction: they are made of a chassis, a cab, an area for placing cargo or equipment, axles, suspension and roadwheels, an engine and a drivetrain. Pneumatic, hydraulic, water, and electrical systems may also be identified. Many also tow one or more trailers or semi-trailers. Given this text, our rule would recognize not only the concept of chassis , which is a component of any motor vehicle (therefore chassis has a direct omninomen/parsnomen link with motor vehicle ), but also the concept of trailer , which is a component of a trailer truck, which in turn is a type of motor vehicle ( trailer has a direct omninomen/parsnomen link with trailer truck , which has a supernomen/subnomen link with truck and, higher in the chain, with motor vehicle ). Using the - filter When navigating two links of an ANCESTOR chain it is very important to have a clear idea on how concepts are represented inside the Knowledge Graph. For example, syncon/geography is a relation between proper nouns expressing geographical inclusion, the most inclusive concept sits at the top of the chain while all other concepts will be listed hierarchally downwards. The same happens for adjective/geography, where the starting point of the chain is a geographical adjective and its child will be the country to which the adjective is related. Using the double-link syntax,a rule can match on geographical adjectives just by specifying the geographic place and the two links to be navigated, as in the rule below: SCOPE SENTENCE { //adjective/geography DOMAIN(ITALY) { ANCESTOR(100000046:1:syncon/geography:adjective/geography) //@SYN: #100000046# [Europe] } } Unfortunately, since we can only navigate downward, we will have matches for the first link\u2014geographical places\u2014but we won't be able to have matches on adjectives because the downward navigation does not suit the way in which in geographical places and adjectives are arranged with the Knowledge Graph. In order to have a match we need to change the navigation direction for the adjective/geography link from downward to upward. This is possible by adding - before the link for which we want to change the navigation direction. Consider the rule below: SCOPE SENTENCE { //adjective/geography DOMAIN(ITALY) { ANCESTOR(100000046:1:syncon/geography:-adjective/geography) //@SYN: #100000046# [Europe] } } The starting point of the chain in the rule is the concept Europe . The chain is navigated downward through the link syncon/geography, allowing matches on all countries in Europe . By adding - before the second link adjective/geography, the link will be navigated upward and, in this case it can be a match on the adjectives linked to the country.","title":"ANCESTOR attribute"},{"location":"attributes/ancestor/#ancestor-attribute","text":"","title":"ANCESTOR attribute"},{"location":"attributes/ancestor/#overview","text":"The ANCESTOR attribute identifies a chain of concepts by specifying the numeric ID of a syncon and considering it as the starting point of the chain. A token will be recognized in a text if, during the disambiguation process, it is associated with one of the syncons in the selected chain. The syntax is: ANCESTOR(id1[, id2, ...]) ANCESTOR must be written in uppercase. id# refers to the unique ID assigned to each and every syncon contained in the Knowledge Graph. It is always a whole number made up of one or several digits. As with the SYNCON attribute, the ANCESTOR attribute allows you to specify the ID of a concept (syncon) contained in the Knowledge Graph. However, where SYNCON considers the specified syncon ID as the only concept to be considered, ANCESTOR considers the ID as the starting point for a chain of concepts. In fact, syncons in the Knowledge Graph are linked to each other through semantic relations called links. When using the ANCESTOR attribute, the selected chain is navigated downwards (never upwards) following the branch structure of the chain from the most general (the syncon ID specified in the rule) to the most specific (the last selected node of the chain) concept. Since a chain consists of several levels, it is possible to limit the number of levels to be navigated. This is done by adding a colon (:) after the syncon ID, followed by the number of levels. ANCESTOR (ID1:level number, ID2:level number, ...) Levels range from 0 to 99, where 0 is the starting point of the chain which only considers the first level and 99 is the default value that considers all levels. If no value is specified, the whole chain is considered (same as declaring level 99). It is also possible to specify the link to be navigated when looking for descendants. This can be done by adding another colon (:) after the level number followed by the name of the link. ANCESTOR (ID1:level number:linkname, ID2:level number:linkname, ...) Valid links are those available in the Knowledge Graph, including any custom link added for a specific project. If no link name is specified and the given ancestor is a noun, the supernomen/subnomen (\"part of\" type of relation) link will be navigated by default. If no link name is specified and the given ancestor is a verb, the superverbum/subverbum (\"way of \" type of relation) link will be navigated by default. Any other links must be specified in order to be considered in the rule. When the ANCESTOR attribute is used in a rule, a token is identified in a document only if it is disambiguated as an instance of one of the syncons that is part of the chain. A syncon can be made up of one or more lemmas representing the same concept (synonyms), therefore all synonyms, variants, abbreviations etc. that are part of the same syncon will be matched if found in a document. Synonyms and variants are also recognized both in their base form and inflected forms. Please be aware that a token in the document that matches one of the lemmas of one of the syncons in the chain is not a sufficient condition for the token to match the ANCESTOR rule. In fact, many lemmas contained in the Knowledge Graph are polysemic, which means that they have several meanings or they represent different concepts. For an ancestor rule to be verified, the disambiguator must associate the token in the text to one of the meanings represented by the syncons chain used in the rules. This means that the ANCESTOR attribute not only considers the form of a word but also its contextual meaning. Consider the following examples: ANCESTOR (17200) The above rule starts at the concept of house meaning a building for human habitation and includes all concepts below it. Since neither a level number nor a link name was specified, this rule considers syncon 17200 and all of its descendants in the supernomen/subnomen chain. Therefore, it will recognize not only the concept of house , but also the concepts for different types of houses. For example, the first level of the supernomen/subnomen chain starting from syncon 17200 contains concepts for apartment building , detached house , duplex house , townhouse etc. Further down the chain, apartment building contains second level concepts such as apartment block and cooperative . Instead, in this example: ANCESTOR (78449:2) syncon 78449 refers to plant with the meaning of any living organism of vegetable origin . The above rule considers only the supernomen/subnomen link up to the second level of descendants. In other words, the concept of plant is recognized in a document along with its direct children ( flower , tree , bush , etc.) which are found in the first level of the hierarchy. The type of flowers and trees are also recognized ( tulip , orchid , shade tree , high-trunk tree , etc.) because they are found in the second level of the hierarchy. The types of roses however are not be considered since they are in the third level of the hierarchy descending from plant. Going further into syntax complexity, consider the following statement: ANCESTOR (12622858:99:syncon/geography) Syncon 12622858 refers to United Kingdom and this statement considers all levels down the chain following the syncon/geography link (99 is the default level number that indicates maximum depth in a concept chain). In other words, the chain linking United Kingdom along with all the administrative places associated with this country (constituent nations like Wales , counties like Derbyshire , cities like Manchester etc.) will be matched if found in a document.","title":"Overview"},{"location":"attributes/ancestor/#matching-a-virtual-supernomen","text":"As a standard, the ANCESTOR attribute will recognize in a text one or several words contained in the Knowledge Graph when the syncon ID of one of their ancestors (father, grandfather, great-grandfather etc.) is specified. However, certain unknown elements can also be matched by rules using the ANCESTOR attribute. In fact, the disambiguator is able to apply a heuristic approach when faced with unknown elements, and guess from the context if an unknown entity can be virtually linked to a concept in the Knowledge Graph. In other words, when the disambiguator comes across an unknown element in a text, it uses the known words surrounding the unknown element to assign it to a virtual supernomen . A typical example of unknown elements which are elevated to the rank of \"entities\" are units of measurements, such as meters. In fact, all possible values indicating a length measure are not contained in the Knowledge Graph; yet it is possible to specify the syncon ID for the concept of \"meter\": ANCESTOR (100011573) to match in a text any value relating to meters, whether it is contained in the Knowledge Graph or not. This is possible because the disambiguator is able to connect an unknown entity to a known concept thus creating a virtual kinship. In the same way, in the following sentence: [..] It is a type of pattern seen in the tiled Islamic mosaics at the Alhambra Palace in Spain and the Darb-i Imam shrine in Iran, but which had never been thought could exist in nature. the disambiguator recognizes that Darb-i Imam is the proper name of a shrine and will be therefore connected to the syncon ID 20444 referring to the concept shrine , which becomes its virtual supernomen. Another example of an unknown entity being linked to a virtual supernomen is in the following sentence: Shechtman was born in Tel Aviv in 1941 and received his PhD from Technion, the Israel Institute of Technology in Haifa, in 1972. Here, Israel Institute of Technology is recognized as a virtual son of the concept of Institute intended as an educational institution (syncon ID 148106). This process can be potentially applied to any unknown element in the text, however the correctness of the output will always be strictly related to the quality and quantity of the contextual information available to the disambiguator.","title":"Matching a virtual supernomen"},{"location":"attributes/ancestor/#with-unknown","text":"It is also possible to use the ANCESTOR attribute with the UNKNOWN value in the place of a syncon ID, as shown below. ANCESTOR (UNKNOWN) In this case, only the elements with no virtual supernomens will be matched in a text. This means that only elements which are not contained in the Knowledge Graph and for which the disambiguator was not able to assign a supernomen will be taken into consideration. The disambiguator searches for the virtual fathers by navigating the supernomen/subnomen link, therefore when the UNKNOWN value is used no other links or levels can be specified. Warning If used by itself, the UNKNOWN value, can be extremely powerful and hyper generative. In fact, it is designed to be used in combination with other attributes.","title":"With UNKNOWN"},{"location":"attributes/ancestor/#double-link-ancestor","text":"Another feature regarding the use of the ANCESTOR attribute is the possibility to specify a second link to be navigated. This can be performed by adding another colon (:) after the first link name and then adding the name of the second link. ANCESTOR(ID:levels:linkname:linkname) This syntax performs a complex task of concept identification, the process is comprised of the following steps: Navigate the first type of chain starting from the specified ancestor ID. In the first chain, identify all syncons that are also part of the second type of chain. Navigate the second chain starting from each of the identified syncons. In the text, identify the syncons that are found in any of the chains belonging to the second type of link and starting from any of the syncons found in the first chain. Consider the following sample text, which contains information about natural geographic features related to the United Kingdom: England is a country that is part of the United Kingdom. It shares land borders with Scotland to the north and Wales to the west; the Irish Sea is to the north west, the Celtic Sea to the south west, while the North Sea to the east and the English Channel to the south separate it from continental Europe. Most of England comprises the central and southern part of the island of Great Britain in the North Atlantic. The country also includes over 100 smaller islands such as the Isle of Sheppey and the Isle of Wight. For demonstration purposes, the goal is to categorize any document against a taxonomy of Countries identifying any geographic concept mentioned in the text. Using the \"double link\" ancestor syntax presented above, it is possible to obtain the result. This categorization rule: SCOPE SENTENCE { //United Kingdom: natural elements DOMAIN(dom1:NORMAL) { ANCESTOR(12622858:99:syncon/geography:omninomen/parsnomen)//United Kingdom } } is triggered by the sample text because four concepts related to the Country are found in the text. They are natural elements that are part of the United Kingdom: Irish Sea , English Channel , Isle of Sheppey and Isle of Wight . Starting from the ancestor syncon ID which corresponds to the concept of United Kingdom (12622858, down 99 is the default to indicate maximum depth into the concepts chain), the syncon/geography link navigates down the chain of concepts and links all the administrative places in which the Country is organized (constituent nations, counties, cities...). The second link, omninomen/parsnomen (\"part of\" type of relation), identifies in the text all the natural elements linked to every administrative place which belongs to the first chain. In the sample text above, some of the natural elements are linked to the UK itself (the starting point of the chain) while others are linked to England (child of UK in the administrative chain). In other words, this syntax identifies all the natural elements associated to the territory of the Country using just a single rule. This is just one of the possible applications of this syntax. Another example is to use the supernomen/subnomen link in combination with the omninomen/parsnomen link to identify the components of different types of motor vehicles. Let's consider the rule below: SCOPE SENTENCE { // Motor vehicle part DOMAIN(dom1:NORMAL) { ANCESTOR(78327:99:supernomen/subnomen:omninomen/parsnomen)//motor vehicle } } and a new sample text: Almost all trucks share a common construction: they are made of a chassis, a cab, an area for placing cargo or equipment, axles, suspension and roadwheels, an engine and a drivetrain. Pneumatic, hydraulic, water, and electrical systems may also be identified. Many also tow one or more trailers or semi-trailers. Given this text, our rule would recognize not only the concept of chassis , which is a component of any motor vehicle (therefore chassis has a direct omninomen/parsnomen link with motor vehicle ), but also the concept of trailer , which is a component of a trailer truck, which in turn is a type of motor vehicle ( trailer has a direct omninomen/parsnomen link with trailer truck , which has a supernomen/subnomen link with truck and, higher in the chain, with motor vehicle ).","title":"Double link ancestor"},{"location":"attributes/ancestor/#using-the-filter","text":"When navigating two links of an ANCESTOR chain it is very important to have a clear idea on how concepts are represented inside the Knowledge Graph. For example, syncon/geography is a relation between proper nouns expressing geographical inclusion, the most inclusive concept sits at the top of the chain while all other concepts will be listed hierarchally downwards. The same happens for adjective/geography, where the starting point of the chain is a geographical adjective and its child will be the country to which the adjective is related. Using the double-link syntax,a rule can match on geographical adjectives just by specifying the geographic place and the two links to be navigated, as in the rule below: SCOPE SENTENCE { //adjective/geography DOMAIN(ITALY) { ANCESTOR(100000046:1:syncon/geography:adjective/geography) //@SYN: #100000046# [Europe] } } Unfortunately, since we can only navigate downward, we will have matches for the first link\u2014geographical places\u2014but we won't be able to have matches on adjectives because the downward navigation does not suit the way in which in geographical places and adjectives are arranged with the Knowledge Graph. In order to have a match we need to change the navigation direction for the adjective/geography link from downward to upward. This is possible by adding - before the link for which we want to change the navigation direction. Consider the rule below: SCOPE SENTENCE { //adjective/geography DOMAIN(ITALY) { ANCESTOR(100000046:1:syncon/geography:-adjective/geography) //@SYN: #100000046# [Europe] } } The starting point of the chain in the rule is the concept Europe . The chain is navigated downward through the link syncon/geography, allowing matches on all countries in Europe . By adding - before the second link adjective/geography, the link will be navigated upward and, in this case it can be a match on the adjectives linked to the country.","title":"Using the - filter"},{"location":"attributes/blemma/","text":"BLEMMA attribute The BLEMMA attribute is very similar to LEMMA , but it is applied at the atom level of the disambiguation output and not the word level . This allows the user to match sub-tokens ( atoms ) that, at the word level, would be part of compounds, collocations or idiomatic expressions. For example, this rule (using LEMMA ): SCOPE SENTENCE { DOMAIN(dom1) { LEMMA(\"safety\") } } when run against this text: The manufacturing plant is in compliance with the fire safety standard. doesn't find a match, because the term safety is part of the collocation lemma fire safety and is not recognized as a lemma by itself at the word level. On the other hand, this rule (using BLEMMA ): SCOPE SENTENCE { DOMAIN(dom1) { BLEMMA(\"safety\") } } matches safety at the atom level, where it is not aggregated with lemma fire .","title":"BLEMMA attribute"},{"location":"attributes/blemma/#blemma-attribute","text":"The BLEMMA attribute is very similar to LEMMA , but it is applied at the atom level of the disambiguation output and not the word level . This allows the user to match sub-tokens ( atoms ) that, at the word level, would be part of compounds, collocations or idiomatic expressions. For example, this rule (using LEMMA ): SCOPE SENTENCE { DOMAIN(dom1) { LEMMA(\"safety\") } } when run against this text: The manufacturing plant is in compliance with the fire safety standard. doesn't find a match, because the term safety is part of the collocation lemma fire safety and is not recognized as a lemma by itself at the word level. On the other hand, this rule (using BLEMMA ): SCOPE SENTENCE { DOMAIN(dom1) { BLEMMA(\"safety\") } } matches safety at the atom level, where it is not aggregated with lemma fire .","title":"BLEMMA attribute"},{"location":"attributes/blist/","text":"BLIST attribute The BLIST attribute is very similar to LIST , but is applied at the atom level of disambiguation output instead of the word level , allowing the user to match sub-tokens ( atoms ) that, at the word level, would be part of compounds, collocations or idiomatic expressions. For example, this rule: SCOPE SENTENCE { DOMAIN(dom1) { LIST(21312)//@SYN: #21312# [equipment] } } when run against this text: The fire safety system is outdated. doesn't find a match, because the term system is part of the collocation lemma fire safety system and is not recognized as a lemma by itself at the word level. On the other hand, this rule: SCOPE SENTENCE { DOMAIN(dom1) { BLIST(21312)//@SYN: #21312# [equipment] } } matches system at the atom level, where it is not aggregated with lemmas fire and safety .","title":"BLIST attribute"},{"location":"attributes/blist/#blist-attribute","text":"The BLIST attribute is very similar to LIST , but is applied at the atom level of disambiguation output instead of the word level , allowing the user to match sub-tokens ( atoms ) that, at the word level, would be part of compounds, collocations or idiomatic expressions. For example, this rule: SCOPE SENTENCE { DOMAIN(dom1) { LIST(21312)//@SYN: #21312# [equipment] } } when run against this text: The fire safety system is outdated. doesn't find a match, because the term system is part of the collocation lemma fire safety system and is not recognized as a lemma by itself at the word level. On the other hand, this rule: SCOPE SENTENCE { DOMAIN(dom1) { BLIST(21312)//@SYN: #21312# [equipment] } } matches system at the atom level, where it is not aggregated with lemmas fire and safety .","title":"BLIST attribute"},{"location":"attributes/combination/","text":"Combination of attributes All available attributes can be combined. This permits more effective and complex matches on documents compared to the use of single attributes. Attributes can be combined using the symbols + and -, which perform, respectively, the intersection and the difference between two or more attributes. The syntax is: attribute1(value1[, value2, ...]) +|- attribute2(value1[, value2, ...]) ... Given two (or more) attributes combined in a rule, a token will be matched in a document, if it satisfies the first attribute and all those preceded by the + sign, but not all those attributes preceded by the - sign. The attributes can be described as a set of values that must be matched by one or more tokens in a text in order for a rule to trigger. When two attributes are combined using the + sign, its goal is to perform an intersection between the two sets of values. When two attributes are combined using the - sign, its goal is to perform the difference between the two sets of values. Consider the following example: TYPE(NPH) + ROLE(SUBJECT) A rule containing this combination will categorize or extract the proper name of a person ( TYPE (NPH) ) only if this name is also recognized as the subject of a sentence or clause ( +ROLE (SUBJECT) ). In a sentence such as: Dale Cregan is accused of the murders of Nicola Hughes and Fiona Bone. only Dale Cregan would match the rule because it is the only NPH that is also the subject of the sentence. Nicola Hughes and Fiona Bone are also analyzed as NPHs , but their role is not the subject. In other words, the token Dale Cregan is considered the only intersection between the set of proper names and the set of subjects in a sentence. Now consider the second example: SYNCON(57720) - LEMMA(\"ck\", \"chq\") A rule containing this combination will categorize or extract the concept of bank check ( SYNCON(57720) ) but not the lemmas ck or chq which are found in the set of synonyms forming syncon 57720. These two lemmas will then be subtracted from the set of lemmas that make up the said syncon. In a sentence such as: Checks can now be cashed even after 6 months from the chq. date only the token Checks would match the given rule because it belongs to the syncon 57720 and is not one of the subtracted lemmas. The lemma chq. is recognized as a part of the syncon 57220 but the rule excludes it from the set and is therefore not generated in the match. In other words, only the token Checks is part of the set of values resulting from the difference between the SYNCON set and the LEMMA set. The following example: ROLE(SUBJECT) + TYPE(NPR) recognizes subjects which are also proper nouns ( NPR ). LEMMA(\"dog\") - KEYWORD(\"dogs\") In this example, dogs is an inflected form of the lemma dog and it is possible to subtract this keyword from the lemma set. ANCESTOR(100000729)//@SYN: #100000729# [animal] + PATTERN(\"[^A-Z]\") The example above will match animal names belonging to the lexical and conceptual chain of the ANCESTOR , only if they don't contain capital letters.","title":"Combination of attributes"},{"location":"attributes/combination/#combination-of-attributes","text":"All available attributes can be combined. This permits more effective and complex matches on documents compared to the use of single attributes. Attributes can be combined using the symbols + and -, which perform, respectively, the intersection and the difference between two or more attributes. The syntax is: attribute1(value1[, value2, ...]) +|- attribute2(value1[, value2, ...]) ... Given two (or more) attributes combined in a rule, a token will be matched in a document, if it satisfies the first attribute and all those preceded by the + sign, but not all those attributes preceded by the - sign. The attributes can be described as a set of values that must be matched by one or more tokens in a text in order for a rule to trigger. When two attributes are combined using the + sign, its goal is to perform an intersection between the two sets of values. When two attributes are combined using the - sign, its goal is to perform the difference between the two sets of values. Consider the following example: TYPE(NPH) + ROLE(SUBJECT) A rule containing this combination will categorize or extract the proper name of a person ( TYPE (NPH) ) only if this name is also recognized as the subject of a sentence or clause ( +ROLE (SUBJECT) ). In a sentence such as: Dale Cregan is accused of the murders of Nicola Hughes and Fiona Bone. only Dale Cregan would match the rule because it is the only NPH that is also the subject of the sentence. Nicola Hughes and Fiona Bone are also analyzed as NPHs , but their role is not the subject. In other words, the token Dale Cregan is considered the only intersection between the set of proper names and the set of subjects in a sentence. Now consider the second example: SYNCON(57720) - LEMMA(\"ck\", \"chq\") A rule containing this combination will categorize or extract the concept of bank check ( SYNCON(57720) ) but not the lemmas ck or chq which are found in the set of synonyms forming syncon 57720. These two lemmas will then be subtracted from the set of lemmas that make up the said syncon. In a sentence such as: Checks can now be cashed even after 6 months from the chq. date only the token Checks would match the given rule because it belongs to the syncon 57720 and is not one of the subtracted lemmas. The lemma chq. is recognized as a part of the syncon 57220 but the rule excludes it from the set and is therefore not generated in the match. In other words, only the token Checks is part of the set of values resulting from the difference between the SYNCON set and the LEMMA set. The following example: ROLE(SUBJECT) + TYPE(NPR) recognizes subjects which are also proper nouns ( NPR ). LEMMA(\"dog\") - KEYWORD(\"dogs\") In this example, dogs is an inflected form of the lemma dog and it is possible to subtract this keyword from the lemma set. ANCESTOR(100000729)//@SYN: #100000729# [animal] + PATTERN(\"[^A-Z]\") The example above will match animal names belonging to the lexical and conceptual chain of the ANCESTOR , only if they don't contain capital letters.","title":"Combination of attributes"},{"location":"attributes/external-files/","text":"Attributes from external files Attribute's values may be replaced by a particular type of file called external list , which contains the same values but is loaded by the rule itself, resulting in the same behavior of a normal list. The syntax is: attribute(LOAD \"file1\"[, LOAD \"file2\", ...]) or: attribute(EXPAND \"file1\"[, EXPAND \"file2\", ...]) where attribute cannot be RELEVANT , because RELEVANT attribute already refers to lists: those are produced by the disambiguator. LOAD and EXPAND must be written in uppercase. file# refers to the complete name of the external file including the extension. Files must be plain text files with .cl extension and the values they contain must comply with the standards described for each attribute. Below are syntax guidelines for listing values in external files: Each value should stand alone in a line. Commas are not to be used to separate values from one another. Quotation marks and spaces at the end of each value are not allowed. Blank lines are also not allowed. Comments can be used, but they must be removed before deploying the project. All list files must use the UTF-8 encoding. The table below gives some examples of the difference between inline lists and an external list. Attribute Inline list External list KEYWORD \"rag\", \"duster\", \"filth\" rag duster filth LEMMA \"dog\", \"kangaroo\", \"slipper\" dog kangaroo slipper SYNCON 17200, 37560, 12985, 15380 17200 37560 12985 15380 ANCESTOR 34567:2, 897, 72914:2:superverbum/subverbum 34567:2 897 72914:2:superverbum/subverbum TYPE PRO, NPH:F, NOU:S, VER:simple_past PRO NPH:F NOU:S VER:simple_past PATTERN \"a.+\", \"[0-1]+\" a.+ [0-1]+ LIST 34567:2, 897, 72914:2:superverbum/subverbum 34567:2 897 72914:2:superverbum/subverbum ROLE SUBJECT, OBJECT SUBJECT OBJECT POSITION BEGIN SENTENCE, END PARAGRAPH BEGIN SENTENCE END PARAGRAPH The use of LOAD or EXPAND is interchangeable in external lists. The use of external lists is particularly useful when an attribute containing a list of multiple values must be used several times for different rules. In essence, transferring the list of values to an external file is convenient, because the list will only need to be defined just once, even if it is used in several rules. If the external list is modified, the changes will be automatically valid for all rules containing the same external list. In other words, there is no need to repeat the same modification for every instance of the list as is the case when using standard syntax. For example: SYNCON(LOAD \"european_union_countries.cl\") The external list european_union_countries.cl which contains the syncon IDs for all member countries of the European Union, corresponds with the following statement, that uses the standard syntax for the SYNCON Attribute: SYNCON(12768485,12788565,10728626,10144226,13063047,12610771,10449831,10655933,13003269,12907079,10387796,10715658,12949572,13160565,10454354,10593395,12946294,12550744,12737184,10794206,12253132,10794211,13043341,13175669,12498789,12649090,12622858) External lists can also be used if the text file is placed in a sub-folder. In this case the LOAD / EXPAND syntax notation is as follows: SYNCON (LOAD \"subfolder_name/.../european_union_countries.cl\") the slash symbol ( / ) is preferred to the backslash ( \\ ) because it is compatible with both Linux and Windows environments. However both notations are accepted if working in Windows.","title":"Attributes from external files"},{"location":"attributes/external-files/#attributes-from-external-files","text":"Attribute's values may be replaced by a particular type of file called external list , which contains the same values but is loaded by the rule itself, resulting in the same behavior of a normal list. The syntax is: attribute(LOAD \"file1\"[, LOAD \"file2\", ...]) or: attribute(EXPAND \"file1\"[, EXPAND \"file2\", ...]) where attribute cannot be RELEVANT , because RELEVANT attribute already refers to lists: those are produced by the disambiguator. LOAD and EXPAND must be written in uppercase. file# refers to the complete name of the external file including the extension. Files must be plain text files with .cl extension and the values they contain must comply with the standards described for each attribute. Below are syntax guidelines for listing values in external files: Each value should stand alone in a line. Commas are not to be used to separate values from one another. Quotation marks and spaces at the end of each value are not allowed. Blank lines are also not allowed. Comments can be used, but they must be removed before deploying the project. All list files must use the UTF-8 encoding. The table below gives some examples of the difference between inline lists and an external list. Attribute Inline list External list KEYWORD \"rag\", \"duster\", \"filth\" rag duster filth LEMMA \"dog\", \"kangaroo\", \"slipper\" dog kangaroo slipper SYNCON 17200, 37560, 12985, 15380 17200 37560 12985 15380 ANCESTOR 34567:2, 897, 72914:2:superverbum/subverbum 34567:2 897 72914:2:superverbum/subverbum TYPE PRO, NPH:F, NOU:S, VER:simple_past PRO NPH:F NOU:S VER:simple_past PATTERN \"a.+\", \"[0-1]+\" a.+ [0-1]+ LIST 34567:2, 897, 72914:2:superverbum/subverbum 34567:2 897 72914:2:superverbum/subverbum ROLE SUBJECT, OBJECT SUBJECT OBJECT POSITION BEGIN SENTENCE, END PARAGRAPH BEGIN SENTENCE END PARAGRAPH The use of LOAD or EXPAND is interchangeable in external lists. The use of external lists is particularly useful when an attribute containing a list of multiple values must be used several times for different rules. In essence, transferring the list of values to an external file is convenient, because the list will only need to be defined just once, even if it is used in several rules. If the external list is modified, the changes will be automatically valid for all rules containing the same external list. In other words, there is no need to repeat the same modification for every instance of the list as is the case when using standard syntax. For example: SYNCON(LOAD \"european_union_countries.cl\") The external list european_union_countries.cl which contains the syncon IDs for all member countries of the European Union, corresponds with the following statement, that uses the standard syntax for the SYNCON Attribute: SYNCON(12768485,12788565,10728626,10144226,13063047,12610771,10449831,10655933,13003269,12907079,10387796,10715658,12949572,13160565,10454354,10593395,12946294,12550744,12737184,10794206,12253132,10794211,13043341,13175669,12498789,12649090,12622858) External lists can also be used if the text file is placed in a sub-folder. In this case the LOAD / EXPAND syntax notation is as follows: SYNCON (LOAD \"subfolder_name/.../european_union_countries.cl\") the slash symbol ( / ) is preferred to the backslash ( \\ ) because it is compatible with both Linux and Windows environments. However both notations are accepted if working in Windows.","title":"Attributes from external files"},{"location":"attributes/keyword/","text":"KEYWORD attribute The KEYWORD attribute identifies a token by specifying the exact sequence of characters that must be found in a text. The syntax is: KEYWORD(\"string1\"[, \"string2\", ...]) KEYWORD must be written in uppercase. string# expression is a sequence of alphabetical characters, numbers, spaces and other punctuation marks. If you need to use quotation marks ( \" ) inside a string, escape them with the backslash character ( \\ ). For example: \\\"cool\\\" matches: That's a \"cool\" car but not: That's a cool car If string# is written in lowercase the match is case insensitive, so: KEYWORD(\"triumph\") matches: triumph Triumph TRIUMPH triumph ... If string# contains at least one upper-case character the match is case sensitive. For example: KEYWORD(\"Triumph\") matches only Triumph . As a result, the following: KEYWORD(\"Triumph\", \"triumph\") is considered an error, since string1 and string2 are equivalent. The KEYWORD attribute can be used in a number of cases. To identify a generic string, regardless of its possible meanings and uses. :::text KEYWORD(\"card\") In this case, any token in a document that matches the string is identified. In other words, every time that the word card appears in a document, it is matched by the KEYWORD attribute. Not only does the attribute match the simple word card , but also credit card , card game , discount card etc. On the other hand, a word such as postcard is not matched because KEYWORD only matches whole words and not sub-strings (use PATTERN for this). To identify a proper noun or a collocation that does not exist in the Knowledge Graph. For example: KEYWORD(\"John Smith\") KEYWORD(\"sulphite reductor\", \"sulphite reductors\") The first example states that John Smith must be found in a document, therefore John Smiths , which is a different name, is not matched by this keyword. The string is written with the first letters as uppercase so the match is case sensitive. This way, it is possible to avoid mismatches on any lower-case or uppercase tokens that appear in the document such as john smith , JOHN SMITH , John SMITH etc. The second example specifies two strings: the singular and plural forms of a collocation. In fact, since the strings are considered as exact sequences of characters, any variations or inflected forms must be specified for a token to be recognized in a document. To identify a particular phraseology, no matter how complex. For example: KEYWORD(\"sulphured hydrogen reduction through Idemitsu process\") The string above will only match its identical token, if found in a document; slightly different versions of the string are not considered equivalent and will not be matched in a text. Therefore the phrases sulphured hydrogen reduced through Idemitsu process and sulphured hydrogen reduction through Idemitsu processes will not match due to the tokens reduced and processes not matching the original string of reduction and process .","title":"KEYWORD attribute"},{"location":"attributes/keyword/#keyword-attribute","text":"The KEYWORD attribute identifies a token by specifying the exact sequence of characters that must be found in a text. The syntax is: KEYWORD(\"string1\"[, \"string2\", ...]) KEYWORD must be written in uppercase. string# expression is a sequence of alphabetical characters, numbers, spaces and other punctuation marks. If you need to use quotation marks ( \" ) inside a string, escape them with the backslash character ( \\ ). For example: \\\"cool\\\" matches: That's a \"cool\" car but not: That's a cool car If string# is written in lowercase the match is case insensitive, so: KEYWORD(\"triumph\") matches: triumph Triumph TRIUMPH triumph ... If string# contains at least one upper-case character the match is case sensitive. For example: KEYWORD(\"Triumph\") matches only Triumph . As a result, the following: KEYWORD(\"Triumph\", \"triumph\") is considered an error, since string1 and string2 are equivalent. The KEYWORD attribute can be used in a number of cases. To identify a generic string, regardless of its possible meanings and uses. :::text KEYWORD(\"card\") In this case, any token in a document that matches the string is identified. In other words, every time that the word card appears in a document, it is matched by the KEYWORD attribute. Not only does the attribute match the simple word card , but also credit card , card game , discount card etc. On the other hand, a word such as postcard is not matched because KEYWORD only matches whole words and not sub-strings (use PATTERN for this). To identify a proper noun or a collocation that does not exist in the Knowledge Graph. For example: KEYWORD(\"John Smith\") KEYWORD(\"sulphite reductor\", \"sulphite reductors\") The first example states that John Smith must be found in a document, therefore John Smiths , which is a different name, is not matched by this keyword. The string is written with the first letters as uppercase so the match is case sensitive. This way, it is possible to avoid mismatches on any lower-case or uppercase tokens that appear in the document such as john smith , JOHN SMITH , John SMITH etc. The second example specifies two strings: the singular and plural forms of a collocation. In fact, since the strings are considered as exact sequences of characters, any variations or inflected forms must be specified for a token to be recognized in a document. To identify a particular phraseology, no matter how complex. For example: KEYWORD(\"sulphured hydrogen reduction through Idemitsu process\") The string above will only match its identical token, if found in a document; slightly different versions of the string are not considered equivalent and will not be matched in a text. Therefore the phrases sulphured hydrogen reduced through Idemitsu process and sulphured hydrogen reduction through Idemitsu processes will not match due to the tokens reduced and processes not matching the original string of reduction and process .","title":"KEYWORD attribute"},{"location":"attributes/lemma/","text":"LEMMA attribute The LEMMA attribute identifies a token by specifying the base form of a word contained in the Knowledge Graph which is called lemma . The syntax is: LEMMA(\"string1\"[, \"string2\", ...]) LEMMA must be written in uppercase. string# refers to any sequence of alphabetical characters, numbers and punctuation marks. Any of the strings to be recognized in a document can be made up of one or several words but must be written between quotation marks. A rule using the LEMMA attribute is valid only if the specified string is found in the Knowledge Graph, unknown words are not accepted. Strings that are not found in the Knowledge Graph can only be managed with the KEYWORD or PATTERN attributes. The LEMMA attribute allows the user to specify the base form of a word contained in the Knowledge Graph and match all of its inflected forms. For nouns, the base form is the singular form (the lemma child matches the text children ). For verbs, the base form is the infinitive ( go matches went , goes , going ). For adverbs and adjectives, the base form also matches comparative and superlative forms (the lemma strong matches stronger and strongest ). The match for lemmas is case sensitive. In fact, since the entities to be matched are in the Knowledge Graph, the string must therefore be typed as it appears in the semantic network. Nouns are often written in lower-case while proper nouns in upper-case. It is always a good idea to check the Knowledge Graph first to verify how a lemma is written (compare the noun, jaguar , an animal to the proper noun, Jaguar , a luxury car maker). For example: LEMMA(\"coach\") Since coach is the base form for both the noun coach and the verb to coach , the attribute above matches any of the following tokens: coach , coaches , coached , coaching , and so on. LEMMA(\"dog\", \"sheep dog\", \"Bordeaux Mastiff\") The above contains a list of three strings: dog and sheep dog are contained in the Knowledge Graph, but Bordeaux Mastiff is not. This would therefore generate an error. Also, sheep dog is an example of a lemma composed of two or more words contained in the Knowledge Graph as a whole element. This is called collocation , which is a sequence of words that often co-occur in a language and become fully fixed expressions through repeated use. Other examples of collocations present in the Knowledge Graph are credit card , boarding school and public finance . LEMMA(\"State\", \"Republic of Trinidad and Tobago\") The above is an example of a noun lemma written in uppercase. State is often written in uppercase when referring to a nation, while the Republic of Trinidad and Tobago is a proper noun for a country and must always be written in uppercase. Since these entities are case sensitive, they must always be written as they are found in the Knowledge Graph.","title":"LEMMA attribute"},{"location":"attributes/lemma/#lemma-attribute","text":"The LEMMA attribute identifies a token by specifying the base form of a word contained in the Knowledge Graph which is called lemma . The syntax is: LEMMA(\"string1\"[, \"string2\", ...]) LEMMA must be written in uppercase. string# refers to any sequence of alphabetical characters, numbers and punctuation marks. Any of the strings to be recognized in a document can be made up of one or several words but must be written between quotation marks. A rule using the LEMMA attribute is valid only if the specified string is found in the Knowledge Graph, unknown words are not accepted. Strings that are not found in the Knowledge Graph can only be managed with the KEYWORD or PATTERN attributes. The LEMMA attribute allows the user to specify the base form of a word contained in the Knowledge Graph and match all of its inflected forms. For nouns, the base form is the singular form (the lemma child matches the text children ). For verbs, the base form is the infinitive ( go matches went , goes , going ). For adverbs and adjectives, the base form also matches comparative and superlative forms (the lemma strong matches stronger and strongest ). The match for lemmas is case sensitive. In fact, since the entities to be matched are in the Knowledge Graph, the string must therefore be typed as it appears in the semantic network. Nouns are often written in lower-case while proper nouns in upper-case. It is always a good idea to check the Knowledge Graph first to verify how a lemma is written (compare the noun, jaguar , an animal to the proper noun, Jaguar , a luxury car maker). For example: LEMMA(\"coach\") Since coach is the base form for both the noun coach and the verb to coach , the attribute above matches any of the following tokens: coach , coaches , coached , coaching , and so on. LEMMA(\"dog\", \"sheep dog\", \"Bordeaux Mastiff\") The above contains a list of three strings: dog and sheep dog are contained in the Knowledge Graph, but Bordeaux Mastiff is not. This would therefore generate an error. Also, sheep dog is an example of a lemma composed of two or more words contained in the Knowledge Graph as a whole element. This is called collocation , which is a sequence of words that often co-occur in a language and become fully fixed expressions through repeated use. Other examples of collocations present in the Knowledge Graph are credit card , boarding school and public finance . LEMMA(\"State\", \"Republic of Trinidad and Tobago\") The above is an example of a noun lemma written in uppercase. State is often written in uppercase when referring to a nation, while the Republic of Trinidad and Tobago is a proper noun for a country and must always be written in uppercase. Since these entities are case sensitive, they must always be written as they are found in the Knowledge Graph.","title":"LEMMA attribute"},{"location":"attributes/list/","text":"LIST attribute The LIST attribute identifies a token by specifying the numeric ID of a syncon and considering the syncon itself as just a container of lemmas. The token is recognized in a document if it matches one of the lemmas contained in the specified syncon, regardless of the concept that the syncon represents. The syntax is: LIST(id1[, id2, ...]) LIST must be written in uppercase as shown above. id# refers to the unique ID assigned to a Knowledge Graph syncon. Unknown numbers are not accepted. Similar to the SYNCON attribute, the LIST attribute allows the user to specify the ID of a concept (syncon) contained in the Knowledge Graph. The difference is that SYNCON considers both the form of a word and its contextual meaning, while LIST considers the form. When the SYNCON attribute is used in a rule, two conditions have to be verified for a token to be identified in a document: The token must match one of the lemmas that is part of the syncon. The token must be associated to the meaning represented by the syncon during the disambiguation process. The second condition however, is optional in the LIST attribute. In fact, the syncon ID is considered merely as a collection of lemmas. All synonyms, variants, abbreviations etc. that are part of the syncon are matched if found in a document. Synonyms and variants are also recognized both in their base form and inflected forms. This will occur whether the token is disambiguated as instance of the specified syncon in the rule or not. LIST is particularly useful when the token to be recognized in a document is ambiguous, i.e., a word is contained in several syncons representing slightly different meanings and could be disambiguated in several ways. For example, the word glass in the Knowledge Graph yields different types of glasses , such as a container for holding liquids (syncon 16634), the quantity a glass will hold (syncon 59462), an article made of glass (syncon 16639) etc. In such cases, it is better to identify the word and its synonyms without taking into account how the lemmas are disambiguated. In fact, a token matches a LIST rule even thought the syncon associated to it is not the same one specified in the rule. The LIST attribute is also similar to the ANCESTOR attribute in that it allows the user to specify a numeric ID of a syncon contained in the Knowledge Graph, and consider it as the starting point of a chain of concepts and therefore, match all synonyms in all syncons as lemmas. To enable this function of the LIST attribute, add a colon (:) after the syncon ID, followed by the number of levels to be navigated downward.. LIST (ID1:level number, ID2:level number, ...) Levels range from 0 to 99, where 0 is the root which only considers the first level and 99 is the default value that considers all levels. Unlike the ANCESTOR attribute which considers the whole chain, LIST will only consider the lemmas that are part of the selected syncon ID, if no level is specified. It is also possible to specify a link to be navigated when looking for descendants. This can be done by adding another colon (:) after the level number followed by the name of the link. LIST (ID1:levels:linkname, ID2:levels:linkname, ...) Valid links are those available in the Knowledge Graph, including any custom link added for a specific project. If the given ancestor is a noun and no link name is specified, the supernomen/subnomen (\"part of\" type of relation) link will be navigated by default. If the given ancestor is a verb and no link name is specified, the superverbum/subverbum (\"way of\" type of relation) link will be navigated by default. Any other links must be specified in order to be considered in the rule. Consider the following examples: LIST (171508) Syncon 171508 refers to the word write as a verb with a meaning of to create books, poems, news paper articles and other original pieces of text . The verb to write can be interpreted in several ways (see syncons 73459, 73491, 171506, etc.), although, in the end, all of them are slightly different variations of the same action. In this case, it is useful to recognize some synonyms without taking into account how these lemmas are disambiguated. Thus, the rule above will recognize the lemmas compose and pen that are synonymous within syncon 171508 along with the lemma write . Tokens such as write , writing , written will also be matched by the rule even though the syncon associated with these inflected forms may not be syncon 171508 but perhaps 73459 or 73491 or 171506 etc. Consider the following example: LIST (73459:1) In this example, the lemma write and the lemmas scribble and scrawl found in syncon 73691 as its first level descendant (on the superverbum/subverbum link) will be matched, if found in a document. The same goes for the lemmas handwrite and hand-write (syncon 70235) as well as the lemma spell (syncon 69957). However, the lemmas \"misspell\" and \"hyphenate\" (syncons 69958 and 73499, respectively) will not be matched because they belong to the second level of descendants.","title":"LIST attribute"},{"location":"attributes/list/#list-attribute","text":"The LIST attribute identifies a token by specifying the numeric ID of a syncon and considering the syncon itself as just a container of lemmas. The token is recognized in a document if it matches one of the lemmas contained in the specified syncon, regardless of the concept that the syncon represents. The syntax is: LIST(id1[, id2, ...]) LIST must be written in uppercase as shown above. id# refers to the unique ID assigned to a Knowledge Graph syncon. Unknown numbers are not accepted. Similar to the SYNCON attribute, the LIST attribute allows the user to specify the ID of a concept (syncon) contained in the Knowledge Graph. The difference is that SYNCON considers both the form of a word and its contextual meaning, while LIST considers the form. When the SYNCON attribute is used in a rule, two conditions have to be verified for a token to be identified in a document: The token must match one of the lemmas that is part of the syncon. The token must be associated to the meaning represented by the syncon during the disambiguation process. The second condition however, is optional in the LIST attribute. In fact, the syncon ID is considered merely as a collection of lemmas. All synonyms, variants, abbreviations etc. that are part of the syncon are matched if found in a document. Synonyms and variants are also recognized both in their base form and inflected forms. This will occur whether the token is disambiguated as instance of the specified syncon in the rule or not. LIST is particularly useful when the token to be recognized in a document is ambiguous, i.e., a word is contained in several syncons representing slightly different meanings and could be disambiguated in several ways. For example, the word glass in the Knowledge Graph yields different types of glasses , such as a container for holding liquids (syncon 16634), the quantity a glass will hold (syncon 59462), an article made of glass (syncon 16639) etc. In such cases, it is better to identify the word and its synonyms without taking into account how the lemmas are disambiguated. In fact, a token matches a LIST rule even thought the syncon associated to it is not the same one specified in the rule. The LIST attribute is also similar to the ANCESTOR attribute in that it allows the user to specify a numeric ID of a syncon contained in the Knowledge Graph, and consider it as the starting point of a chain of concepts and therefore, match all synonyms in all syncons as lemmas. To enable this function of the LIST attribute, add a colon (:) after the syncon ID, followed by the number of levels to be navigated downward.. LIST (ID1:level number, ID2:level number, ...) Levels range from 0 to 99, where 0 is the root which only considers the first level and 99 is the default value that considers all levels. Unlike the ANCESTOR attribute which considers the whole chain, LIST will only consider the lemmas that are part of the selected syncon ID, if no level is specified. It is also possible to specify a link to be navigated when looking for descendants. This can be done by adding another colon (:) after the level number followed by the name of the link. LIST (ID1:levels:linkname, ID2:levels:linkname, ...) Valid links are those available in the Knowledge Graph, including any custom link added for a specific project. If the given ancestor is a noun and no link name is specified, the supernomen/subnomen (\"part of\" type of relation) link will be navigated by default. If the given ancestor is a verb and no link name is specified, the superverbum/subverbum (\"way of\" type of relation) link will be navigated by default. Any other links must be specified in order to be considered in the rule. Consider the following examples: LIST (171508) Syncon 171508 refers to the word write as a verb with a meaning of to create books, poems, news paper articles and other original pieces of text . The verb to write can be interpreted in several ways (see syncons 73459, 73491, 171506, etc.), although, in the end, all of them are slightly different variations of the same action. In this case, it is useful to recognize some synonyms without taking into account how these lemmas are disambiguated. Thus, the rule above will recognize the lemmas compose and pen that are synonymous within syncon 171508 along with the lemma write . Tokens such as write , writing , written will also be matched by the rule even though the syncon associated with these inflected forms may not be syncon 171508 but perhaps 73459 or 73491 or 171506 etc. Consider the following example: LIST (73459:1) In this example, the lemma write and the lemmas scribble and scrawl found in syncon 73691 as its first level descendant (on the superverbum/subverbum link) will be matched, if found in a document. The same goes for the lemmas handwrite and hand-write (syncon 70235) as well as the lemma spell (syncon 69957). However, the lemmas \"misspell\" and \"hyphenate\" (syncons 69958 and 73499, respectively) will not be matched because they belong to the second level of descendants.","title":"LIST attribute"},{"location":"attributes/pattern/","text":"PATTERN attribute overview Syntax The PATTERN attribute matches the text of one or more consecutive tokens by means of regular expressions . The syntax is: PATTERN(\"regular_expression1\"[, \"regular_expression2\", ...]) The PATTERN keyword must be written in uppercase as shown above. regular_expression# refers to a regular expression which must be written in quotation marks. Behavior The following rules determine the behavior of a PATTERN attribute: The attribute will be true only if the text it matches completely covers the text of one or more consecutive tokens. Regular expressions can span consecutive tokens within the rule's scope. When a regular expression is matched, subsequent regular expressions are ignored. If a single regular expression contains alternatives and one alternative is matched, the subsequent alternatives are ignored. For example, consider these two categorization rules: SCOPE SENTENCE { DOMAIN(housing) { PATTERN(\"hous(e|ed|es)\") } } SCOPE SENTENCE { DOMAIN(housing) { PATTERN(\"hous(es|ed|e)\") } } The two PATTERN attributes in the rules seem equivalent: hous followed by any string between e , ed and es , but they do not produce the same effect. If the text is: house housed houses The regular expression in the first rule: hous(e|ed|es) matches all the lines, but activates the rule in the first line only. This is because the match is always triggered by the first alternative ( hous + e ), therefore the other two alternatives are ignored. In other words, the pattern completely covers line 1 so it triggers; it does not, however, cover the last characters of lines 2 and 3 so the match is only partial, thus the PATTERN is false and consequently, the whole rule's condition is false, therefore it does not trigger. The sequence of operations is the following:: First line ( hous e ) Does hous + e match? YES! \u2192 ignore subsequent alternatives, the match is full, the PATTERN attribute is true, the condition is true, the rule is activated. Second line ( hous ed ) Does hous + e match? YES! \u2192 ignore subsequent alternatives, the match is partial, the PATTERN attribute is false, the condition is false, the rule is not activated. Third line ( hous es ) Does hous + e match? YES! \u2192 ignore subsequent alternatives, the match is partial, the PATTERN attribute is false, the condition is false, the rule is not activated. The regular expression in the second rule: hous(es|ed|e) matches all lines and activates the rule every time. The sequence of operations is the following: First output ( hous e ) Does hous + es match? NO. Does hous + ed match? NO. Does hous + e match? YES! \u2192 no subsequent alternatives to ignore, the match is full, the PATTERN attribute is true, the condition is true, the rule is activated. Second output ( hous ed ) Does hous + es match? NO. Does hous + ed match? YES! \u2192 ignore subsequent alternatives, the match is full, the PATTERN attribute is true, the condition is true, the rule is activated. Third output ( hous es ) Does hous + es match? YES! \u2192 ignore subsequent alternatives, the match is full, the PATTERN attribute is true, the condition is true, the rule is activated.","title":"PATTERN attribute overview"},{"location":"attributes/pattern/#pattern-attribute-overview","text":"","title":"PATTERN attribute overview"},{"location":"attributes/pattern/#syntax","text":"The PATTERN attribute matches the text of one or more consecutive tokens by means of regular expressions . The syntax is: PATTERN(\"regular_expression1\"[, \"regular_expression2\", ...]) The PATTERN keyword must be written in uppercase as shown above. regular_expression# refers to a regular expression which must be written in quotation marks.","title":"Syntax"},{"location":"attributes/pattern/#behavior","text":"The following rules determine the behavior of a PATTERN attribute: The attribute will be true only if the text it matches completely covers the text of one or more consecutive tokens. Regular expressions can span consecutive tokens within the rule's scope. When a regular expression is matched, subsequent regular expressions are ignored. If a single regular expression contains alternatives and one alternative is matched, the subsequent alternatives are ignored. For example, consider these two categorization rules: SCOPE SENTENCE { DOMAIN(housing) { PATTERN(\"hous(e|ed|es)\") } } SCOPE SENTENCE { DOMAIN(housing) { PATTERN(\"hous(es|ed|e)\") } } The two PATTERN attributes in the rules seem equivalent: hous followed by any string between e , ed and es , but they do not produce the same effect. If the text is: house housed houses The regular expression in the first rule: hous(e|ed|es) matches all the lines, but activates the rule in the first line only. This is because the match is always triggered by the first alternative ( hous + e ), therefore the other two alternatives are ignored. In other words, the pattern completely covers line 1 so it triggers; it does not, however, cover the last characters of lines 2 and 3 so the match is only partial, thus the PATTERN is false and consequently, the whole rule's condition is false, therefore it does not trigger. The sequence of operations is the following:: First line ( hous e ) Does hous + e match? YES! \u2192 ignore subsequent alternatives, the match is full, the PATTERN attribute is true, the condition is true, the rule is activated. Second line ( hous ed ) Does hous + e match? YES! \u2192 ignore subsequent alternatives, the match is partial, the PATTERN attribute is false, the condition is false, the rule is not activated. Third line ( hous es ) Does hous + e match? YES! \u2192 ignore subsequent alternatives, the match is partial, the PATTERN attribute is false, the condition is false, the rule is not activated. The regular expression in the second rule: hous(es|ed|e) matches all lines and activates the rule every time. The sequence of operations is the following: First output ( hous e ) Does hous + es match? NO. Does hous + ed match? NO. Does hous + e match? YES! \u2192 no subsequent alternatives to ignore, the match is full, the PATTERN attribute is true, the condition is true, the rule is activated. Second output ( hous ed ) Does hous + es match? NO. Does hous + ed match? YES! \u2192 ignore subsequent alternatives, the match is full, the PATTERN attribute is true, the condition is true, the rule is activated. Third output ( hous es ) Does hous + es match? YES! \u2192 ignore subsequent alternatives, the match is full, the PATTERN attribute is true, the condition is true, the rule is activated.","title":"Behavior"},{"location":"attributes/pattern/groups/","text":"Groups peculiarities The difference between match and capture If you use the PATTERN attribute to set a field in an extraction rule, the groups used in the regular expressions can affect the value of the field. For example, consider this extraction rule: SCOPE SENTENCE { IDENTIFY(OPENDATA) { @KNOWLEDGE_BASE[PATTERN(\"Wikipedia\")] } } This rule sets the KNOWLEDGE_BASE field to Wikipedia if the text contains a token whose text is Wikipedia . In this case the entire matched text is captured and transferred to the field. This slightly different rule, however: SCOPE SENTENCE { IDENTIFY(OPENDATA) { @KNOWLEDGE_BASE[PATTERN(\"(Wiki)pedia\")] } } sets the KNOWLEDGE_BASE field to Wiki if the text contains a token whose text is Wikipedia . Therefore, for the match, it is the entire regular expression which determines the text to be matched. For the capture, however, if there are no capturing groups, then the entire matched text will be captured and transferred to the field; if there are capturing groups, only the text that was matched by those groups will be captured. Consider another PATTERN example: PATTERN(\"\\+\\d+\\s(\\d+\\s\\d+\\s)\") applied to this text: +44 744 0963112 The attribute is evaluated as true, because the regular expression matches the entire text of three consecutive tokens, however only the sub-expression between parentheses determines the capture, so if the attribute is used to set a field, the value of the field will be: 744 0963112 thus, omitting the first part. Non-capturing groups affect the match, but not the capture. For example, if this rule: SCOPE SENTENCE { IDENTIFY(OPENDATA) { @KNOWLEDGE_BASE[PATTERN(\"Wiki(?:pedia|data|base)\")] } } is applied to this text: Wikipedia Wikidata Wikibase the KNOWLEDGE_BASE field is set three times, respectively to: Wikipedia Wikidata Wikibase since all the matched text is captured, regardless of the group (which is non-capturing). Nested groups rule When a capturing group contains other capturing groups\u2014which, recursively, can contain even more capturing groups within themselves\u2014the text corresponding to the outermost group is captured. For example, if the text is: He was awarded the Silver Star for military valor. and the regular expression is: ((Gold|Silver|Purple) (Star|Cross|Heart)) the overall capture is: Silver Star Tip When there are groups and sub-groups and the entire match needs to be captured, surround the regular expression with parentheses. Consecutive groups rule If there are multiple capturing groups at the same level in a regular expression, the overall capture will be the concatenation of what is captured by all the groups, in the order in which the groups are found. A blank character is added as a separator in the concatenation. For example, if the text is: AH_808_BF_915 and the regular expression is: ([A-Z]{2})_[0-9]{3}_([A-Z]{2})_[0-9]{3} the overall capture is: AH BF Repetitions Pay attention to the use of repetitions with capturing groups. If the text is: XXXL the regular expression: (X)*L matches the entire text, but captures only: X because the capturing group, in fact, corresponds only to the X character, even if the repetition of \"zero or more occurrences\" is applied to the group using the asterisk character ( * ). In order to capture all of the Xs , simply surround the expression with parentheses to create an outer capturing group: ((X)*)L Lookahead and lookbehind groups The lookahead and lookbehind groups, whether positive or negative, must not be used in the regular expression syntax of the PATTERN attribute. To obtain the same effect, use the capturing groups or sequence operators plus negations appropriately. For example, to obtain the same effect of as an expression like: \"Capture blue only if followed by sky \" (positive lookahead) a lookahead group isn't necessary, just use the capturing groups like this: (blue) sky To obtain the effect of a negative lookahead like: \"Capture dark only if NOT followed by matter \", use a condition like this: @FIELDX[PATTERN(\"dark\")] >> !PATTERN(\"matter\") Similarly, for a positive lookbehind like: \"Capture star only if preceded by red \", just use the capturing groups like this: red (star) while for a negative lookbehind like: \"Capture star only if NOT preceded by cake \", you can use a condition like this: !PATTERN(\"cake\") >> @FIELDX[PATTERN(\"star\")] Backward references A backward reference is a reference to the text of a previously defined group. Note The reference is to the matched text and not to the expression itself. A backward reference consists of the escape character \\ followed by a number between 1 and 9. \\1 refers to the first group, \\2 to the second, and so on. For example: (.*)-\\1 matches any string which repeats itself, with a central hyphen, such as: go-go ha-ha walla-walla Comments Comments are a special type of group. The syntax is: (?# comment ) Comments are useful to explain complex regular expressions; they affect neither matches nor captures.","title":"Groups peculiarities"},{"location":"attributes/pattern/groups/#groups-peculiarities","text":"","title":"Groups peculiarities"},{"location":"attributes/pattern/groups/#the-difference-between-match-and-capture","text":"If you use the PATTERN attribute to set a field in an extraction rule, the groups used in the regular expressions can affect the value of the field. For example, consider this extraction rule: SCOPE SENTENCE { IDENTIFY(OPENDATA) { @KNOWLEDGE_BASE[PATTERN(\"Wikipedia\")] } } This rule sets the KNOWLEDGE_BASE field to Wikipedia if the text contains a token whose text is Wikipedia . In this case the entire matched text is captured and transferred to the field. This slightly different rule, however: SCOPE SENTENCE { IDENTIFY(OPENDATA) { @KNOWLEDGE_BASE[PATTERN(\"(Wiki)pedia\")] } } sets the KNOWLEDGE_BASE field to Wiki if the text contains a token whose text is Wikipedia . Therefore, for the match, it is the entire regular expression which determines the text to be matched. For the capture, however, if there are no capturing groups, then the entire matched text will be captured and transferred to the field; if there are capturing groups, only the text that was matched by those groups will be captured. Consider another PATTERN example: PATTERN(\"\\+\\d+\\s(\\d+\\s\\d+\\s)\") applied to this text: +44 744 0963112 The attribute is evaluated as true, because the regular expression matches the entire text of three consecutive tokens, however only the sub-expression between parentheses determines the capture, so if the attribute is used to set a field, the value of the field will be: 744 0963112 thus, omitting the first part. Non-capturing groups affect the match, but not the capture. For example, if this rule: SCOPE SENTENCE { IDENTIFY(OPENDATA) { @KNOWLEDGE_BASE[PATTERN(\"Wiki(?:pedia|data|base)\")] } } is applied to this text: Wikipedia Wikidata Wikibase the KNOWLEDGE_BASE field is set three times, respectively to: Wikipedia Wikidata Wikibase since all the matched text is captured, regardless of the group (which is non-capturing).","title":"The difference between match and capture"},{"location":"attributes/pattern/groups/#nested-groups-rule","text":"When a capturing group contains other capturing groups\u2014which, recursively, can contain even more capturing groups within themselves\u2014the text corresponding to the outermost group is captured. For example, if the text is: He was awarded the Silver Star for military valor. and the regular expression is: ((Gold|Silver|Purple) (Star|Cross|Heart)) the overall capture is: Silver Star Tip When there are groups and sub-groups and the entire match needs to be captured, surround the regular expression with parentheses.","title":"Nested groups rule"},{"location":"attributes/pattern/groups/#consecutive-groups-rule","text":"If there are multiple capturing groups at the same level in a regular expression, the overall capture will be the concatenation of what is captured by all the groups, in the order in which the groups are found. A blank character is added as a separator in the concatenation. For example, if the text is: AH_808_BF_915 and the regular expression is: ([A-Z]{2})_[0-9]{3}_([A-Z]{2})_[0-9]{3} the overall capture is: AH BF","title":"Consecutive groups rule"},{"location":"attributes/pattern/groups/#repetitions","text":"Pay attention to the use of repetitions with capturing groups. If the text is: XXXL the regular expression: (X)*L matches the entire text, but captures only: X because the capturing group, in fact, corresponds only to the X character, even if the repetition of \"zero or more occurrences\" is applied to the group using the asterisk character ( * ). In order to capture all of the Xs , simply surround the expression with parentheses to create an outer capturing group: ((X)*)L","title":"Repetitions"},{"location":"attributes/pattern/groups/#lookahead-and-lookbehind-groups","text":"The lookahead and lookbehind groups, whether positive or negative, must not be used in the regular expression syntax of the PATTERN attribute. To obtain the same effect, use the capturing groups or sequence operators plus negations appropriately. For example, to obtain the same effect of as an expression like: \"Capture blue only if followed by sky \" (positive lookahead) a lookahead group isn't necessary, just use the capturing groups like this: (blue) sky To obtain the effect of a negative lookahead like: \"Capture dark only if NOT followed by matter \", use a condition like this: @FIELDX[PATTERN(\"dark\")] >> !PATTERN(\"matter\") Similarly, for a positive lookbehind like: \"Capture star only if preceded by red \", just use the capturing groups like this: red (star) while for a negative lookbehind like: \"Capture star only if NOT preceded by cake \", you can use a condition like this: !PATTERN(\"cake\") >> @FIELDX[PATTERN(\"star\")]","title":"Lookahead and lookbehind groups"},{"location":"attributes/pattern/groups/#backward-references","text":"A backward reference is a reference to the text of a previously defined group. Note The reference is to the matched text and not to the expression itself. A backward reference consists of the escape character \\ followed by a number between 1 and 9. \\1 refers to the first group, \\2 to the second, and so on. For example: (.*)-\\1 matches any string which repeats itself, with a central hyphen, such as: go-go ha-ha walla-walla","title":"Backward references"},{"location":"attributes/pattern/groups/#comments","text":"Comments are a special type of group. The syntax is: (?# comment ) Comments are useful to explain complex regular expressions; they affect neither matches nor captures.","title":"Comments"},{"location":"attributes/pattern/regular-expressions/","text":"Regular expressions overview Definition and fundamental properties Regular expressions are a form of language which provide concise and flexible means to match strings of text such as characters, words, or patterns of characters. They are useful for matching product codes, phone numbers, license plates and everything featuring a fixed pattern with variable content. The followings are examples of how regular expressions are used. Consider this regular expression: [\\dA-Z\\-\\#]+ It matches any sequence of one or more ( + ) digits ( \\d ) or uppercase letters ( A-Z ) along with some non-alphanumeric characters ( - and # ). This expression will then match, for example, US license plates such as 63R2189 , DGM529 or 14Y2692 . The following expression: \\\"[^\\\"]+\\\" matches any sentence or word typed between quotes. For example, in a sentence like this: \"The market is not operating in a normal way,\" Mr. Bernanke said on that August call, in a moment of historic understatement. the regular expression would match: \"The market is not operating in a normal way,\" Regular expressions reference There are several types of regex engines on the market, each supporting its own regular expressions dialect. The PATTERN attribute supports Perl compatible regular expressions . Tip You can use the regex101 web app to test your regular expressions. Select the PCRE (PHP) flavor and enable the Anchored option to faithfully reproduce the behavior of the regular expression syntax of the PATTERN attribute.","title":"Regular expressions overview"},{"location":"attributes/pattern/regular-expressions/#regular-expressions-overview","text":"","title":"Regular expressions overview"},{"location":"attributes/pattern/regular-expressions/#definition-and-fundamental-properties","text":"Regular expressions are a form of language which provide concise and flexible means to match strings of text such as characters, words, or patterns of characters. They are useful for matching product codes, phone numbers, license plates and everything featuring a fixed pattern with variable content. The followings are examples of how regular expressions are used. Consider this regular expression: [\\dA-Z\\-\\#]+ It matches any sequence of one or more ( + ) digits ( \\d ) or uppercase letters ( A-Z ) along with some non-alphanumeric characters ( - and # ). This expression will then match, for example, US license plates such as 63R2189 , DGM529 or 14Y2692 . The following expression: \\\"[^\\\"]+\\\" matches any sentence or word typed between quotes. For example, in a sentence like this: \"The market is not operating in a normal way,\" Mr. Bernanke said on that August call, in a moment of historic understatement. the regular expression would match: \"The market is not operating in a normal way,\"","title":"Definition and fundamental properties"},{"location":"attributes/pattern/regular-expressions/#regular-expressions-reference","text":"There are several types of regex engines on the market, each supporting its own regular expressions dialect. The PATTERN attribute supports Perl compatible regular expressions . Tip You can use the regex101 web app to test your regular expressions. Select the PCRE (PHP) flavor and enable the Anchored option to faithfully reproduce the behavior of the regular expression syntax of the PATTERN attribute.","title":"Regular expressions reference"},{"location":"attributes/position/","text":"POSITION attribute The POSITION attribute identifies a token by specifying its position in a text. The token will be recognized in a text, if it is found in the specified position. The syntax is: POSITION(position1[, position2, ...]) POSITION must be written in uppercase. position# refers to a list of predefined values identifying key positions for textual elements inside the document itself. These textual elements include any sequence of alphabetical characters, numbers and punctuation marks. A rule using the POSITION attribute will be valid, only if the position is specified in a predefined format. All positions, with a brief description for each one of them, are listed in the table below. Position Description BEGIN SENTENCE First token in a sentence END SENTENCE Last token in a sentence BEGIN PARAGRAPH First token in a paragraph END PARAGRAPH Last token in a paragraph BEGIN SECTION First token in a document END SECTION Last token in a document Warning Please note: the POSITION attribute, if used alone, is hyper generative. It is highly recommended to use the POSITION attribute in conjunction with other attributes. The POSITION attribute allows the use of one or more positions in a given statement. A token will be identified in a text, if it is found in the specified position. For example: POSITION(BEGIN SENTENCE) This statement would identify any element found at the beginning of a sentence. For demonstrative purposes, let's imagine the statement above is used alone in rule-writing. In a sentence such as: Investigators said it could take months to create a full account of the events preceding and during the killing rampage. The State Police officially confirmed the identity of the killer. The elements that are recognized as the beginning of the sentence would be Investigators and The .","title":"POSITION attribute"},{"location":"attributes/position/#position-attribute","text":"The POSITION attribute identifies a token by specifying its position in a text. The token will be recognized in a text, if it is found in the specified position. The syntax is: POSITION(position1[, position2, ...]) POSITION must be written in uppercase. position# refers to a list of predefined values identifying key positions for textual elements inside the document itself. These textual elements include any sequence of alphabetical characters, numbers and punctuation marks. A rule using the POSITION attribute will be valid, only if the position is specified in a predefined format. All positions, with a brief description for each one of them, are listed in the table below. Position Description BEGIN SENTENCE First token in a sentence END SENTENCE Last token in a sentence BEGIN PARAGRAPH First token in a paragraph END PARAGRAPH Last token in a paragraph BEGIN SECTION First token in a document END SECTION Last token in a document Warning Please note: the POSITION attribute, if used alone, is hyper generative. It is highly recommended to use the POSITION attribute in conjunction with other attributes. The POSITION attribute allows the use of one or more positions in a given statement. A token will be identified in a text, if it is found in the specified position. For example: POSITION(BEGIN SENTENCE) This statement would identify any element found at the beginning of a sentence. For demonstrative purposes, let's imagine the statement above is used alone in rule-writing. In a sentence such as: Investigators said it could take months to create a full account of the events preceding and during the killing rampage. The State Police officially confirmed the identity of the killer. The elements that are recognized as the beginning of the sentence would be Investigators and The .","title":"POSITION attribute"},{"location":"attributes/relevant/","text":"RELEVANT attribute The RELEVANT attribute matches tokens that the disambiguator marked as \"relevant\". The lists of relevant elements recognized by the disambiguator are: Keywords Lemmas Syncons Knowledge Graph domains Sentences However, only keywords, lemmas and syncons can be used with the RELEVANT attribute. In fact, the other two refer to linguistic elements that go beyond the token limits, therefore making them unusable. The syntax is: RELEVANT(list1[, list2, ...]) RELEVANT must be written in uppercase. list# refers to one of the lists above, its possible values are: LEMMA SYNCON KEYWORD Warning Please note: the RELEVANT attribute, if used alone, is hyper generative. It is highly recommended to use the RELEVANT attribute in conjunction with other attributes. During the disambiguation process, a complex analysis is performed to identify the most significant elements within the text. The disambiguator identifies all lemmas, syncons and keywords contained in a document and ranks them by relevance on a percentage scale. Only tokens which exceed a predefined threshold score are inserted into one of the three lists. Lemmas and syncons lists contain only elements found in the Knowledge Graph while Keyword lists contain single terms unknown to the Knowledge Graph as well as sequences or terms (both known and unknown) that may go beyond lemma, syncon and phrase limits. Keywords, therefore, do not easily synchronize with other attributes. To be more precise, RELEVANT (KEYWORD) refers to \"compound terms\" extracted by the disambiguator. The RELEVANT attribute allows the use of one or more lists in a given statement. A token will be identified in a text, if it matches one of the elements included in the selected list. It is also possible to define a relevance threshold score to restrict the behavior of the RELEVANT attribute. The syntax is: RELEVANT(list1:threshold) threshold is a percentage between 0 and 100. In this case, the RELEVANT attribute will be validated, only if the token to be matched in a document matches one of the elements included in the selected list and its score is equal or greater than the threshold defined in the rule. Consider the following examples: RELEVANT (LEMMA) This statement will identify any relevant lemma in the corresponding list. For demonstrative purposes, let's imagine the statement above is used by itself in rule-writing. In a paragraph such as: Although Congress may leave the details of Medicare savings to be worked out next year, there is already discussion of cutting special payments to teaching hospitals and small rural hospitals. Lawmakers are also considering reducing payments to hospitals for certain outpatient services that can be performed at lower cost in doctors' offices. Medicare pays substantially higher rates for the same services when they are provided in a hospital outpatient department rather than a doctor's office. The differential added $1.5 billion to Medicare costs last year, and as hospitals buy physician practices around the country, the costs are likely to grow, the Medicare commission says. the elements identified as relevant lemmas (along with their score) would be: Lemma Score Medicare 17.9% hospital 9.7% doctor 7.3% outpatient 6.8% teaching hospital 6.7% payment 6.3% discussion 6.0% Consider the same text processed with a RELEVANT attribute defining a threshold score: RELEVANT (LEMMA:7%) In this case, only the first three tokens would be matched.","title":"RELEVANT attribute"},{"location":"attributes/relevant/#relevant-attribute","text":"The RELEVANT attribute matches tokens that the disambiguator marked as \"relevant\". The lists of relevant elements recognized by the disambiguator are: Keywords Lemmas Syncons Knowledge Graph domains Sentences However, only keywords, lemmas and syncons can be used with the RELEVANT attribute. In fact, the other two refer to linguistic elements that go beyond the token limits, therefore making them unusable. The syntax is: RELEVANT(list1[, list2, ...]) RELEVANT must be written in uppercase. list# refers to one of the lists above, its possible values are: LEMMA SYNCON KEYWORD Warning Please note: the RELEVANT attribute, if used alone, is hyper generative. It is highly recommended to use the RELEVANT attribute in conjunction with other attributes. During the disambiguation process, a complex analysis is performed to identify the most significant elements within the text. The disambiguator identifies all lemmas, syncons and keywords contained in a document and ranks them by relevance on a percentage scale. Only tokens which exceed a predefined threshold score are inserted into one of the three lists. Lemmas and syncons lists contain only elements found in the Knowledge Graph while Keyword lists contain single terms unknown to the Knowledge Graph as well as sequences or terms (both known and unknown) that may go beyond lemma, syncon and phrase limits. Keywords, therefore, do not easily synchronize with other attributes. To be more precise, RELEVANT (KEYWORD) refers to \"compound terms\" extracted by the disambiguator. The RELEVANT attribute allows the use of one or more lists in a given statement. A token will be identified in a text, if it matches one of the elements included in the selected list. It is also possible to define a relevance threshold score to restrict the behavior of the RELEVANT attribute. The syntax is: RELEVANT(list1:threshold) threshold is a percentage between 0 and 100. In this case, the RELEVANT attribute will be validated, only if the token to be matched in a document matches one of the elements included in the selected list and its score is equal or greater than the threshold defined in the rule. Consider the following examples: RELEVANT (LEMMA) This statement will identify any relevant lemma in the corresponding list. For demonstrative purposes, let's imagine the statement above is used by itself in rule-writing. In a paragraph such as: Although Congress may leave the details of Medicare savings to be worked out next year, there is already discussion of cutting special payments to teaching hospitals and small rural hospitals. Lawmakers are also considering reducing payments to hospitals for certain outpatient services that can be performed at lower cost in doctors' offices. Medicare pays substantially higher rates for the same services when they are provided in a hospital outpatient department rather than a doctor's office. The differential added $1.5 billion to Medicare costs last year, and as hospitals buy physician practices around the country, the costs are likely to grow, the Medicare commission says. the elements identified as relevant lemmas (along with their score) would be: Lemma Score Medicare 17.9% hospital 9.7% doctor 7.3% outpatient 6.8% teaching hospital 6.7% payment 6.3% discussion 6.0% Consider the same text processed with a RELEVANT attribute defining a threshold score: RELEVANT (LEMMA:7%) In this case, only the first three tokens would be matched.","title":"RELEVANT attribute"},{"location":"attributes/role/","text":"ROLE attribute Basic syntax The ROLE attribute identifies a token by specifying the syntactic role it takes in a clause. The token will be recognized in a text, if it matches the specified role. The syntax is: ROLE(role1[, role2, ...]) ROLE must be written in uppercase. role# refers to one of the basic syntactic units commonly recognized during the logical analysis of sentences or clauses: subject, object, verb etc. During the disambiguation process, after word classes (noun, pronoun, adjective, adverb, etc.) have been recognized and assigned to each and every element in the clause/sentence, words are then analyzed from a different point of view. The disambiguator groups them and analyzes the syntactic function that each group performs in the clause/sentence with the aim of recognizing and assigning a role to each group of words. A rule using the ROLE attribute will be valid, only if the role is specified in a predefined format. All available roles, with a brief description for each one of them, are shown in the table below. Role Description OBJECT Direct object SUBJECT Subject NOMINAL_P Predicate nominal VERBAL_P Verbal predicate COPULA Copula INDIRECT Indirect object OTHER Other complement Warning Please note: the ROLE attribute, if used alone, is hyper generative. It is highly recommended to use the ROLE attribute in conjunction with other attributes. Please note that all but the last of the roles listed above are commonly recognized classes in English, Italian, Spanish and German grammar. The role OTHER has a special status as described in the next section. The ROLE attribute allows the use of one or more roles in a given statement. A token will be identified in a text, if, during the disambiguation process, the token is associated with the specified role. For example: ROLE (SUBJECT) This statement would identify any element acting as a subject in a sentence or a clause. For demonstrative purposes, imagine that the statement above is used alone in rule-writing. In a sentence such as: The President offered no specific proposals. the text recognized as the subject is The President . This group of words acting as a subject is made up of one main clause that expresses its action in an active voice. The same statement defined above would also recognize subjects within passive voice clauses. Consider the following sentence: All victims were wounded by officers. The element that would be recognized as a subject here is by officers . This is the object of a passive voice but could be \"promoted\" to the role of real syntactic subject of the clause, if this is turned into the active voice. The concept underlying the clause All victims were wounded by officers can also be expressed in the following way: The officers wounded all the victims . The victims are not those who performed the action (normally the role attributed to the subject), instead they \"received\" the action performed by officers . OTHER role While using the ROLE attribute, it is possible to specify not only the \"standard\" roles corresponding to recognized classes in the English, Italian, Spanish and German grammar, but also the role named OTHER . During the disambiguation process, if it is not possible to accurately interpret the correct syntactical roles of all elements in a sentence or none of the available roles are suitable, the role OTHER will be assigned by default. Thus, it is possible to use the logical structure of the text to develop both categorization or extraction rules.","title":"ROLE attribute"},{"location":"attributes/role/#role-attribute","text":"","title":"ROLE attribute"},{"location":"attributes/role/#basic-syntax","text":"The ROLE attribute identifies a token by specifying the syntactic role it takes in a clause. The token will be recognized in a text, if it matches the specified role. The syntax is: ROLE(role1[, role2, ...]) ROLE must be written in uppercase. role# refers to one of the basic syntactic units commonly recognized during the logical analysis of sentences or clauses: subject, object, verb etc. During the disambiguation process, after word classes (noun, pronoun, adjective, adverb, etc.) have been recognized and assigned to each and every element in the clause/sentence, words are then analyzed from a different point of view. The disambiguator groups them and analyzes the syntactic function that each group performs in the clause/sentence with the aim of recognizing and assigning a role to each group of words. A rule using the ROLE attribute will be valid, only if the role is specified in a predefined format. All available roles, with a brief description for each one of them, are shown in the table below. Role Description OBJECT Direct object SUBJECT Subject NOMINAL_P Predicate nominal VERBAL_P Verbal predicate COPULA Copula INDIRECT Indirect object OTHER Other complement Warning Please note: the ROLE attribute, if used alone, is hyper generative. It is highly recommended to use the ROLE attribute in conjunction with other attributes. Please note that all but the last of the roles listed above are commonly recognized classes in English, Italian, Spanish and German grammar. The role OTHER has a special status as described in the next section. The ROLE attribute allows the use of one or more roles in a given statement. A token will be identified in a text, if, during the disambiguation process, the token is associated with the specified role. For example: ROLE (SUBJECT) This statement would identify any element acting as a subject in a sentence or a clause. For demonstrative purposes, imagine that the statement above is used alone in rule-writing. In a sentence such as: The President offered no specific proposals. the text recognized as the subject is The President . This group of words acting as a subject is made up of one main clause that expresses its action in an active voice. The same statement defined above would also recognize subjects within passive voice clauses. Consider the following sentence: All victims were wounded by officers. The element that would be recognized as a subject here is by officers . This is the object of a passive voice but could be \"promoted\" to the role of real syntactic subject of the clause, if this is turned into the active voice. The concept underlying the clause All victims were wounded by officers can also be expressed in the following way: The officers wounded all the victims . The victims are not those who performed the action (normally the role attributed to the subject), instead they \"received\" the action performed by officers .","title":"Basic syntax"},{"location":"attributes/role/#other-role","text":"While using the ROLE attribute, it is possible to specify not only the \"standard\" roles corresponding to recognized classes in the English, Italian, Spanish and German grammar, but also the role named OTHER . During the disambiguation process, if it is not possible to accurately interpret the correct syntactical roles of all elements in a sentence or none of the available roles are suitable, the role OTHER will be assigned by default. Thus, it is possible to use the logical structure of the text to develop both categorization or extraction rules.","title":"OTHER role"},{"location":"attributes/script/","text":"SCRIPT attribute The SCRIPT attribute allows the user to integrate scripting functions into categorization and extraction rules. By using SCRIPT in combination with all other standard attributes, it is possible to perform very powerful and project-oriented reasoning. The syntax for the SCRIPT attribute is as follows: attribute1 + SCRIPT(\"function_name\") In order to be referenced, the script function must have been defined in a special file, like this: function function_name ( token_index , params ) { } where: token_index is the token ordinal number in the disambiguation output. params is an optional parameter that can be passed by the attribute. Being able to navigate the whole disambiguation output, the function can add very specific constraints to the attribute to which it relates. Consider the following extraction rule: SCOPE SENTENCE { IDENTIFY(PEOPLE) { @NAME[TYPE(NPH) + SCRIPT(\"onExcludeLemma:John Smith\")] } } It aims to extract people's names except John Smith . The following function cancels the extraction of the LEMMA passed as parameter. function onExcludeLemma ( token_index , params ) { var token = DIS . getToken ( token_index ); if ( token . lemma == params ) return false ; return true ; } By leveraging the disambiguation output and being able to perform a check on the base forms, this function is able to avoid the extraction of John Smith even when an abbreviated form of the name occurs in the text.","title":"SCRIPT attribute"},{"location":"attributes/script/#script-attribute","text":"The SCRIPT attribute allows the user to integrate scripting functions into categorization and extraction rules. By using SCRIPT in combination with all other standard attributes, it is possible to perform very powerful and project-oriented reasoning. The syntax for the SCRIPT attribute is as follows: attribute1 + SCRIPT(\"function_name\") In order to be referenced, the script function must have been defined in a special file, like this: function function_name ( token_index , params ) { } where: token_index is the token ordinal number in the disambiguation output. params is an optional parameter that can be passed by the attribute. Being able to navigate the whole disambiguation output, the function can add very specific constraints to the attribute to which it relates. Consider the following extraction rule: SCOPE SENTENCE { IDENTIFY(PEOPLE) { @NAME[TYPE(NPH) + SCRIPT(\"onExcludeLemma:John Smith\")] } } It aims to extract people's names except John Smith . The following function cancels the extraction of the LEMMA passed as parameter. function onExcludeLemma ( token_index , params ) { var token = DIS . getToken ( token_index ); if ( token . lemma == params ) return false ; return true ; } By leveraging the disambiguation output and being able to perform a check on the base forms, this function is able to avoid the extraction of John Smith even when an abbreviated form of the name occurs in the text.","title":"SCRIPT attribute"},{"location":"attributes/syncon/","text":"SYNCON attribute With syncon ID The SYNCON attribute identifies a concept by specifying the numeric ID of a syncon contained in the Knowledge Graph. A token will be recognized in a text, if, during the disambiguation process, it is associated with the specified syncon. The syntax is: SYNCON(id1[, id2, ...]) SYNCON must be written in uppercase. id# is the identification number of a concept ( syncon ) contained in the Knowledge Graph. Since a syncon can be made up of one or more lemmas representing the same concept, all synonyms, variants, abbreviations etc. that are part of the same syncon will be matched, if a corresponding text is found. Synonyms and variants are also recognized both in their base form and inflected forms. When the SYNCON attribute is used in a rule, a token will be identified in a document, only if it is analyzed (disambiguated) as being an instance of the syncon specified in the rule. The token in the document must match one of the lemmas that are part of the syncon. However this is not a sufficient condition for a token to match a SYNCON rule. In fact, many lemmas contained in the Knowledge Graph are polysemic, meaning they have several meanings or they represent different concepts. For a SYNCON rule to be verified, the disambiguator must associate the token in the document to the meaning represented by the syncon in the rules. This means that the SYNCON attribute not only considers the form of a word, but also its contextual meaning. For example: SYNCON (17200) The above refers to the concept of house with a meaning of a building for human habitation . In the Knowledge Graph, this syncon contains three lemmas: house , casa and the abbreviation ho. . If a rule uses this syncon ID, every time that house or casa or ho. are found in a document with a meaning of a dwelling that serves as living quarters for one or more families , the above rule will be verified/triggered. Let's compare the statement above with the following: SYNCON (123085) This syncon also contains the lemma house , but instead of referring to a building for human habitation , it refers to house as a company that designs, produces and sells clothing and accessories . In this case, the lemma house is found in the syncon together with the lemma fashion house . As in the case above, a rule using this syncon ID (123085) will be verified/triggered every time that any of the above lemmas ( house , fashion house ) are found in a document and their contextual meaning is the one represented by the selected syncon. Using the SYNCON attribute, it is also possible to select a list of several IDs: SYNCON (12985, 15039, 15743) Such a rule would recognize the concepts apartment , country house and duplex in a document and the rule would search for all of the synonyms and variants grouped in each syncon. Pay careful attention however when using the SYNCON attribute with terms that are difficult to disambiguate or with terms that contain unwanted synonyms. In this case, it is advisable to furtherly specify the rule using an instruction of LEMMA or KEYWORD . With UNKNOWN It is also possible to use the attribute SYNCON with the value UNKNOWN instead of a syncon ID, as shown below. SYNCON (UNKNOWN) Using this syntax, only \"unknown\" elements will be matched in a text. Unknown elements include words which are not contained in the Knowledge Graph and unknown entities which have been assigned a virtual supernomen by the disambiguator. Warning The value UNKNOWN , if used alone, can be extremely powerful and hyper generative. In fact, it is designed and advised to be used in combination with other attributes.","title":"SYNCON attribute"},{"location":"attributes/syncon/#syncon-attribute","text":"","title":"SYNCON attribute"},{"location":"attributes/syncon/#with-syncon-id","text":"The SYNCON attribute identifies a concept by specifying the numeric ID of a syncon contained in the Knowledge Graph. A token will be recognized in a text, if, during the disambiguation process, it is associated with the specified syncon. The syntax is: SYNCON(id1[, id2, ...]) SYNCON must be written in uppercase. id# is the identification number of a concept ( syncon ) contained in the Knowledge Graph. Since a syncon can be made up of one or more lemmas representing the same concept, all synonyms, variants, abbreviations etc. that are part of the same syncon will be matched, if a corresponding text is found. Synonyms and variants are also recognized both in their base form and inflected forms. When the SYNCON attribute is used in a rule, a token will be identified in a document, only if it is analyzed (disambiguated) as being an instance of the syncon specified in the rule. The token in the document must match one of the lemmas that are part of the syncon. However this is not a sufficient condition for a token to match a SYNCON rule. In fact, many lemmas contained in the Knowledge Graph are polysemic, meaning they have several meanings or they represent different concepts. For a SYNCON rule to be verified, the disambiguator must associate the token in the document to the meaning represented by the syncon in the rules. This means that the SYNCON attribute not only considers the form of a word, but also its contextual meaning. For example: SYNCON (17200) The above refers to the concept of house with a meaning of a building for human habitation . In the Knowledge Graph, this syncon contains three lemmas: house , casa and the abbreviation ho. . If a rule uses this syncon ID, every time that house or casa or ho. are found in a document with a meaning of a dwelling that serves as living quarters for one or more families , the above rule will be verified/triggered. Let's compare the statement above with the following: SYNCON (123085) This syncon also contains the lemma house , but instead of referring to a building for human habitation , it refers to house as a company that designs, produces and sells clothing and accessories . In this case, the lemma house is found in the syncon together with the lemma fashion house . As in the case above, a rule using this syncon ID (123085) will be verified/triggered every time that any of the above lemmas ( house , fashion house ) are found in a document and their contextual meaning is the one represented by the selected syncon. Using the SYNCON attribute, it is also possible to select a list of several IDs: SYNCON (12985, 15039, 15743) Such a rule would recognize the concepts apartment , country house and duplex in a document and the rule would search for all of the synonyms and variants grouped in each syncon. Pay careful attention however when using the SYNCON attribute with terms that are difficult to disambiguate or with terms that contain unwanted synonyms. In this case, it is advisable to furtherly specify the rule using an instruction of LEMMA or KEYWORD .","title":"With syncon ID"},{"location":"attributes/syncon/#with-unknown","text":"It is also possible to use the attribute SYNCON with the value UNKNOWN instead of a syncon ID, as shown below. SYNCON (UNKNOWN) Using this syntax, only \"unknown\" elements will be matched in a text. Unknown elements include words which are not contained in the Knowledge Graph and unknown entities which have been assigned a virtual supernomen by the disambiguator. Warning The value UNKNOWN , if used alone, can be extremely powerful and hyper generative. In fact, it is designed and advised to be used in combination with other attributes.","title":"With UNKNOWN"},{"location":"attributes/tag/","text":"TAG attribute The TAG attribute identifies a token by matching the tag label specified during tagging . The syntax is: TAG(tag1[, tag2, ...]) The TAG attribute can be used alone in an attribute, as in: SCOPE SENTENCE { IDENTIFY(TEST) { @EXTRACTION_FIELD[TAG(tag1)] } } or combined with other attributes as in: SCOPE SENTENCE { IDENTIFY(TEST) { @EXTRACTION_FIELD[SYNCON(1234) + TAG(tag2)] } }","title":"TAG attribute"},{"location":"attributes/tag/#tag-attribute","text":"The TAG attribute identifies a token by matching the tag label specified during tagging . The syntax is: TAG(tag1[, tag2, ...]) The TAG attribute can be used alone in an attribute, as in: SCOPE SENTENCE { IDENTIFY(TEST) { @EXTRACTION_FIELD[TAG(tag1)] } } or combined with other attributes as in: SCOPE SENTENCE { IDENTIFY(TEST) { @EXTRACTION_FIELD[SYNCON(1234) + TAG(tag2)] } }","title":"TAG attribute"},{"location":"attributes/type/","text":"TYPE attribute overview Basic syntax The TYPE attribute matches the type of tokens, that is their word class or their entity type. The syntax is: TYPE(type1[, type2, ...]) TYPE must be written in uppercase as shown above. type# refers to a predefined value chosen between the language word classes\u2014for example nouns, verbs, adjectives\u2014and the types of entity the disambiguator can recognize\u2014for example person names, dates, addresses. Available world classes and entity types are listed below. Word classes Word class Description ADJ Adjective ART Article AUX Auxiliary verb ADV Adverb CON Conjunction NOU Noun NPR Proper noun PNT Punctuation mark PRE Preposition PRO Pronoun PRT Particle[^1] VER Verb Entity types Label Description Example ADR Street address Who lived at 221B Baker Street ? ANM Animal Felix is an anthropomorphic black cat. BLD Building While in London I attended a concert at the Royal Albert Hall . COM Company, business Tesla Inc. sold 10% of its Bitcoin holdings. DAT Date Napoleon died on May 5, 1821 . DEV Device My new Galaxy smartphone has seven cameras. DOC Document I appeal to the Geneva Convention ! EVN Event Felice Gimondi won the Tour de France in 1965. FDD Food, beverage Frank likes to drink Guinness beer. GEA Physical geographic feature I crossed the Mississipi river with my boat GEO Administrative geographic area Alaska is the least densely populated state in the United States . GEX Extended geography The astronauts have landed on Mars . HOU Hours The eclipse reached its peak at 3pm . LEN Legal entity Of course I pay the FICA tax. MAI Email address For any questions do not hesitate to write to helpme@somedomain.com . MEA Measure The chest is five feet wide and 40 inches tall. MMD Mass media I read it in the Guardian . MON Money I sold half of my stock and made six hundred thousand dollars . NPH Person Hakeem Olajuwon dunked effortlessly. ORG Organization, institution, society Now they threaten to quit the United Nations if they are not heard. PCT Percentage The richest 10% of adults in the world own 85% of global wealth. PHO Phone number For poor database design, call (214) 748-3647 . PPH Physical phenomena The COVID-19 infection is slowing down. PRD Product The Rolex Daytona is a wonderful watch. VCL Vehicle A Ferrari 250 GTO was the most expensive car ever sold. WEB Web address Find the best technical documentation at docs.expert.ai . WRK Work of human intelligence Grease is a funny musical romantic comedy. Examples Consider the following example: TYPE (NOU, ADJ) The operand above matches two word classes: nouns ( NOU ) like cat and adjectives ( ADJ ) like strong . The following operand, on the other hand: TYPE (DAT, ADR) matches entities of two types, dates ( DAT ) and addresses ( ADR ), when recognized in a text. See the topic about entity recognition for more information. Warning If used alone, the TYPE attribute, can be hyper generative, so it's advisable to use it in conjunction with other attributes. Sub-attributes Depending on the language, the disambiguator is also capable of identifying some features\u2014mostly grammatical\u2014in addition to the token type. For example, the gender and number of a noun or the tense of a verb. These features are attributes of the type, so they are referred to as sub-attributes . Possible sub-attribute values, like singular and plural for the number of nouns, can be matched by using specific codes ( S for singular or P for plural) in the TYPE statement after the word class or entity type specification, using a colon ( : ) as separator. Multiple codes can be specified one after the other. For example: TYPE(ADJ:P:i) matches any demonstrative (sub-attribute code: i ) plural (sub-attribute code: P ) adjective (word class: ADJ ) such as these and those . This operand: TYPE(NPH:F) matches female (sub-attribute code: F ) person names (sub-attribute code: NPH ) such as Joana , Victoria , Hillary Clinton and Sophia Loren . This operand: TYPE(VER:simple_past) matches any verb (word class: VER ) in the simple past tense (sub-attribute code: simple_past ) like was , received and spoke . You can find the list of sub-attributes and their values codes in the following topics: Sub-attributes - English Sub-attributes - Italian Sub-attributes - French Sub-attributes - Spanish Sub-attributes - German","title":"TYPE attribute overview"},{"location":"attributes/type/#type-attribute-overview","text":"","title":"TYPE attribute overview"},{"location":"attributes/type/#basic-syntax","text":"The TYPE attribute matches the type of tokens, that is their word class or their entity type. The syntax is: TYPE(type1[, type2, ...]) TYPE must be written in uppercase as shown above. type# refers to a predefined value chosen between the language word classes\u2014for example nouns, verbs, adjectives\u2014and the types of entity the disambiguator can recognize\u2014for example person names, dates, addresses. Available world classes and entity types are listed below.","title":"Basic syntax"},{"location":"attributes/type/#word-classes","text":"Word class Description ADJ Adjective ART Article AUX Auxiliary verb ADV Adverb CON Conjunction NOU Noun NPR Proper noun PNT Punctuation mark PRE Preposition PRO Pronoun PRT Particle[^1] VER Verb","title":"Word classes"},{"location":"attributes/type/#entity-types","text":"Label Description Example ADR Street address Who lived at 221B Baker Street ? ANM Animal Felix is an anthropomorphic black cat. BLD Building While in London I attended a concert at the Royal Albert Hall . COM Company, business Tesla Inc. sold 10% of its Bitcoin holdings. DAT Date Napoleon died on May 5, 1821 . DEV Device My new Galaxy smartphone has seven cameras. DOC Document I appeal to the Geneva Convention ! EVN Event Felice Gimondi won the Tour de France in 1965. FDD Food, beverage Frank likes to drink Guinness beer. GEA Physical geographic feature I crossed the Mississipi river with my boat GEO Administrative geographic area Alaska is the least densely populated state in the United States . GEX Extended geography The astronauts have landed on Mars . HOU Hours The eclipse reached its peak at 3pm . LEN Legal entity Of course I pay the FICA tax. MAI Email address For any questions do not hesitate to write to helpme@somedomain.com . MEA Measure The chest is five feet wide and 40 inches tall. MMD Mass media I read it in the Guardian . MON Money I sold half of my stock and made six hundred thousand dollars . NPH Person Hakeem Olajuwon dunked effortlessly. ORG Organization, institution, society Now they threaten to quit the United Nations if they are not heard. PCT Percentage The richest 10% of adults in the world own 85% of global wealth. PHO Phone number For poor database design, call (214) 748-3647 . PPH Physical phenomena The COVID-19 infection is slowing down. PRD Product The Rolex Daytona is a wonderful watch. VCL Vehicle A Ferrari 250 GTO was the most expensive car ever sold. WEB Web address Find the best technical documentation at docs.expert.ai . WRK Work of human intelligence Grease is a funny musical romantic comedy.","title":"Entity types"},{"location":"attributes/type/#examples","text":"Consider the following example: TYPE (NOU, ADJ) The operand above matches two word classes: nouns ( NOU ) like cat and adjectives ( ADJ ) like strong . The following operand, on the other hand: TYPE (DAT, ADR) matches entities of two types, dates ( DAT ) and addresses ( ADR ), when recognized in a text. See the topic about entity recognition for more information. Warning If used alone, the TYPE attribute, can be hyper generative, so it's advisable to use it in conjunction with other attributes.","title":"Examples"},{"location":"attributes/type/#sub-attributes","text":"Depending on the language, the disambiguator is also capable of identifying some features\u2014mostly grammatical\u2014in addition to the token type. For example, the gender and number of a noun or the tense of a verb. These features are attributes of the type, so they are referred to as sub-attributes . Possible sub-attribute values, like singular and plural for the number of nouns, can be matched by using specific codes ( S for singular or P for plural) in the TYPE statement after the word class or entity type specification, using a colon ( : ) as separator. Multiple codes can be specified one after the other. For example: TYPE(ADJ:P:i) matches any demonstrative (sub-attribute code: i ) plural (sub-attribute code: P ) adjective (word class: ADJ ) such as these and those . This operand: TYPE(NPH:F) matches female (sub-attribute code: F ) person names (sub-attribute code: NPH ) such as Joana , Victoria , Hillary Clinton and Sophia Loren . This operand: TYPE(VER:simple_past) matches any verb (word class: VER ) in the simple past tense (sub-attribute code: simple_past ) like was , received and spoke . You can find the list of sub-attributes and their values codes in the following topics: Sub-attributes - English Sub-attributes - Italian Sub-attributes - French Sub-attributes - Spanish Sub-attributes - German","title":"Sub-attributes"},{"location":"attributes/type/english/","text":"Sub-attributes - English The features of word classes and entity types the disambiguator can identify for the English language are: Word class or entity type Features NOU Gender, number NPR Gender, number NPH Gender, number ENT Gender, number ADJ Gender, number, adjective type VER Tense, continuous, person, form PRO Pronoun type, pronoun sub-type CON Conjunction type ADV Adverb type AUX Auxiliary type Values codes for each grammatical feature are listed in the following tables. Gender Code Description M Male F Female N Neuter Number Code Description S Singular P Plural Adjective type Code Description p Possessive i Demonstrative a Other q Descriptive c Cardinal Tense Code Description simple_present Simple present simple_past Simple past ing_form -ing form ed_form -ed form present_perfect Present perfect past_perfect Past perfect future Future conditional Conditional Person Code Description 1 First person singular 3 Third person singular Form Code Description negative Negative reflexive Reflexive passive Passive Continuous Code Description continuous Continuous not_continuous Not continuous Conjunction type Code Description n Conditional c Coordinate clause s Subordinate clause b Coordinate clause/subordinate clause t That Adverb type Code Description M Manner L Place T Time F Frequency Q Quantity Pronoun type Code Description p Personal r Relative Pronoun sub-type Code Description s Subject o Object b Both Auxiliary type Code Description a Auxiliary m Modal s Aspectual","title":"Sub-attributes - English"},{"location":"attributes/type/english/#sub-attributes-english","text":"The features of word classes and entity types the disambiguator can identify for the English language are: Word class or entity type Features NOU Gender, number NPR Gender, number NPH Gender, number ENT Gender, number ADJ Gender, number, adjective type VER Tense, continuous, person, form PRO Pronoun type, pronoun sub-type CON Conjunction type ADV Adverb type AUX Auxiliary type Values codes for each grammatical feature are listed in the following tables.","title":"Sub-attributes - English"},{"location":"attributes/type/english/#gender","text":"Code Description M Male F Female N Neuter","title":"Gender"},{"location":"attributes/type/english/#number","text":"Code Description S Singular P Plural","title":"Number"},{"location":"attributes/type/english/#adjective-type","text":"Code Description p Possessive i Demonstrative a Other q Descriptive c Cardinal","title":"Adjective type"},{"location":"attributes/type/english/#tense","text":"Code Description simple_present Simple present simple_past Simple past ing_form -ing form ed_form -ed form present_perfect Present perfect past_perfect Past perfect future Future conditional Conditional","title":"Tense"},{"location":"attributes/type/english/#person","text":"Code Description 1 First person singular 3 Third person singular","title":"Person"},{"location":"attributes/type/english/#form","text":"Code Description negative Negative reflexive Reflexive passive Passive","title":"Form"},{"location":"attributes/type/english/#continuous","text":"Code Description continuous Continuous not_continuous Not continuous","title":"Continuous"},{"location":"attributes/type/english/#conjunction-type","text":"Code Description n Conditional c Coordinate clause s Subordinate clause b Coordinate clause/subordinate clause t That","title":"Conjunction type"},{"location":"attributes/type/english/#adverb-type","text":"Code Description M Manner L Place T Time F Frequency Q Quantity","title":"Adverb type"},{"location":"attributes/type/english/#pronoun-type","text":"Code Description p Personal r Relative","title":"Pronoun type"},{"location":"attributes/type/english/#pronoun-sub-type","text":"Code Description s Subject o Object b Both","title":"Pronoun sub-type"},{"location":"attributes/type/english/#auxiliary-type","text":"Code Description a Auxiliary m Modal s Aspectual","title":"Auxiliary type"},{"location":"attributes/type/entities/","text":"Entities Overview The disambiguator recognizes entities like people's names, dates, addresses, monetary values, measures, etc., when present in a text. For instance, in the sentence: John Smith lives in a 900 sq ft apartment at 22 Green Park Street. the disambiguator recognizes John Smith as a person's name (entity type NPH ), 900 sq ft as a measure (entity type MEA ) and 22 Green Park Street as address (entity type ADR ). These are also referred to as structured entities , because they are usually made of components like numbers, letters and punctuation marks. Recognized entities cited in a text can be matched by a rule condition using the TYPE attribute and the entity type. Structure decomposition As mentioned above, a structured entity is an aggregation of components, for example a date contains at least a day and a month or a month and a year, an address contains at least a street number and a street name, etc. NPH , DAT , HOU , MEA , MON and ADR types have two properties: They are always associated with a virtual supernomen which specifies what type of entity they are. They can be subdivided into logical components. The first property is related to the fact that tokens like 22 Green Park Street don't correspond to standard Knowledge Graph syncons. Yet the disambiguator understands it's an address and assigns it the meaning of street . Therefore, unknown entities automatically receive a syncon ID of a known concept. For example: February 28, 1893 is recognized as a date and as an instance of the DAT type during disambiguation, but it's also assigned the virtual supernomen corresponding to syncon 65454 ( date , tag_date ). Due to this recognition, the disambiguator can also distinguish between the day, the month and the year and these components can be used in rules using the TRANSFORM feature. In another example: 900 sq ft the text is recognized as an instance of MEA type and is assigned the virtual supernomen corresponding to syncon 58572 ( square foot ). Such an entity has two parts: the numeric value ( 900 ) and the unit of measurement ( sq ft ). The TRANSFORM allows the user to define the way in which structured entities can be divided into components. Below is a list of the virtual supernomens and their corresponding entity types along with the concepts of their components. Check the Knowledge Graph to find the syncon ID for each of them depending on your project language. Entity type Virtual supernomen Components' syncons NPH cat. person tag_first_name, tag_surname, tag_gender DAT tag_date tag_weekday, tag_day, tag_month, tag_year HOU tag_hour tag_hour, tag_minute MEA cat. unit of measurement tag_number, cat. unit of measurement MON cat. money tag_number, tag_currency ADR street tag_road, tag_proper_noun, tag_street_number","title":"Entities"},{"location":"attributes/type/entities/#entities","text":"","title":"Entities"},{"location":"attributes/type/entities/#overview","text":"The disambiguator recognizes entities like people's names, dates, addresses, monetary values, measures, etc., when present in a text. For instance, in the sentence: John Smith lives in a 900 sq ft apartment at 22 Green Park Street. the disambiguator recognizes John Smith as a person's name (entity type NPH ), 900 sq ft as a measure (entity type MEA ) and 22 Green Park Street as address (entity type ADR ). These are also referred to as structured entities , because they are usually made of components like numbers, letters and punctuation marks. Recognized entities cited in a text can be matched by a rule condition using the TYPE attribute and the entity type.","title":"Overview"},{"location":"attributes/type/entities/#structure-decomposition","text":"As mentioned above, a structured entity is an aggregation of components, for example a date contains at least a day and a month or a month and a year, an address contains at least a street number and a street name, etc. NPH , DAT , HOU , MEA , MON and ADR types have two properties: They are always associated with a virtual supernomen which specifies what type of entity they are. They can be subdivided into logical components. The first property is related to the fact that tokens like 22 Green Park Street don't correspond to standard Knowledge Graph syncons. Yet the disambiguator understands it's an address and assigns it the meaning of street . Therefore, unknown entities automatically receive a syncon ID of a known concept. For example: February 28, 1893 is recognized as a date and as an instance of the DAT type during disambiguation, but it's also assigned the virtual supernomen corresponding to syncon 65454 ( date , tag_date ). Due to this recognition, the disambiguator can also distinguish between the day, the month and the year and these components can be used in rules using the TRANSFORM feature. In another example: 900 sq ft the text is recognized as an instance of MEA type and is assigned the virtual supernomen corresponding to syncon 58572 ( square foot ). Such an entity has two parts: the numeric value ( 900 ) and the unit of measurement ( sq ft ). The TRANSFORM allows the user to define the way in which structured entities can be divided into components. Below is a list of the virtual supernomens and their corresponding entity types along with the concepts of their components. Check the Knowledge Graph to find the syncon ID for each of them depending on your project language. Entity type Virtual supernomen Components' syncons NPH cat. person tag_first_name, tag_surname, tag_gender DAT tag_date tag_weekday, tag_day, tag_month, tag_year HOU tag_hour tag_hour, tag_minute MEA cat. unit of measurement tag_number, cat. unit of measurement MON cat. money tag_number, tag_currency ADR street tag_road, tag_proper_noun, tag_street_number","title":"Structure decomposition"},{"location":"attributes/type/french/","text":"Sub-attributes - French The features of word classes and entity types the disambiguator can identify for the French language are: Word class or entity type Features ART Gender, number PRE Gender, number NOU Gender, number NPR Gender, number NPH Gender, number ENT Gender, number ADJ Gender, number, adjective type, grade VER Tense, person, form PRO Gender, number, person, pronoun type, reflexive pronoun CON Conjunction type AUX Auxiliary type Values codes for each grammatical feature are listed in the following tables. Gender Code Description M Male F Female Number Code Description S Singular P Plural Adjective type Code Description p Possessive i Demonstrative q Descriptive a Other n Quantitative c Plural number Tense Code Mood Tense indicative Indicatif / subjunctive Subjonctif / conditional Conditionnel / imperative Imp\u00e9ratif / infinitive Infinitif / participle Participe / gerund G\u00e9rondif / present_indicative Indicatif Pr\u00e9sent present_perfect_indicative Indicatif Pass\u00e9 compos\u00e9 imperfect_indicative Indicatif Imparfait past_perfect_indicative Indicatif Pass\u00e9 simple simple_past_indicative Indicatif Pass\u00e9 simple past_anterior_indicative Indicatif Pass\u00e9 ant\u00e9rieur future_indicative Indicatif Futur future_perfect_indicative Indicatif Futur ant\u00e9rieur present_subjunctive Subjonctif Pr\u00e9sent perfect_subjunctive Subjonctif Pass\u00e9 imperfect_subjunctive Subjonctif Imparfait past_perfect_subjunctive Subjonctif Plus-que-parfait present_conditional Conditionnel Pr\u00e9sent past_conditional Conditionnel Pass\u00e9 present_imperative Imp\u00e9ratif Pr\u00e9sent past_imperative Imp\u00e9ratif Pass\u00e9 present_infinitive Infinitif Pr\u00e9sent past_infinitive Infinitif Pass\u00e9 present_participle Participe Pr\u00e9sent past_participle Participe Pass\u00e9 simple_gerund G\u00e9rondif / past_gerund Participe Pass\u00e9 compos\u00e9 Person Code Description 1 First person singular 2 Second person singular 3 Third person singular 4 First person plural 5 Second person plural 6 Third person plural 7 First, second or third person singular (when identical) 8 First or second person singular (when identical) Form Code Description negative Negative reflexive Reflexive passive Passive Grade Code Description B Positive A Absolute superlative s Relative superlative G Majority comparative m Minority comparative E Equality comparative Conjunction type Code Description s Subordinate clause g It requires conjunctive i It requires indicative v It accepts both conjunctive and indicative c Coordinate clause Pronoun type Code Description s Only subject p Pronominal particle u Complement/subject v Particular pronoun complement a Other Reflexive pronoun Code Description s Reflexive n Not reflexive r Pronominal particle of the reflexive Auxiliary type Code Description a Auxiliary m Modal s Aspectual","title":"Sub-attributes - French"},{"location":"attributes/type/french/#sub-attributes-french","text":"The features of word classes and entity types the disambiguator can identify for the French language are: Word class or entity type Features ART Gender, number PRE Gender, number NOU Gender, number NPR Gender, number NPH Gender, number ENT Gender, number ADJ Gender, number, adjective type, grade VER Tense, person, form PRO Gender, number, person, pronoun type, reflexive pronoun CON Conjunction type AUX Auxiliary type Values codes for each grammatical feature are listed in the following tables.","title":"Sub-attributes - French"},{"location":"attributes/type/french/#gender","text":"Code Description M Male F Female","title":"Gender"},{"location":"attributes/type/french/#number","text":"Code Description S Singular P Plural","title":"Number"},{"location":"attributes/type/french/#adjective-type","text":"Code Description p Possessive i Demonstrative q Descriptive a Other n Quantitative c Plural number","title":"Adjective type"},{"location":"attributes/type/french/#tense","text":"Code Mood Tense indicative Indicatif / subjunctive Subjonctif / conditional Conditionnel / imperative Imp\u00e9ratif / infinitive Infinitif / participle Participe / gerund G\u00e9rondif / present_indicative Indicatif Pr\u00e9sent present_perfect_indicative Indicatif Pass\u00e9 compos\u00e9 imperfect_indicative Indicatif Imparfait past_perfect_indicative Indicatif Pass\u00e9 simple simple_past_indicative Indicatif Pass\u00e9 simple past_anterior_indicative Indicatif Pass\u00e9 ant\u00e9rieur future_indicative Indicatif Futur future_perfect_indicative Indicatif Futur ant\u00e9rieur present_subjunctive Subjonctif Pr\u00e9sent perfect_subjunctive Subjonctif Pass\u00e9 imperfect_subjunctive Subjonctif Imparfait past_perfect_subjunctive Subjonctif Plus-que-parfait present_conditional Conditionnel Pr\u00e9sent past_conditional Conditionnel Pass\u00e9 present_imperative Imp\u00e9ratif Pr\u00e9sent past_imperative Imp\u00e9ratif Pass\u00e9 present_infinitive Infinitif Pr\u00e9sent past_infinitive Infinitif Pass\u00e9 present_participle Participe Pr\u00e9sent past_participle Participe Pass\u00e9 simple_gerund G\u00e9rondif / past_gerund Participe Pass\u00e9 compos\u00e9","title":"Tense"},{"location":"attributes/type/french/#person","text":"Code Description 1 First person singular 2 Second person singular 3 Third person singular 4 First person plural 5 Second person plural 6 Third person plural 7 First, second or third person singular (when identical) 8 First or second person singular (when identical)","title":"Person"},{"location":"attributes/type/french/#form","text":"Code Description negative Negative reflexive Reflexive passive Passive","title":"Form"},{"location":"attributes/type/french/#grade","text":"Code Description B Positive A Absolute superlative s Relative superlative G Majority comparative m Minority comparative E Equality comparative","title":"Grade"},{"location":"attributes/type/french/#conjunction-type","text":"Code Description s Subordinate clause g It requires conjunctive i It requires indicative v It accepts both conjunctive and indicative c Coordinate clause","title":"Conjunction type"},{"location":"attributes/type/french/#pronoun-type","text":"Code Description s Only subject p Pronominal particle u Complement/subject v Particular pronoun complement a Other","title":"Pronoun type"},{"location":"attributes/type/french/#reflexive-pronoun","text":"Code Description s Reflexive n Not reflexive r Pronominal particle of the reflexive","title":"Reflexive pronoun"},{"location":"attributes/type/french/#auxiliary-type","text":"Code Description a Auxiliary m Modal s Aspectual","title":"Auxiliary type"},{"location":"attributes/type/german/","text":"Sub-attributes - German The features of word classes and entity types the disambiguator can identify for the German language are: Word class or entity type Features NOU Gender, number, case NPR Gender, number NPH Gender, number ENT Gender, number ADJ Gender, number, case, adjective type, form VER Tense, passive, person PRO Gender, number, case, person, pronoun type CON Conjunction type ADV Adverb type Values codes for each grammatical feature are listed in the following tables. Gender Code Description M Male F Female N Neuter I Undefined Number Code Description S Singular P Plural Adjective type Code Description P Possessive I Indicative T Interrogative Q Predicative N Numeral Case Code Description N Nominative G Genitive D Dative A Accusative Tense Code Mood Tense indicative Indikativ / subjunctive Konjunktiv I / conditional Konjunktiv II / imperative Imperativ / infinitive Infinitiv / participle Partizip / present_indicative Indikativ Pr\u00e4sens present_perfect_indicative Indikativ Perfekt imperfect_indicative Indikativ Pr\u00e4teritum past_perfect_indicative Indikativ Plusquamperfekt future_indicative Indikativ Futur I future perfect_indicative Indikativ Futur II present_subjunctive Konjunktiv I Pr\u00e4sens perfect_subjunctive Konjunktiv I Perfekt future_subjunctive Konjunktiv I Futur I future_perfect_subjunctive Konjunktiv I Futur II present_conditional Konjunktiv II Pr\u00e4teritum past_conditional Konjunktiv II Plusquamperfekt future_conditional Konjunktiv II Futur I future_perfect_conditional Konjunktiv II Futur II present_imperative Imperativ / present_infinitive Infinitiv Pr\u00e4sens past_infinitive Infinitiv Perfekt present_participle Partizip Pr\u00e4sens past_participle Partizip Perfekt Person Code Description 1 First person singular 2 Second person singular 3 Third person singular 4 First person plural 5 Second person plural 6 Third person plural Form Code Description B Positive C Comparative S Superlative Passive Code Description P Passive A Active Conjunction type Code Description S Subordinate clause C Coordinate clause Adverb type Code Description P Particle V Verb-particle T Interrogative R Pronominal C Conjunctive A Simple Pronoun type Code Description P Personal S Reflexive R Relative T Interrogative A Other","title":"Sub-attributes - German"},{"location":"attributes/type/german/#sub-attributes-german","text":"The features of word classes and entity types the disambiguator can identify for the German language are: Word class or entity type Features NOU Gender, number, case NPR Gender, number NPH Gender, number ENT Gender, number ADJ Gender, number, case, adjective type, form VER Tense, passive, person PRO Gender, number, case, person, pronoun type CON Conjunction type ADV Adverb type Values codes for each grammatical feature are listed in the following tables.","title":"Sub-attributes - German"},{"location":"attributes/type/german/#gender","text":"Code Description M Male F Female N Neuter I Undefined","title":"Gender"},{"location":"attributes/type/german/#number","text":"Code Description S Singular P Plural","title":"Number"},{"location":"attributes/type/german/#adjective-type","text":"Code Description P Possessive I Indicative T Interrogative Q Predicative N Numeral","title":"Adjective type"},{"location":"attributes/type/german/#case","text":"Code Description N Nominative G Genitive D Dative A Accusative","title":"Case"},{"location":"attributes/type/german/#tense","text":"Code Mood Tense indicative Indikativ / subjunctive Konjunktiv I / conditional Konjunktiv II / imperative Imperativ / infinitive Infinitiv / participle Partizip / present_indicative Indikativ Pr\u00e4sens present_perfect_indicative Indikativ Perfekt imperfect_indicative Indikativ Pr\u00e4teritum past_perfect_indicative Indikativ Plusquamperfekt future_indicative Indikativ Futur I future perfect_indicative Indikativ Futur II present_subjunctive Konjunktiv I Pr\u00e4sens perfect_subjunctive Konjunktiv I Perfekt future_subjunctive Konjunktiv I Futur I future_perfect_subjunctive Konjunktiv I Futur II present_conditional Konjunktiv II Pr\u00e4teritum past_conditional Konjunktiv II Plusquamperfekt future_conditional Konjunktiv II Futur I future_perfect_conditional Konjunktiv II Futur II present_imperative Imperativ / present_infinitive Infinitiv Pr\u00e4sens past_infinitive Infinitiv Perfekt present_participle Partizip Pr\u00e4sens past_participle Partizip Perfekt","title":"Tense"},{"location":"attributes/type/german/#person","text":"Code Description 1 First person singular 2 Second person singular 3 Third person singular 4 First person plural 5 Second person plural 6 Third person plural","title":"Person"},{"location":"attributes/type/german/#form","text":"Code Description B Positive C Comparative S Superlative","title":"Form"},{"location":"attributes/type/german/#passive","text":"Code Description P Passive A Active","title":"Passive"},{"location":"attributes/type/german/#conjunction-type","text":"Code Description S Subordinate clause C Coordinate clause","title":"Conjunction type"},{"location":"attributes/type/german/#adverb-type","text":"Code Description P Particle V Verb-particle T Interrogative R Pronominal C Conjunctive A Simple","title":"Adverb type"},{"location":"attributes/type/german/#pronoun-type","text":"Code Description P Personal S Reflexive R Relative T Interrogative A Other","title":"Pronoun type"},{"location":"attributes/type/italian/","text":"Sub-attributes - Italian The features of word classes and entity types the disambiguator can identify for the Italian language are: Word class or entity type Features ART Gender, number PRE Gender, number NOU Gender, number NPR Gender, number NPH Gender, number ENT Gender, number ADJ Gender, number, adjective type, grade VER Tense, person, form PRO Gender, number, person, pronoun type, reflexive pronoun CON Conjunction type AUX Auxiliary type Values codes for each grammatical feature are listed in the following tables. Gender Code Description M Male F Female Number Code Description S Singular P Plural Adjective type Code Description p Possessive i Demonstrative x Descriptive which can act as an adverb (-mente form) q Descriptive a Other n Quantitative c Plural number Tense Code Mood Tense indicativo Indicativo / congiuntivo Congiuntivo / condizionale Condizionale / imperativo Imperativo / infinito Infinito / participio Participio / gerundio Gerundio / indicativo_presente Indicativo Presente indicativo_passato_prossimo Indicativo Passato prossimo indicativo_imperfetto Indicativo Imperfetto indicativo_trapassato_prossimo Indicativo Trapassato prossimo indicativo_passato_remoto Indicativo Passato remoto indicativo_trapassato_remoto Indicativo Trapassato remoto indicativo_futuro_semplice Indicativo Futuro semplice indicativo_futuro_anteriore Indicativo Futuro anteriore congiuntivo_presente Congiuntivo Presente congiuntivo_passato Congiuntivo Passato congiuntivo_imperfetto Congiuntivo Imperfetto congiuntivo_trapassato Congiuntivo Trapassato condizionale_presente Condizionale Presente condizionale_passato Condizionale Passato imperativo_presente Imperativo Presente infinito_presente Infinito Presente infinito_passato Infinito Passato participio_presente Participio Presente participio_passato Participio Passato gerundio_presente Gerundio Presente gerundio_passato Gerundio Passato Person Code Description 1 First person singular 2 Second person singular 3 Third person singular 4 First person plural 5 Second person plural 6 Third person plural 7 First, second or third person singular (when identical) 8 First or second person singular (when identical) Form Code Description negativo Negative riflessivo Reflexive passivo Passive Grade Code Description B Positive A Absolute superlative s Relative superlative G Majority comparative m Minority comparative E Equality comparative Conjunction type Code Description s Subordinate clause g It requires conjunctive i It requires indicative v It accepts both conjunctive and indicative c Coordinate clause Pronoun type Code Description s Only subject p Pronominal particle u Complement/subject v Particular pronoun complement a Other t One between: lo, la, gli, le Reflexive pronoun Code Description s Reflexive n Not reflexive r Pronominal particle of the reflexive Auxiliary type Code Description a Auxiliary m Modal s Aspectual","title":"Sub-attributes - Italian"},{"location":"attributes/type/italian/#sub-attributes-italian","text":"The features of word classes and entity types the disambiguator can identify for the Italian language are: Word class or entity type Features ART Gender, number PRE Gender, number NOU Gender, number NPR Gender, number NPH Gender, number ENT Gender, number ADJ Gender, number, adjective type, grade VER Tense, person, form PRO Gender, number, person, pronoun type, reflexive pronoun CON Conjunction type AUX Auxiliary type Values codes for each grammatical feature are listed in the following tables.","title":"Sub-attributes - Italian"},{"location":"attributes/type/italian/#gender","text":"Code Description M Male F Female","title":"Gender"},{"location":"attributes/type/italian/#number","text":"Code Description S Singular P Plural","title":"Number"},{"location":"attributes/type/italian/#adjective-type","text":"Code Description p Possessive i Demonstrative x Descriptive which can act as an adverb (-mente form) q Descriptive a Other n Quantitative c Plural number","title":"Adjective type"},{"location":"attributes/type/italian/#tense","text":"Code Mood Tense indicativo Indicativo / congiuntivo Congiuntivo / condizionale Condizionale / imperativo Imperativo / infinito Infinito / participio Participio / gerundio Gerundio / indicativo_presente Indicativo Presente indicativo_passato_prossimo Indicativo Passato prossimo indicativo_imperfetto Indicativo Imperfetto indicativo_trapassato_prossimo Indicativo Trapassato prossimo indicativo_passato_remoto Indicativo Passato remoto indicativo_trapassato_remoto Indicativo Trapassato remoto indicativo_futuro_semplice Indicativo Futuro semplice indicativo_futuro_anteriore Indicativo Futuro anteriore congiuntivo_presente Congiuntivo Presente congiuntivo_passato Congiuntivo Passato congiuntivo_imperfetto Congiuntivo Imperfetto congiuntivo_trapassato Congiuntivo Trapassato condizionale_presente Condizionale Presente condizionale_passato Condizionale Passato imperativo_presente Imperativo Presente infinito_presente Infinito Presente infinito_passato Infinito Passato participio_presente Participio Presente participio_passato Participio Passato gerundio_presente Gerundio Presente gerundio_passato Gerundio Passato","title":"Tense"},{"location":"attributes/type/italian/#person","text":"Code Description 1 First person singular 2 Second person singular 3 Third person singular 4 First person plural 5 Second person plural 6 Third person plural 7 First, second or third person singular (when identical) 8 First or second person singular (when identical)","title":"Person"},{"location":"attributes/type/italian/#form","text":"Code Description negativo Negative riflessivo Reflexive passivo Passive","title":"Form"},{"location":"attributes/type/italian/#grade","text":"Code Description B Positive A Absolute superlative s Relative superlative G Majority comparative m Minority comparative E Equality comparative","title":"Grade"},{"location":"attributes/type/italian/#conjunction-type","text":"Code Description s Subordinate clause g It requires conjunctive i It requires indicative v It accepts both conjunctive and indicative c Coordinate clause","title":"Conjunction type"},{"location":"attributes/type/italian/#pronoun-type","text":"Code Description s Only subject p Pronominal particle u Complement/subject v Particular pronoun complement a Other t One between: lo, la, gli, le","title":"Pronoun type"},{"location":"attributes/type/italian/#reflexive-pronoun","text":"Code Description s Reflexive n Not reflexive r Pronominal particle of the reflexive","title":"Reflexive pronoun"},{"location":"attributes/type/italian/#auxiliary-type","text":"Code Description a Auxiliary m Modal s Aspectual","title":"Auxiliary type"},{"location":"attributes/type/spanish/","text":"Sub-attributes - Spanish The features of word classes and entity types the disambiguator can identify for the Spanish language are: Word class or entity type Features ART Gender, number PRE Gender, number NOU Gender, number NPR Gender, number NPH Gender, number ENT Gender, number ADJ Gender, number, adjective type, grade VER Tense, person, form PRO Gender, number, person, pronoun type, reflexive pronoun CON Conjunction type AUX Auxiliary type Values codes for each grammatical feature are listed in the following tables. Gender Code Description M Male F Female Number Code Description S Singular P Plural Adjective type Code Description p Possessive i Demonstrative q Descriptive a Other n Quantitative c Plural number Tense Code Mood Tense indicative Indicativo / subjunctive Subjuntivo / imperative Imperativo / infinitive Infinitivo / participle Participio / gerund Gerundio / present_indicative Indicativo Presente present_perfect_indicative Indicativo Pret\u00e9rito perfecto compuesto imperfect_indicative Indicativo Pret\u00e9rito imperfecto past_perfect_indicative Indicativo Pret\u00e9rito pluscuamperfecto simple_past_indicative Indicativo Pret\u00e9rito perfecto simple past_anterior_indicative Indicativo Pret\u00e9rito anterior future_indicative Indicativo Futuro future_perfect_indicative Indicativo Futuro perfecto present_subjunctive Subjuntivo Presente perfect_subjunctive Subjuntivo Pret\u00e9rito perfecto imperfect_subjunctive Subjuntivo Pret\u00e9rito imperfecto past_perfect_subjunctive Subjuntivo Pret\u00e9rito pluscuamperfecto future_subjunctive Subjuntivo Futuro future_perfect_subjunctive Subjuntivo Futuro perfecto present_conditional Indicativo Condicional past_conditional Indicativo Condicional perfecto present_imperative Imperativo / present_infinitive Infinitivo / past_infinitive Infinitivo Compuesto past_participle Participio Pasado simple_gerund Gerundio / past_gerund Gerundio Compuesto Person Code Description 1 First person singular 2 Second person singular 3 Third person singular 4 First person plural 5 Second person plural 6 Third person plural 7 First, second or third person singular (when identical) 8 First or second person singular (when identical) Form Code Description negative Negative reflexive Reflexive passive Passive Grade Code Description B Positive A Absolute superlative s Relative superlative G Majority comparative m Minority comparative E Equality comparative Conjunction type Code Description s Subordinate clause g It requires conjunctive i It requires indicative v It accepts both conjunctive and indicative c Coordinate clause Pronoun type Code Description s Only subject p Pronominal particle u Complement/subject v Particular pronoun complement a Other Reflexive pronoun Code Description s Reflexive n Not reflexive r Pronominal particle of the reflexive Auxiliary type Code Description a Auxiliary m Modal s Aspectual","title":"Sub-attributes - Spanish"},{"location":"attributes/type/spanish/#sub-attributes-spanish","text":"The features of word classes and entity types the disambiguator can identify for the Spanish language are: Word class or entity type Features ART Gender, number PRE Gender, number NOU Gender, number NPR Gender, number NPH Gender, number ENT Gender, number ADJ Gender, number, adjective type, grade VER Tense, person, form PRO Gender, number, person, pronoun type, reflexive pronoun CON Conjunction type AUX Auxiliary type Values codes for each grammatical feature are listed in the following tables.","title":"Sub-attributes - Spanish"},{"location":"attributes/type/spanish/#gender","text":"Code Description M Male F Female","title":"Gender"},{"location":"attributes/type/spanish/#number","text":"Code Description S Singular P Plural","title":"Number"},{"location":"attributes/type/spanish/#adjective-type","text":"Code Description p Possessive i Demonstrative q Descriptive a Other n Quantitative c Plural number","title":"Adjective type"},{"location":"attributes/type/spanish/#tense","text":"Code Mood Tense indicative Indicativo / subjunctive Subjuntivo / imperative Imperativo / infinitive Infinitivo / participle Participio / gerund Gerundio / present_indicative Indicativo Presente present_perfect_indicative Indicativo Pret\u00e9rito perfecto compuesto imperfect_indicative Indicativo Pret\u00e9rito imperfecto past_perfect_indicative Indicativo Pret\u00e9rito pluscuamperfecto simple_past_indicative Indicativo Pret\u00e9rito perfecto simple past_anterior_indicative Indicativo Pret\u00e9rito anterior future_indicative Indicativo Futuro future_perfect_indicative Indicativo Futuro perfecto present_subjunctive Subjuntivo Presente perfect_subjunctive Subjuntivo Pret\u00e9rito perfecto imperfect_subjunctive Subjuntivo Pret\u00e9rito imperfecto past_perfect_subjunctive Subjuntivo Pret\u00e9rito pluscuamperfecto future_subjunctive Subjuntivo Futuro future_perfect_subjunctive Subjuntivo Futuro perfecto present_conditional Indicativo Condicional past_conditional Indicativo Condicional perfecto present_imperative Imperativo / present_infinitive Infinitivo / past_infinitive Infinitivo Compuesto past_participle Participio Pasado simple_gerund Gerundio / past_gerund Gerundio Compuesto","title":"Tense"},{"location":"attributes/type/spanish/#person","text":"Code Description 1 First person singular 2 Second person singular 3 Third person singular 4 First person plural 5 Second person plural 6 Third person plural 7 First, second or third person singular (when identical) 8 First or second person singular (when identical)","title":"Person"},{"location":"attributes/type/spanish/#form","text":"Code Description negative Negative reflexive Reflexive passive Passive","title":"Form"},{"location":"attributes/type/spanish/#grade","text":"Code Description B Positive A Absolute superlative s Relative superlative G Majority comparative m Minority comparative E Equality comparative","title":"Grade"},{"location":"attributes/type/spanish/#conjunction-type","text":"Code Description s Subordinate clause g It requires conjunctive i It requires indicative v It accepts both conjunctive and indicative c Coordinate clause","title":"Conjunction type"},{"location":"attributes/type/spanish/#pronoun-type","text":"Code Description s Only subject p Pronominal particle u Complement/subject v Particular pronoun complement a Other","title":"Pronoun type"},{"location":"attributes/type/spanish/#reflexive-pronoun","text":"Code Description s Reflexive n Not reflexive r Pronominal particle of the reflexive","title":"Reflexive pronoun"},{"location":"attributes/type/spanish/#auxiliary-type","text":"Code Description a Auxiliary m Modal s Aspectual","title":"Auxiliary type"},{"location":"categorization/","text":"Categorization peculiarities This section describes the peculiarities of the rules language regarding the categorization task. As written in the introduction, categorization consists in determining what a document is about and the possible domains (the categories) to choose from those indicated in the taxonomy . All categorization projects include a taxonomy, the latter containing all of the domains of a given project. Note Taxonomies do not apply to extraction tasks. To make a comparison with the \"spotter cards\" mentioned in the introduction, the taxonomy must contain \"the names of all the planes\" that could potentially be be identified. For example, here is a possible taxonomy of a project in which the engine is required to categorize news about a professional basketball association such as the NBA: CONFERENCE EasternConference WesternConference SEASON Regular Playoffs Finals FOUL PersonalFoul FlagrantFoul TechnicalFoul Money OTHERNEWS Awards MVP DefensivePlayer RookiePlayer TopScorer Retirement CoachingChanges NBADraft The taxonomy can be considered as a hierarchical tree structure; in fact it is also called \"domain tree\". It usually reflects all or part of a knowledge domain, hence the usefulness of the hierarchical structure. However, it is also possible to define a flat taxonomy that is a list of non-interdependent elements. In a project, the taxonomy is not defined within the rules language source code , rather, it is an external data structure which is defined using the graphical development tool. Each domain has a unique name , in other words, there can not be two or more domains with the same name within the same taxonomy. While each categorization rule refers to a domain by its name, the domains also have an optional description. If triggered, a categorization rule will attribute a certain amount of points (score) to the domain to which it is associated. The categories which receive the most points are considered the \"winners\". The output resulting from an input document processed by a categorization engine will consist in one or more domains and their corresponding scores. The following topics cover the relationship between rules and domains as well as the domain scoring mechanism.","title":"Categorization peculiarities"},{"location":"categorization/#categorization-peculiarities","text":"This section describes the peculiarities of the rules language regarding the categorization task. As written in the introduction, categorization consists in determining what a document is about and the possible domains (the categories) to choose from those indicated in the taxonomy . All categorization projects include a taxonomy, the latter containing all of the domains of a given project. Note Taxonomies do not apply to extraction tasks. To make a comparison with the \"spotter cards\" mentioned in the introduction, the taxonomy must contain \"the names of all the planes\" that could potentially be be identified. For example, here is a possible taxonomy of a project in which the engine is required to categorize news about a professional basketball association such as the NBA: CONFERENCE EasternConference WesternConference SEASON Regular Playoffs Finals FOUL PersonalFoul FlagrantFoul TechnicalFoul Money OTHERNEWS Awards MVP DefensivePlayer RookiePlayer TopScorer Retirement CoachingChanges NBADraft The taxonomy can be considered as a hierarchical tree structure; in fact it is also called \"domain tree\". It usually reflects all or part of a knowledge domain, hence the usefulness of the hierarchical structure. However, it is also possible to define a flat taxonomy that is a list of non-interdependent elements. In a project, the taxonomy is not defined within the rules language source code , rather, it is an external data structure which is defined using the graphical development tool. Each domain has a unique name , in other words, there can not be two or more domains with the same name within the same taxonomy. While each categorization rule refers to a domain by its name, the domains also have an optional description. If triggered, a categorization rule will attribute a certain amount of points (score) to the domain to which it is associated. The categories which receive the most points are considered the \"winners\". The output resulting from an input document processed by a categorization engine will consist in one or more domains and their corresponding scores. The following topics cover the relationship between rules and domains as well as the domain scoring mechanism.","title":"Categorization peculiarities"},{"location":"categorization/rules/","text":"Categorization rules syntax The typical syntax of a categorization rule is: DOMAIN[[rule label]](domain[:score option]) { condition } Parts between brackets ( [...] ) are optional. DOMAIN is a language keyword and must be written in uppercase. rule label is a label that helps identify the rule. domain is the name of one of the domains in the taxonomy. score option is the name of one of the available score options that determines the amount of \"points\" that are added to the overall domain score every time the rule is triggered. The default is NORMAL . condition is the rule's condition . Rules of this kind, when they activate, add points to the score of the domain specified in the header. Every rule must be contained in a scope specifier: SCOPE scope { DOMAIN[[rule label]](domain[:score option]) { condition } } For example, the rule: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { LEMMA(\"abandon\") AND LEMMA(\"oil well\") } } is triggered when lemma abandon and lemma oil well are found in the same sentence. When triggered, it generates a NORMAL amount of points and adds them to the cumulative score of the dom1 domain. It is also possible to define categorization rules that affect the score of more domains at once using this syntax: DOMAIN[[rule label]](domain#1[:score option], domain#2[:score option],... , domain#n[:score option]) { condition } For example, the rule: SCOPE SENTENCE { DOMAIN(dom1:NORMAL, dom2:LOW, dom3:HIGH) { LEMMA(\"abandon\") AND LEMMA(\"oil well\") } } affects the score of the domain dom1 with the NORMAL score option, the domain dom2 with the LOW score option and the domain dom3 with the HIGH score option. More rules can be put inside the same scope specifier: SCOPE scope { //Rule #1 DOMAIN[[rule label]](domain[:score option]]) { condition } //Rule #2 DOMAIN[[rule]](domain[:score option]) { condition } ... }","title":"Categorization rules syntax"},{"location":"categorization/rules/#categorization-rules-syntax","text":"The typical syntax of a categorization rule is: DOMAIN[[rule label]](domain[:score option]) { condition } Parts between brackets ( [...] ) are optional. DOMAIN is a language keyword and must be written in uppercase. rule label is a label that helps identify the rule. domain is the name of one of the domains in the taxonomy. score option is the name of one of the available score options that determines the amount of \"points\" that are added to the overall domain score every time the rule is triggered. The default is NORMAL . condition is the rule's condition . Rules of this kind, when they activate, add points to the score of the domain specified in the header. Every rule must be contained in a scope specifier: SCOPE scope { DOMAIN[[rule label]](domain[:score option]) { condition } } For example, the rule: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { LEMMA(\"abandon\") AND LEMMA(\"oil well\") } } is triggered when lemma abandon and lemma oil well are found in the same sentence. When triggered, it generates a NORMAL amount of points and adds them to the cumulative score of the dom1 domain. It is also possible to define categorization rules that affect the score of more domains at once using this syntax: DOMAIN[[rule label]](domain#1[:score option], domain#2[:score option],... , domain#n[:score option]) { condition } For example, the rule: SCOPE SENTENCE { DOMAIN(dom1:NORMAL, dom2:LOW, dom3:HIGH) { LEMMA(\"abandon\") AND LEMMA(\"oil well\") } } affects the score of the domain dom1 with the NORMAL score option, the domain dom2 with the LOW score option and the domain dom3 with the HIGH score option. More rules can be put inside the same scope specifier: SCOPE scope { //Rule #1 DOMAIN[[rule label]](domain[:score option]]) { condition } //Rule #2 DOMAIN[[rule]](domain[:score option]) { condition } ... }","title":"Categorization rules syntax"},{"location":"categorization/rules/domain/","text":"Domain name The part of a categorization rule that associates it to its domain is the DOMAIN instruction: DOMAIN(domain_name) DOMAIN is a keyword that must be written in uppercase, whereas domain_name is the exact name of the domain defined in the project's taxonomy to which the rule has to be associated. For example, given the following taxonomy: CONFERENCE EasternConference WesternConference SEASON Regular Playoffs Finals FOUL PersonalFoul FlagrantFoul TechnicalFoul Money OTHERNEWS Awards MVP DefensivePlayer RookiePlayer TopScorer Retirement CoachingChanges NBADraft a rule like this: SCOPE scope_option { DOMAIN(EasternConference) { //condition// } } states that domain EasternConference will receive a certain amount of \"points\", becoming a candidate output category, whenever the rule's condition matches the input text. The exact amount of points the domain receives when the rule is triggered is determined by the optional score option. Restrictions on domain names The categorization domain naming convention allows the use of the hyphen ( - ) neither in the domain name in the taxonomy nor within the rules. This is an example of a valid declaration: <DOMAINTREE> <DOMAIN NAME=\"dom1\" DESCRIPTION=\"Sport\"/> </DOMAINTREE> And this is an example of an invalid declaration: <DOMAINTREE> <DOMAIN NAME=\"dom-1\" DESCRIPTION=\"Sport\"/> </DOMAINTREE> Moreover, when declaring the domain name in a rule, if the domain name can be interpreted as a number, it will have to be declared in quotes. These are example of valid declarations: DOMAIN(\"00001\") DOMAIN(\"00001.01\") DOMAIN(domain1) Instead, these declarations are not valid: DOMAIN(00001.01) DOMAIN(00001) It is possible to have domain names end with a period (.), however this practice is strongly discouraged.","title":"Domain name"},{"location":"categorization/rules/domain/#domain-name","text":"The part of a categorization rule that associates it to its domain is the DOMAIN instruction: DOMAIN(domain_name) DOMAIN is a keyword that must be written in uppercase, whereas domain_name is the exact name of the domain defined in the project's taxonomy to which the rule has to be associated. For example, given the following taxonomy: CONFERENCE EasternConference WesternConference SEASON Regular Playoffs Finals FOUL PersonalFoul FlagrantFoul TechnicalFoul Money OTHERNEWS Awards MVP DefensivePlayer RookiePlayer TopScorer Retirement CoachingChanges NBADraft a rule like this: SCOPE scope_option { DOMAIN(EasternConference) { //condition// } } states that domain EasternConference will receive a certain amount of \"points\", becoming a candidate output category, whenever the rule's condition matches the input text. The exact amount of points the domain receives when the rule is triggered is determined by the optional score option.","title":"Domain name"},{"location":"categorization/rules/domain/#restrictions-on-domain-names","text":"The categorization domain naming convention allows the use of the hyphen ( - ) neither in the domain name in the taxonomy nor within the rules. This is an example of a valid declaration: <DOMAINTREE> <DOMAIN NAME=\"dom1\" DESCRIPTION=\"Sport\"/> </DOMAINTREE> And this is an example of an invalid declaration: <DOMAINTREE> <DOMAIN NAME=\"dom-1\" DESCRIPTION=\"Sport\"/> </DOMAINTREE> Moreover, when declaring the domain name in a rule, if the domain name can be interpreted as a number, it will have to be declared in quotes. These are example of valid declarations: DOMAIN(\"00001\") DOMAIN(\"00001.01\") DOMAIN(domain1) Instead, these declarations are not valid: DOMAIN(00001.01) DOMAIN(00001) It is possible to have domain names end with a period (.), however this practice is strongly discouraged.","title":"Restrictions on domain names"},{"location":"categorization/rules/score/","text":"Domain score Overview At the beginning of the categorization process, every domain defined in the project's taxonomy has a score of zero. During the process, every time a rule is activated, the score of its domain is increased 1 by a certain amount of points. At the end of the categorization process, some domains may then have a positive score and be returned as output categories. It is possible that an input text does not trigger any rule (and therefore, no categories are returned), either because the text is not related to any of the domains, or because the rules are incomplete or not well-designed. Post-processing scripts can be used to filter the output categories, for example to keep only those with the highest scores. Score options The amount of points a domain receives depends on several factors, such as: The type and the structure of the condition. The portion of document in which the condition is met. The amount of text that's matched. The rule's score option . The score option syntax is: DOMAIN(domain_name:score_option) The score option is the fundamental variable in the calculation of points. In the simplest cases, the option alone determines the exact amount of points given to the domain when the rule is triggered, while in more complex cases it is combined with other variables. Read about the scoring mechanism to know more about how the other variables are used to compute points. Standard options The standard score options are listed in the following table. Option Description Points NORMAL The default/implicit score option 10 LOW Lower than the default 3 HIGH Higher than the default 15 SELECT Forces a domain in the categorization output A large positive number DISCARD Forces a domain out of the categorization output A large negative number The first three options assign three slightly different amounts of points. They are useful to give more or less relevance to some rules. When deciding between these options, the rule of thumb is: Do not specify any score option (or specify NORMAL , which is equivalent, if you prefer to have explicit options in your source code) in most cases. Use the HIGH option to attribute more weight to few, very selective rules, where \"selective\" means a rule with a condition made to match text that is very specific of the domain. When triggered, these rules will boost a domain's score. Use the LOW option for \"weak\" rules, where \"weak\" means a rule with a condition which is not specific of a certain domain. When triggered, a weak rule has a negligible impact on the overall domain score. However, if several of these rules trigger, then their impact will become significant because many tiny clues are like one big clue. The last two options influence the domain score so much that the domain either becomes a guaranteed winner or it's removed from the categorization results. The SELECT score option has an even greater impact, because every time a rule with this option generates a hit, the domain to which it belongs is automatically inserted into the highest ranking output. On the other hand, every time that a rule containing a DISCARD score option generates a hit, the domain to which the rule belongs is automatically discarded from the output, even if the domain has a positive score due to other rules. Using the DISCARD score option is like defining a \"negative rule\" for a domain because it is invalidated when the rule's condition is met. Custom options It is possible to create custom score options. They can be defined in the config.cr file. The syntax is: SCORES { @score_option_name:points, ... } For example: SCORES { @LOWER:1, @HIGHER:20 } Once defined, the names of the new options can be used in the categorization rules thus providing greater variability of the rules' scores. With the exception of the DISCARD score option that corresponds to a large negative amount. \u21a9","title":"Domain score"},{"location":"categorization/rules/score/#domain-score","text":"","title":"Domain score"},{"location":"categorization/rules/score/#overview","text":"At the beginning of the categorization process, every domain defined in the project's taxonomy has a score of zero. During the process, every time a rule is activated, the score of its domain is increased 1 by a certain amount of points. At the end of the categorization process, some domains may then have a positive score and be returned as output categories. It is possible that an input text does not trigger any rule (and therefore, no categories are returned), either because the text is not related to any of the domains, or because the rules are incomplete or not well-designed. Post-processing scripts can be used to filter the output categories, for example to keep only those with the highest scores.","title":"Overview"},{"location":"categorization/rules/score/#score-options","text":"The amount of points a domain receives depends on several factors, such as: The type and the structure of the condition. The portion of document in which the condition is met. The amount of text that's matched. The rule's score option . The score option syntax is: DOMAIN(domain_name:score_option) The score option is the fundamental variable in the calculation of points. In the simplest cases, the option alone determines the exact amount of points given to the domain when the rule is triggered, while in more complex cases it is combined with other variables. Read about the scoring mechanism to know more about how the other variables are used to compute points.","title":"Score options"},{"location":"categorization/rules/score/#standard-options","text":"The standard score options are listed in the following table. Option Description Points NORMAL The default/implicit score option 10 LOW Lower than the default 3 HIGH Higher than the default 15 SELECT Forces a domain in the categorization output A large positive number DISCARD Forces a domain out of the categorization output A large negative number The first three options assign three slightly different amounts of points. They are useful to give more or less relevance to some rules. When deciding between these options, the rule of thumb is: Do not specify any score option (or specify NORMAL , which is equivalent, if you prefer to have explicit options in your source code) in most cases. Use the HIGH option to attribute more weight to few, very selective rules, where \"selective\" means a rule with a condition made to match text that is very specific of the domain. When triggered, these rules will boost a domain's score. Use the LOW option for \"weak\" rules, where \"weak\" means a rule with a condition which is not specific of a certain domain. When triggered, a weak rule has a negligible impact on the overall domain score. However, if several of these rules trigger, then their impact will become significant because many tiny clues are like one big clue. The last two options influence the domain score so much that the domain either becomes a guaranteed winner or it's removed from the categorization results. The SELECT score option has an even greater impact, because every time a rule with this option generates a hit, the domain to which it belongs is automatically inserted into the highest ranking output. On the other hand, every time that a rule containing a DISCARD score option generates a hit, the domain to which the rule belongs is automatically discarded from the output, even if the domain has a positive score due to other rules. Using the DISCARD score option is like defining a \"negative rule\" for a domain because it is invalidated when the rule's condition is met.","title":"Standard options"},{"location":"categorization/rules/score/#custom-options","text":"It is possible to create custom score options. They can be defined in the config.cr file. The syntax is: SCORES { @score_option_name:points, ... } For example: SCORES { @LOWER:1, @HIGHER:20 } Once defined, the names of the new options can be used in the categorization rules thus providing greater variability of the rules' scores. With the exception of the DISCARD score option that corresponds to a large negative amount. \u21a9","title":"Custom options"},{"location":"categorization/rules/scoring-mechanism/","text":"The scoring mechanism The basic algorithm The basic algorithm used to determine the amount of points generated by a categorization rule consists in the amount of points by which the rule domain is increased each time the rule is activated. For each operand of rule's condition: The number of matched tokens is computed, considering that a single KEYWORD or PATTERN attribute can match two or more tokens. The number of tokens is multiplied by the amount associated with the score option ( NORMAL : 10, HIGH : 15, etc.). The amounts computed for all the operands are summed. If the section, in which the rule's condition was met, has a score multiplication factor, the amount of points will be multiplied by that factor. If the rule's condition was met inside a segment and that segment has a score multiplication factor, the amount of points will be multiplied by that factor. A more complex algorithm is used in the case of conditions containing positional sequence operators. KEYWORD attribute contribution A single KEYWORD attribute generates an amount of points that's proportional to the number of matched tokens, and this number, in turn, is influenced by the number of lemmas. For example, in this text: This credit card can help you establish a positive credit history. the disambiguator recognizes credit card and credit history as multi-word lemmas. This rule: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { KEYWORD(\"credit card\") } } is activated by the sample text and generates a NORMAL amount of points (10). This is because credit card corresponds to one token in the disambiguation output. On the other hand, this rule: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { KEYWORD(\"can help\") } } generates the NORMAL amount of points twice\u2014that is, 10 X 2 = 20\u2014because can and help correspond to two lemmas and hence to two tokens. If the rule is changed to: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { KEYWORD(\"credit card can help\") } } the amount of points generated by the KEYWORD attribute will be 30, 20 due to can and help plus 10 due to credit card . If more expressions are specified within the same attribute, as in: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { KEYWORD(\"help\", \"can\") } } the amount of points generated by each rule activation depends on the single expression. In the case of the sample text, the rule is activated twice , the first time because of can and the second because of help . The amount of points generated by the first activation is 10 and the amount generated by the second activation is also 10. The sum of points for the domain is 20, because of the two distinct hits and not due to the single activation as seen in the first sample rule. Boolean combinations A condition made of two or more operands combined with Boolean operators ( AND , OR , AND NOT , XOR ) generates an amount of points proportional to the number of the operands that match. The following rule, for example, always generates a score of 20, 10 for each operand: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { LEMMA(\"tennis\") AND LEMMA(\"match\") } } On the other hand, this rule: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { LEMMA(\"tennis\") OR LEMMA(\"match\") } } generates a variable amount of points: 10 points if only one of the two operands finds a match, 20 points when both operands match. Complex Boolean combinations are deconstructed into simpler expressions. For example, this rule: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { LEMMA(\"tennis\") AND ( LEMMA(\"match\") OR LEMMA(\"series\") ) } } has an inner OR combination of two operands that can generate 10 or 20 points based on the number of operands that match, which can be only one or both. The first operand always generates 10 points, so the total amount of points generated by the rule can be 10 + 10 = 20 or 10 + 20 = 30. Any operand or sub-condition which follows the AND NOT operator will not generate any points . Therefore, every hit from the following rule will generate 20 points, as each of the two operands combined with AND generates 10 points while the third operand does not generate any points. SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { LEMMA(\"tennis\") AND LEMMA(\"match\") AND NOT LEMMA(\"terrorism\") } } Complex condition with positional or logical operators A complex condition\u2014containing more than one operand, co-joined with positional or logical operators\u2014generates a score based on the following variant of the basic algorithm: Positional and logical operators become part of the operands. A condition built with these operators generates the square product of the number of the operands on both sides of operators, multiplied by the score defined in the header of the rule. For example, this rule generates 40 points: SCOPE SENTENCE { // Strict sequence of two one-token operands DOMAIN(dom1:NORMAL) { LEMMA(\"tennis\") >> LEMMA(\"match\") } } The points are computed as follows: Number of operands present on both sides of a positional or logical operator: 2. Square product of the number of operands, on both sides of the positional or logical operator in the rule: 2 * 2 = 4. Square product of the number of the operands multiplied by the score defined in the header of the rule: 4 * 10 = 40. A rule constructed as follows: SCOPE SENTENCE { // Flexible sequence of three one-token operands DOMAIN(dom1:HIGH) { LEMMA(\"member\") <> KEYWORD(\"of\") <> LEMMA(\"House of Commons\") } } has three operands, each which matches one token, and are conjoined by the flexible sequence operator; the score option is set to HIGH , therefore this rule generates 135 points. Here is the breakdown: Number of operands present on both sides of a positional or logical operator: 3. Square product of the number of operands, present on both sides of the positional or logical operator in the rule: 3 * 3. Square product of the number of the operands multiplied by the score defined in the header of the file: 9 * 15 = 135. Complex combinations are decomposed in simpler expressions. Consider for example this rule: SCOPE SENTENCE { // Combination of flexible sequence and Boolean operator DOMAIN(dom1:NORMAL) { LEMMA(\"tennis\") <> LEMMA(\"match\") AND NOT LEMMA(\"terrorism\") } } Every instance of this rule generates 50 points: Number of operands present on both sides of a positional or logical operator: 2. Square product of the number of operands, present on both sides of the positional or logical operator in the rule: 2 * 2. Square product of the number of the operands multiplied by the score defined in the header of the rule: 4 * 10 = 40. Score generated by operand, not included by a positional or logical operator: 10 Sum of the score of all the operands: 40 + 10 = 50. If two tokens disambiguated as one lemma are matched by two different operands, each operand will add its individual value to the final score of the rule, regardless of the disambiguation. The following rule, for example, generates 40 points, because of the two operands co-joined by a positional operator. SCOPE SENTENCE { // Strict sequence of two words of the same lemma. DOMAIN(dom1:NORMAL) { KEYWORD(\"credit\") >> KEYWORD(\"card\") } } Operands following the exclamation mark do not contribute to points generation. The following rule, for example, generates 40 points: SCOPE SENTENCE { //Exclamation mark does not contribute to the score DOMAIN(dom1:NORMAL) { LEMMA(\"tennis\") <> LEMMA(\"match\") >> !LEMMA(\"parliament\") } } Breakdown: Number of operands present on both sides of a positional or logical operator: 2. Square product of the number of operands, present on both sides of the positional or logical operator in the rule: 2 * 2 = 4. Square product of the number of the operands multiplied by the score defined in the header of the rule: 4 * 10 = 40. Score generated by operand, following the exclamation mark: 0. Sum of the two scores of all operands: 40 + 0 = 40. Options Some options affect the scoring mechanism. FIXED_SCORE When this option is set, step one of the algorithm is replaced by: The amount of points is set equal to the amount associated with the rule's score option. In other words, the amount of points before the application of the multiplication factors depends only on the rule's score option (default: NORMAL ), the number of operands and the number of matched tokens no longer have any impact. STATIC_SCORE When this option is set, step one of the algorithm is replaced by: Each operand contributes a fixed amount equal to the amount associated with the rule's score option. In other words, the number of matched tokens is no longer considered. Each operand's contribution to the score will not change in accordance to the length of the matched text, but rather it will be equal to the nominal score which was declared for the rule (default: NORMAL ). CHILD_TO_FATHER When this option is set, if a domain has a parent in the taxonomy, the domain's score will be added to its parent's score. In this way, a parent domain will receive the score of all its children. This is called propagation of score from children to fathers and it is a recursive mechanism, meaning that even a father which is child of a higher domain, will transmit its score (possibly inherited from its children) to its respective father. This option can be useful when high-level domains with no children have many rules and other high-level domains have no rules, but have children with a few rules each. Based on categorization rules only, high-level domains with no children would probably have the best of \"weaker\" subdomains. With this option set, domains with descendants become contenders. For example, suppose that for this taxonomy: environment climate change global warming conservation energy saving parks science and technology some rules are defined for the global warming , energy saving and parks domains and relatively many more rules are defined for the first level domain science and technology . If a text is about technologies with reduced environmental impact, the possible score after the application of the rules could be: environment climate change global warming 10 conservation energy saving 10 parks 10 science and technology 20 In this case, science and technology would win, even if several sub-topics of environment were detected. However, by activating the propagation of the scores from children to fathers, the situation would change as follows: environment 30 climate change 10 global warming 10 conservation 20 energy saving 10 parks 10 science and technology 20 therefore, the environment domain would be the winner. As can be seen, the score of the children domains was propagated to the respective fathers and the score of fathers was in turn propagated to the first level domain, which thus accumulated all the points of its descendants. Expert.ai text intelligence engine also offers a second score, called compound , which can be useful in these cases. In the default setting, its value \"copies\" that of the standard score, so there's no reason to use it. However, when the CHILD_TO_FATHER option is set, this additional score is calculated in a way that amplifies the effects of the propagation from children to fathers. With the option set, after the first step illustrated above, a second step, which affects only the compound score, is performed. In this step, starting from the children domains, the propagation is repeated while taking into account the pre-existing score. Therefore, using the above example, the conservation domain would now be assigned to have 40 points as a result of the sum of the 20 points of its children and its 20 points score calculated in the previous step. Therefore, keeping in mind that the standard score has not changed, the final compound values for the example would be: environment 90 climate change 20 global warming 10 conservation 40 energy saving 10 parks 10 science and technology 20","title":"The scoring mechanism"},{"location":"categorization/rules/scoring-mechanism/#the-scoring-mechanism","text":"","title":"The scoring mechanism"},{"location":"categorization/rules/scoring-mechanism/#the-basic-algorithm","text":"The basic algorithm used to determine the amount of points generated by a categorization rule consists in the amount of points by which the rule domain is increased each time the rule is activated. For each operand of rule's condition: The number of matched tokens is computed, considering that a single KEYWORD or PATTERN attribute can match two or more tokens. The number of tokens is multiplied by the amount associated with the score option ( NORMAL : 10, HIGH : 15, etc.). The amounts computed for all the operands are summed. If the section, in which the rule's condition was met, has a score multiplication factor, the amount of points will be multiplied by that factor. If the rule's condition was met inside a segment and that segment has a score multiplication factor, the amount of points will be multiplied by that factor. A more complex algorithm is used in the case of conditions containing positional sequence operators.","title":"The basic algorithm"},{"location":"categorization/rules/scoring-mechanism/#keyword-attribute-contribution","text":"A single KEYWORD attribute generates an amount of points that's proportional to the number of matched tokens, and this number, in turn, is influenced by the number of lemmas. For example, in this text: This credit card can help you establish a positive credit history. the disambiguator recognizes credit card and credit history as multi-word lemmas. This rule: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { KEYWORD(\"credit card\") } } is activated by the sample text and generates a NORMAL amount of points (10). This is because credit card corresponds to one token in the disambiguation output. On the other hand, this rule: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { KEYWORD(\"can help\") } } generates the NORMAL amount of points twice\u2014that is, 10 X 2 = 20\u2014because can and help correspond to two lemmas and hence to two tokens. If the rule is changed to: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { KEYWORD(\"credit card can help\") } } the amount of points generated by the KEYWORD attribute will be 30, 20 due to can and help plus 10 due to credit card . If more expressions are specified within the same attribute, as in: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { KEYWORD(\"help\", \"can\") } } the amount of points generated by each rule activation depends on the single expression. In the case of the sample text, the rule is activated twice , the first time because of can and the second because of help . The amount of points generated by the first activation is 10 and the amount generated by the second activation is also 10. The sum of points for the domain is 20, because of the two distinct hits and not due to the single activation as seen in the first sample rule.","title":"KEYWORD attribute contribution"},{"location":"categorization/rules/scoring-mechanism/#boolean-combinations","text":"A condition made of two or more operands combined with Boolean operators ( AND , OR , AND NOT , XOR ) generates an amount of points proportional to the number of the operands that match. The following rule, for example, always generates a score of 20, 10 for each operand: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { LEMMA(\"tennis\") AND LEMMA(\"match\") } } On the other hand, this rule: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { LEMMA(\"tennis\") OR LEMMA(\"match\") } } generates a variable amount of points: 10 points if only one of the two operands finds a match, 20 points when both operands match. Complex Boolean combinations are deconstructed into simpler expressions. For example, this rule: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { LEMMA(\"tennis\") AND ( LEMMA(\"match\") OR LEMMA(\"series\") ) } } has an inner OR combination of two operands that can generate 10 or 20 points based on the number of operands that match, which can be only one or both. The first operand always generates 10 points, so the total amount of points generated by the rule can be 10 + 10 = 20 or 10 + 20 = 30. Any operand or sub-condition which follows the AND NOT operator will not generate any points . Therefore, every hit from the following rule will generate 20 points, as each of the two operands combined with AND generates 10 points while the third operand does not generate any points. SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { LEMMA(\"tennis\") AND LEMMA(\"match\") AND NOT LEMMA(\"terrorism\") } }","title":"Boolean combinations"},{"location":"categorization/rules/scoring-mechanism/#complex-condition-with-positional-or-logical-operators","text":"A complex condition\u2014containing more than one operand, co-joined with positional or logical operators\u2014generates a score based on the following variant of the basic algorithm: Positional and logical operators become part of the operands. A condition built with these operators generates the square product of the number of the operands on both sides of operators, multiplied by the score defined in the header of the rule. For example, this rule generates 40 points: SCOPE SENTENCE { // Strict sequence of two one-token operands DOMAIN(dom1:NORMAL) { LEMMA(\"tennis\") >> LEMMA(\"match\") } } The points are computed as follows: Number of operands present on both sides of a positional or logical operator: 2. Square product of the number of operands, on both sides of the positional or logical operator in the rule: 2 * 2 = 4. Square product of the number of the operands multiplied by the score defined in the header of the rule: 4 * 10 = 40. A rule constructed as follows: SCOPE SENTENCE { // Flexible sequence of three one-token operands DOMAIN(dom1:HIGH) { LEMMA(\"member\") <> KEYWORD(\"of\") <> LEMMA(\"House of Commons\") } } has three operands, each which matches one token, and are conjoined by the flexible sequence operator; the score option is set to HIGH , therefore this rule generates 135 points. Here is the breakdown: Number of operands present on both sides of a positional or logical operator: 3. Square product of the number of operands, present on both sides of the positional or logical operator in the rule: 3 * 3. Square product of the number of the operands multiplied by the score defined in the header of the file: 9 * 15 = 135. Complex combinations are decomposed in simpler expressions. Consider for example this rule: SCOPE SENTENCE { // Combination of flexible sequence and Boolean operator DOMAIN(dom1:NORMAL) { LEMMA(\"tennis\") <> LEMMA(\"match\") AND NOT LEMMA(\"terrorism\") } } Every instance of this rule generates 50 points: Number of operands present on both sides of a positional or logical operator: 2. Square product of the number of operands, present on both sides of the positional or logical operator in the rule: 2 * 2. Square product of the number of the operands multiplied by the score defined in the header of the rule: 4 * 10 = 40. Score generated by operand, not included by a positional or logical operator: 10 Sum of the score of all the operands: 40 + 10 = 50. If two tokens disambiguated as one lemma are matched by two different operands, each operand will add its individual value to the final score of the rule, regardless of the disambiguation. The following rule, for example, generates 40 points, because of the two operands co-joined by a positional operator. SCOPE SENTENCE { // Strict sequence of two words of the same lemma. DOMAIN(dom1:NORMAL) { KEYWORD(\"credit\") >> KEYWORD(\"card\") } } Operands following the exclamation mark do not contribute to points generation. The following rule, for example, generates 40 points: SCOPE SENTENCE { //Exclamation mark does not contribute to the score DOMAIN(dom1:NORMAL) { LEMMA(\"tennis\") <> LEMMA(\"match\") >> !LEMMA(\"parliament\") } } Breakdown: Number of operands present on both sides of a positional or logical operator: 2. Square product of the number of operands, present on both sides of the positional or logical operator in the rule: 2 * 2 = 4. Square product of the number of the operands multiplied by the score defined in the header of the rule: 4 * 10 = 40. Score generated by operand, following the exclamation mark: 0. Sum of the two scores of all operands: 40 + 0 = 40.","title":"Complex condition with positional or logical operators"},{"location":"categorization/rules/scoring-mechanism/#options","text":"Some options affect the scoring mechanism.","title":"Options"},{"location":"categorization/rules/scoring-mechanism/#fixed_score","text":"When this option is set, step one of the algorithm is replaced by: The amount of points is set equal to the amount associated with the rule's score option. In other words, the amount of points before the application of the multiplication factors depends only on the rule's score option (default: NORMAL ), the number of operands and the number of matched tokens no longer have any impact.","title":"FIXED_SCORE"},{"location":"categorization/rules/scoring-mechanism/#static_score","text":"When this option is set, step one of the algorithm is replaced by: Each operand contributes a fixed amount equal to the amount associated with the rule's score option. In other words, the number of matched tokens is no longer considered. Each operand's contribution to the score will not change in accordance to the length of the matched text, but rather it will be equal to the nominal score which was declared for the rule (default: NORMAL ).","title":"STATIC_SCORE"},{"location":"categorization/rules/scoring-mechanism/#child_to_father","text":"When this option is set, if a domain has a parent in the taxonomy, the domain's score will be added to its parent's score. In this way, a parent domain will receive the score of all its children. This is called propagation of score from children to fathers and it is a recursive mechanism, meaning that even a father which is child of a higher domain, will transmit its score (possibly inherited from its children) to its respective father. This option can be useful when high-level domains with no children have many rules and other high-level domains have no rules, but have children with a few rules each. Based on categorization rules only, high-level domains with no children would probably have the best of \"weaker\" subdomains. With this option set, domains with descendants become contenders. For example, suppose that for this taxonomy: environment climate change global warming conservation energy saving parks science and technology some rules are defined for the global warming , energy saving and parks domains and relatively many more rules are defined for the first level domain science and technology . If a text is about technologies with reduced environmental impact, the possible score after the application of the rules could be: environment climate change global warming 10 conservation energy saving 10 parks 10 science and technology 20 In this case, science and technology would win, even if several sub-topics of environment were detected. However, by activating the propagation of the scores from children to fathers, the situation would change as follows: environment 30 climate change 10 global warming 10 conservation 20 energy saving 10 parks 10 science and technology 20 therefore, the environment domain would be the winner. As can be seen, the score of the children domains was propagated to the respective fathers and the score of fathers was in turn propagated to the first level domain, which thus accumulated all the points of its descendants. Expert.ai text intelligence engine also offers a second score, called compound , which can be useful in these cases. In the default setting, its value \"copies\" that of the standard score, so there's no reason to use it. However, when the CHILD_TO_FATHER option is set, this additional score is calculated in a way that amplifies the effects of the propagation from children to fathers. With the option set, after the first step illustrated above, a second step, which affects only the compound score, is performed. In this step, starting from the children domains, the propagation is repeated while taking into account the pre-existing score. Therefore, using the above example, the conservation domain would now be assigned to have 40 points as a result of the sum of the 20 points of its children and its 20 points score calculated in the previous step. Therefore, keeping in mind that the standard score has not changed, the final compound values for the example would be: environment 90 climate change 20 global warming 10 conservation 40 energy saving 10 parks 10 science and technology 20","title":"CHILD_TO_FATHER"},{"location":"condition/","text":"Condition overview The condition of an extraction or categorization rule is the declaration of the features the disambiguation output must have in order for the rule to be activated: when the condition is met, the rule is triggered. A condition can be as simple as a single operand composed of an attribute with one value or it can be a complex combination of sub-conditions and operands composed of combined attributes. For example, this is a one operand, one attribute, single value condition: LEMMA(\"dog\") Below there is a Boolean combination of a combined attributes operand, a single attribute operand and a sub-condition which, in turn, is a Boolean combination of a positional sequence and a multi-valued single-attribute operand: ANCESTOR(12828) /*12828: plane, aeroplane, airplane*/ -SYNCON(UNKNOWN) AND LEMMA(\"hijack\") AND NOT ( LEMMA (\"fiction\") > LEMMA(\"story\") OR ANCESTOR(30419, 30451)// 30419: film, movie // 30451: television show, television program )","title":"Condition overview"},{"location":"condition/#condition-overview","text":"The condition of an extraction or categorization rule is the declaration of the features the disambiguation output must have in order for the rule to be activated: when the condition is met, the rule is triggered. A condition can be as simple as a single operand composed of an attribute with one value or it can be a complex combination of sub-conditions and operands composed of combined attributes. For example, this is a one operand, one attribute, single value condition: LEMMA(\"dog\") Below there is a Boolean combination of a combined attributes operand, a single attribute operand and a sub-condition which, in turn, is a Boolean combination of a positional sequence and a multi-valued single-attribute operand: ANCESTOR(12828) /*12828: plane, aeroplane, airplane*/ -SYNCON(UNKNOWN) AND LEMMA(\"hijack\") AND NOT ( LEMMA (\"fiction\") > LEMMA(\"story\") OR ANCESTOR(30419, 30451)// 30419: film, movie // 30451: television show, television program )","title":"Condition overview"},{"location":"condition/expressions/","text":"Condition expressions and sub-expressions Within extraction or categorization rules, an expression is a complex linguistic condition composed, in its most basic form, of attributes and operators: SCOPE scope_option { DOMAIN(domain_name:score_option)|IDENTIFY(template_name) { Attribute1 Operator Attribute2 ... } } For further information about the specific features of an extraction rule and how these are integrated into the above generic syntax see the related pages . However, advanced rule-writing can optionally imply a particular usage of attributes and operators and the awareness of a priority scale prescribing the use of Boolean operators as seen in the following table: Operator Priority AND NOT 1 XOR 2 AND 3 OR 4 This means that, in a rule containing more than one Boolean operator, the order in which the elements of the expression are evaluated starts from the operator with the highest priority and proceeds to those with lower priority. For example, a condition such as the following: LEMMA(\"flight\") OR LEMMA (\"plane\") AND LEMMA(\"passenger\") where AND has a higher priority than OR , would be interpreted as follows: LEMMA(\"plane\") AND LEMMA(\"passenger\") OR LEMMA(\"flight\") in other words, the engine will look for the simultaneous presence of the lemmas plane and passenger or the presence of the lemma flight by itself. However, it is possible to use round brackets to specify a different order than the one prescribed by the Boolean operators' priority scale to create what can be called \" sub-expressions \". For example, if we wanted to rewrite the condition stated above so that it would look for the presence of the lemmas flight or plane in combination with the lemma passenger , the rule would look like this: ( LEMMA(\"flight\") OR LEMMA (\"plane\") ) AND LEMMA(\"passenger\") This approach can be used with every Boolean operator and can be used in all cases in which the order of operators must be different than the one stated in the table above. Another advantage of using parentheses is that every sub-expression within them can be used as an operand itself and become part of a more complex expression. That being so, the syntax describing an expression as provided at the beginning of this section can be modified as follows: SCOPE scope_option { DOMAIN(domain_name:score_option)|IDENTIFY(template_name) { Operand1 Boolean operator Operand2 ... } } where operands are: a simple attribute, a combination of attributes, a sequence of attributes (both positional and logical), a sub-rule or a sub-expression. For example: ANCESTOR(12828) /*12828: plane, aeroplane, airplane*/ -SYNCON(UNKNOWN) AND LEMMA(\"hijack\") AND NOT ( LEMMA (\"fiction\") > LEMMA(\"story\") OR ANCESTOR(30419, 30451)// 30419: film, movie // 30451: television show, television program ) In the sample expression above we have 3 operands: The operand in the first line identifies a token in the input text using a combination of attributes ANCESTOR and SYNCON(UNKNOWN) . The operand in the third line identifies a token in the input text using a single attribute (a LEMMA ). The third operand is a sub-expression composed of two elements: a positional sequence of attributes ( LEMMA followed by LEMMA ) and an ANCESTOR attribute specifying two concepts. The expression resulting from the use of the above operands and the three operators can be read as follows: //A// AND //B// AND NOT //C// where C is (//C1// OR //C2//) To sum up: //A// AND //B// AND NOT (//C1// OR //C2//) In other words, the concept of airplane or any of its descendants should be found in the text along with the term hijack whenever the text does not mention a fictional story and/or films , TV shows , etc. Such a rule could be useful in a news article categorization project when looking for news of hijacked airplanes while excluding any articles about films or TV shows telling stories of airplanes hijacking. When specific sub-expressions or other complex operands become particularly important or useful in a project, they can be turned into sub-rules, which allow users to define an expression, give it a name and use that name in several rules to reference the expression.","title":"Condition expressions and sub-expressions"},{"location":"condition/expressions/#condition-expressions-and-sub-expressions","text":"Within extraction or categorization rules, an expression is a complex linguistic condition composed, in its most basic form, of attributes and operators: SCOPE scope_option { DOMAIN(domain_name:score_option)|IDENTIFY(template_name) { Attribute1 Operator Attribute2 ... } } For further information about the specific features of an extraction rule and how these are integrated into the above generic syntax see the related pages . However, advanced rule-writing can optionally imply a particular usage of attributes and operators and the awareness of a priority scale prescribing the use of Boolean operators as seen in the following table: Operator Priority AND NOT 1 XOR 2 AND 3 OR 4 This means that, in a rule containing more than one Boolean operator, the order in which the elements of the expression are evaluated starts from the operator with the highest priority and proceeds to those with lower priority. For example, a condition such as the following: LEMMA(\"flight\") OR LEMMA (\"plane\") AND LEMMA(\"passenger\") where AND has a higher priority than OR , would be interpreted as follows: LEMMA(\"plane\") AND LEMMA(\"passenger\") OR LEMMA(\"flight\") in other words, the engine will look for the simultaneous presence of the lemmas plane and passenger or the presence of the lemma flight by itself. However, it is possible to use round brackets to specify a different order than the one prescribed by the Boolean operators' priority scale to create what can be called \" sub-expressions \". For example, if we wanted to rewrite the condition stated above so that it would look for the presence of the lemmas flight or plane in combination with the lemma passenger , the rule would look like this: ( LEMMA(\"flight\") OR LEMMA (\"plane\") ) AND LEMMA(\"passenger\") This approach can be used with every Boolean operator and can be used in all cases in which the order of operators must be different than the one stated in the table above. Another advantage of using parentheses is that every sub-expression within them can be used as an operand itself and become part of a more complex expression. That being so, the syntax describing an expression as provided at the beginning of this section can be modified as follows: SCOPE scope_option { DOMAIN(domain_name:score_option)|IDENTIFY(template_name) { Operand1 Boolean operator Operand2 ... } } where operands are: a simple attribute, a combination of attributes, a sequence of attributes (both positional and logical), a sub-rule or a sub-expression. For example: ANCESTOR(12828) /*12828: plane, aeroplane, airplane*/ -SYNCON(UNKNOWN) AND LEMMA(\"hijack\") AND NOT ( LEMMA (\"fiction\") > LEMMA(\"story\") OR ANCESTOR(30419, 30451)// 30419: film, movie // 30451: television show, television program ) In the sample expression above we have 3 operands: The operand in the first line identifies a token in the input text using a combination of attributes ANCESTOR and SYNCON(UNKNOWN) . The operand in the third line identifies a token in the input text using a single attribute (a LEMMA ). The third operand is a sub-expression composed of two elements: a positional sequence of attributes ( LEMMA followed by LEMMA ) and an ANCESTOR attribute specifying two concepts. The expression resulting from the use of the above operands and the three operators can be read as follows: //A// AND //B// AND NOT //C// where C is (//C1// OR //C2//) To sum up: //A// AND //B// AND NOT (//C1// OR //C2//) In other words, the concept of airplane or any of its descendants should be found in the text along with the term hijack whenever the text does not mention a fictional story and/or films , TV shows , etc. Such a rule could be useful in a news article categorization project when looking for news of hijacked airplanes while excluding any articles about films or TV shows telling stories of airplanes hijacking. When specific sub-expressions or other complex operands become particularly important or useful in a project, they can be turned into sub-rules, which allow users to define an expression, give it a name and use that name in several rules to reference the expression.","title":"Condition expressions and sub-expressions"},{"location":"condition/sub-rules/","text":"Sub-rules A sub-rule is a linguistic condition which has been defined and given a name. The name can be used in any project rule to quickly and repeatedly reference the condition in the sub-rule. The syntax to define a sub-rule is the following: #sub-rule_name# = condition ; where sub-rule_name is a meaningful name chosen by the user to represent the content of the sub-rule itself, and the semi-colon at the end of the string is part of the syntax itself, representing the \"closure\" of the sub-rule definition. Note The sub-rules definition syntax requires a unique name for each sub-rule in a project. The syntax to use a sub-rule in a categorization or extraction rule is the following: SCOPE scope_option { DOMAIN(domain_name:score_option)|IDENTIFY(template_name) { operand boolean_operator #sub-rule_name# } } where operand can be a simple attribute, a combination of attributes, a sequence of attributes (both positional and logical) a sub-expression or another sub-rule. The sub-rule definition must be correctly positioned in an extraction or categorization source file and with respect to the positions of any rules which contain the sub-rule name. In fact, a sub-rule definition must precede all the rules that use it, otherwise an error will be issued. Note It is advisable to define sub-rules at the very beginning of a source file to ensure that any rule containing a sub-rule follows the sub-rule definition. A sub-rule can be used in a rule as the only element constituting the condition; however it is best used in combination with other attributes using any of the available Boolean operators. For example, the following lines: #fiction# = LEMMA (\"fiction\") > LEMMA(\"story\") OR ANCESTOR(30419, 30451)// 30419: film, movie // 30451: television show, television program OR SYNCON(29428,18571)// 29428: novel (genre) 18571: novel (physical object); SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { ANCESTOR(12828) /*12828: plane, aeroplane, airplane*/ AND LEMMA(\"hijack\") AND NOT #fiction# } first define a sub-rule called #fiction# and then use it in a rule. The engine will automatically add the condition defined in the sub-rule to the categorization rule in the exact position where the sub-rule name is typed. The rule in the example is then equivalent to: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { ANCESTOR(12828) /*12828: plane, aeroplane, airplane*/ AND LEMMA(\"hijack\") AND NOT ( LEMMA (\"fiction\") > LEMMA(\"story\") OR ANCESTOR(30419, 30451)// 30419: film, movie // 30451: television show, television program OR SYNCON(29428,18571)// 29428: novel (genre) 18571: novel (physical object); ) } } As you can infer from the lines above, a sub-rule content is inserted in a rule as a unique operand just as if parentheses were used. Sub-rules are therefore particularly useful when a specific sub-expression is meant to be reused in more than one rule. In fact, they are convenient because a user can define a condition just once and then easy reference it using its predefined name. In addition, a user can modify the condition numerous times without having to repeat the same modification in several rules (as opposed to the use of a sub-expression). This cuts down the risk of mistakes and random omissions.","title":"Sub-rules"},{"location":"condition/sub-rules/#sub-rules","text":"A sub-rule is a linguistic condition which has been defined and given a name. The name can be used in any project rule to quickly and repeatedly reference the condition in the sub-rule. The syntax to define a sub-rule is the following: #sub-rule_name# = condition ; where sub-rule_name is a meaningful name chosen by the user to represent the content of the sub-rule itself, and the semi-colon at the end of the string is part of the syntax itself, representing the \"closure\" of the sub-rule definition. Note The sub-rules definition syntax requires a unique name for each sub-rule in a project. The syntax to use a sub-rule in a categorization or extraction rule is the following: SCOPE scope_option { DOMAIN(domain_name:score_option)|IDENTIFY(template_name) { operand boolean_operator #sub-rule_name# } } where operand can be a simple attribute, a combination of attributes, a sequence of attributes (both positional and logical) a sub-expression or another sub-rule. The sub-rule definition must be correctly positioned in an extraction or categorization source file and with respect to the positions of any rules which contain the sub-rule name. In fact, a sub-rule definition must precede all the rules that use it, otherwise an error will be issued. Note It is advisable to define sub-rules at the very beginning of a source file to ensure that any rule containing a sub-rule follows the sub-rule definition. A sub-rule can be used in a rule as the only element constituting the condition; however it is best used in combination with other attributes using any of the available Boolean operators. For example, the following lines: #fiction# = LEMMA (\"fiction\") > LEMMA(\"story\") OR ANCESTOR(30419, 30451)// 30419: film, movie // 30451: television show, television program OR SYNCON(29428,18571)// 29428: novel (genre) 18571: novel (physical object); SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { ANCESTOR(12828) /*12828: plane, aeroplane, airplane*/ AND LEMMA(\"hijack\") AND NOT #fiction# } first define a sub-rule called #fiction# and then use it in a rule. The engine will automatically add the condition defined in the sub-rule to the categorization rule in the exact position where the sub-rule name is typed. The rule in the example is then equivalent to: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { ANCESTOR(12828) /*12828: plane, aeroplane, airplane*/ AND LEMMA(\"hijack\") AND NOT ( LEMMA (\"fiction\") > LEMMA(\"story\") OR ANCESTOR(30419, 30451)// 30419: film, movie // 30451: television show, television program OR SYNCON(29428,18571)// 29428: novel (genre) 18571: novel (physical object); ) } } As you can infer from the lines above, a sub-rule content is inserted in a rule as a unique operand just as if parentheses were used. Sub-rules are therefore particularly useful when a specific sub-expression is meant to be reused in more than one rule. In fact, they are convenient because a user can define a condition just once and then easy reference it using its predefined name. In addition, a user can modify the condition numerous times without having to repeat the same modification in several rules (as opposed to the use of a sub-expression). This cuts down the risk of mistakes and random omissions.","title":"Sub-rules"},{"location":"disambiguation-output/","text":"Disambiguation output The text to be analyzed The text intelligence engine workflow is as follows: The input text is analyzed by the disambiguator using its resources (algorithms and data structures, among which the Knowledge Graph plays a crucial role). The disambiguator returns the disambiguation string . Categorization and extraction rules are applied to the disambiguation string. To keep it simple, the disambiguation string can be described as an enriched version of the original input text, in which the text itself is divided into a series of textual blocks, each of which is tagged with many attributes (positional, morphological, grammatical, logical, syntactical, semantic, etc.) which have been determined during the analysis. These textual blocks are organized in a hierarchy so that each one of them features some attributes of other blocks and is linked to them by different types of relationships. From top to bottom, the textual blocks produced by the disambiguator are: Document Paragraph Sentence Clause Phrase Word Atom Two more levels can be optionally added to this model if sections or segments have been defined for the input documents. Sections and segments are two custom text subdivisions that can modify the text analysis model, as seen in the following image. Document represents the whole input text. Sections , being optional subdivisions, may or may not be present in the input document. If they are present, the disambiguator will recognize them and will place the text blocks inside them according to their position. Segments are also optional subdivisions which can be defined in the input document (static segments) or be dynamically built after the disambiguation and before the rules are applied. A segment is always a part of a section, therefore its boundaries can never cross a section's boundaries. A paragraph is a unit of discourse consisting of one or more sentences dealing with a particular idea. Its start is typically indicated by a full stop and the beginning of a new line. A paragraph is always a part of a section, therefore its boundaries can never cross a section's boundaries. A sentence is a portion of text consisting of one or several words, which are linked by a syntactic relation and are able to convey meaning. Its start is typically indicated by a punctuation mark such as a period, question mark or exclamation mark. A clause consists of one or several words within a sentence representing the smallest grammatical unit that can express a complete proposition. It consists of at least a predicate (a phrase containing a verb) and possibly other phrases which have the role of subject, direct object and other complements. No clause is recognized if no verb is present. Clauses might consist of non-contiguous text parts. In the sentence: The dog that was barking ran into the street There are two clauses: The dog ran into the street and that was barking . A phrase consists of one or several words that are sort of constituents of the sentence and act as single units in its syntax. Common linguistics recognizes noun phrases, verb phrases, etc. Phrases have a logical role in a clause: subject, direct object and other complements. For example, in the sentence: John's dog ate a big bone the phrases are: dog (subject), John's (possessive case), ate (verb predicate), a big bone (direct object). A word is a portion of text corresponding to concepts, collocations, entities, idioms and also punctuation marks recognized during the semantic analysis. Example of words are: dog , the , credit card , triumph , Miami , ! , 234 5th Avenue , a million and a half . Atom indicates a portion of text that cannot be further divided, that is to say a word or a punctuation mark, without considering its possible semantic value. Therefore, atoms can coincide with words or be the constituents of a word. The latter is the case of collocations, idioms and entities, which are composed of several atoms. In fact, these three textual elements posses a semantic value which is stronger than the value of each atom considered separately. To have a clearer picture of how the subdivisions described above are applied to text, let's observe the following sample sentence: The goldfinches that usually spend the winter in Central Europe have invaded Italy because of the unusual cold. The sentence can be divided into two clauses: The goldfinches have invaded Italy because of the unusual cold (independent) that usually spend the winter in Central Europe (relative) The elements constituting the independent clause are not contiguous since the relative one splits the independent clause into two portions. Eight phrases can be recognized, four for each clause: Clause #1: The goldfinches have invaded Italy because of the unusual cold Clause #2: that usually spend the winter in Central Europe One word ( because of ) is a collocation consisting of two atoms ( because and of ). Tokens Generally speaking, a token is a string of characters grouped together during a process called tokenization. This process consists of breaking up a stream of text into meaningful elements called tokens. expert.ai technology considers a token to be a word, or more precisely, a portion of text that cannot be further parsed. In the process of developing linguistic projects, textual tokens can be searched, matched and processed by means of attributes within the linguistic rules.","title":"Disambiguation output"},{"location":"disambiguation-output/#disambiguation-output","text":"","title":"Disambiguation output"},{"location":"disambiguation-output/#the-text-to-be-analyzed","text":"The text intelligence engine workflow is as follows: The input text is analyzed by the disambiguator using its resources (algorithms and data structures, among which the Knowledge Graph plays a crucial role). The disambiguator returns the disambiguation string . Categorization and extraction rules are applied to the disambiguation string. To keep it simple, the disambiguation string can be described as an enriched version of the original input text, in which the text itself is divided into a series of textual blocks, each of which is tagged with many attributes (positional, morphological, grammatical, logical, syntactical, semantic, etc.) which have been determined during the analysis. These textual blocks are organized in a hierarchy so that each one of them features some attributes of other blocks and is linked to them by different types of relationships. From top to bottom, the textual blocks produced by the disambiguator are: Document Paragraph Sentence Clause Phrase Word Atom Two more levels can be optionally added to this model if sections or segments have been defined for the input documents. Sections and segments are two custom text subdivisions that can modify the text analysis model, as seen in the following image. Document represents the whole input text. Sections , being optional subdivisions, may or may not be present in the input document. If they are present, the disambiguator will recognize them and will place the text blocks inside them according to their position. Segments are also optional subdivisions which can be defined in the input document (static segments) or be dynamically built after the disambiguation and before the rules are applied. A segment is always a part of a section, therefore its boundaries can never cross a section's boundaries. A paragraph is a unit of discourse consisting of one or more sentences dealing with a particular idea. Its start is typically indicated by a full stop and the beginning of a new line. A paragraph is always a part of a section, therefore its boundaries can never cross a section's boundaries. A sentence is a portion of text consisting of one or several words, which are linked by a syntactic relation and are able to convey meaning. Its start is typically indicated by a punctuation mark such as a period, question mark or exclamation mark. A clause consists of one or several words within a sentence representing the smallest grammatical unit that can express a complete proposition. It consists of at least a predicate (a phrase containing a verb) and possibly other phrases which have the role of subject, direct object and other complements. No clause is recognized if no verb is present. Clauses might consist of non-contiguous text parts. In the sentence: The dog that was barking ran into the street There are two clauses: The dog ran into the street and that was barking . A phrase consists of one or several words that are sort of constituents of the sentence and act as single units in its syntax. Common linguistics recognizes noun phrases, verb phrases, etc. Phrases have a logical role in a clause: subject, direct object and other complements. For example, in the sentence: John's dog ate a big bone the phrases are: dog (subject), John's (possessive case), ate (verb predicate), a big bone (direct object). A word is a portion of text corresponding to concepts, collocations, entities, idioms and also punctuation marks recognized during the semantic analysis. Example of words are: dog , the , credit card , triumph , Miami , ! , 234 5th Avenue , a million and a half . Atom indicates a portion of text that cannot be further divided, that is to say a word or a punctuation mark, without considering its possible semantic value. Therefore, atoms can coincide with words or be the constituents of a word. The latter is the case of collocations, idioms and entities, which are composed of several atoms. In fact, these three textual elements posses a semantic value which is stronger than the value of each atom considered separately. To have a clearer picture of how the subdivisions described above are applied to text, let's observe the following sample sentence: The goldfinches that usually spend the winter in Central Europe have invaded Italy because of the unusual cold. The sentence can be divided into two clauses: The goldfinches have invaded Italy because of the unusual cold (independent) that usually spend the winter in Central Europe (relative) The elements constituting the independent clause are not contiguous since the relative one splits the independent clause into two portions. Eight phrases can be recognized, four for each clause: Clause #1: The goldfinches have invaded Italy because of the unusual cold Clause #2: that usually spend the winter in Central Europe One word ( because of ) is a collocation consisting of two atoms ( because and of ).","title":"The text to be analyzed"},{"location":"disambiguation-output/#tokens","text":"Generally speaking, a token is a string of characters grouped together during a process called tokenization. This process consists of breaking up a stream of text into meaningful elements called tokens. expert.ai technology considers a token to be a word, or more precisely, a portion of text that cannot be further parsed. In the process of developing linguistic projects, textual tokens can be searched, matched and processed by means of attributes within the linguistic rules.","title":"Tokens"},{"location":"extraction/","text":"Extraction peculiarities This section describes the peculiarities of the rules language regarding the extraction task. As written in the introduction, extraction consists in identifying and \"pulling\" useful data out of input documents. A text intelligence engine based on expert.ai technology: Retrieves data from (usually) unstructured documents. Can extract complex sets of data thanks to sophisticated mechanisms of prediction and recognition of unknown entities. Can transform and normalize data to produce a highly-refined final value. In other words, extraction involves recognizing and extracting unknown instances of well defined types of data, as well as aggregating and normalizing data in order to make them ready for further processing and, possibly, storage. To facilitate the extraction task and provide a structure for the extracted data, the process is based on a templates system . Templates \"attract\" and aggregate data. A template is composed of one or more fields, each representing a data type. As mentioned in the introduction, when activated, an extraction rule fills the template field (or fields) declared inside the rule and generates a record. A single rule can fill more than one field. Extracting more than one field in the same rule represents a strong relationship between fields and brings to the generation of a multi-field record. This mechanism is called by-rule aggregation . Note A rule cannot refer to the same field more than once, except in the case of composition. On the other hand, the same field can be filled by more than one rule since several rules are often required to identify the field in all the expected forms defined by the project requirements. Field attributes and merge options merge basic records to reach a higher level of data aggregation. A single rule can be activated multiple times by the input text, thus extracting multiple instances of the same value. In these cases, an automatic reduction mechanism called \"bundling\" combines the basic records, by effectively extracting a single value while preserving the single occurrences as lower-level detail data. The output resulting from an input document processed by an extraction engine consists of one or more records per document, each containing the data extracted by means of extraction rules and aggregated using one of the available options or the by-rule aggregation technique. In these records, each extracted value is associated to its field, which is in turn associated to the template it belongs to. In other words, the table structure previously defined through the creation of template(s) and field(s) is returned in the output filled with the specific pieces of data retrieved in the document. As an optional part in the extraction process, the data retrieved from a document can be kept in its original form or can be refined and manipulated through the use of different normalization or transformation approaches. Please see the dedicated pages for detailed information.","title":"Extraction peculiarities"},{"location":"extraction/#extraction-peculiarities","text":"This section describes the peculiarities of the rules language regarding the extraction task. As written in the introduction, extraction consists in identifying and \"pulling\" useful data out of input documents. A text intelligence engine based on expert.ai technology: Retrieves data from (usually) unstructured documents. Can extract complex sets of data thanks to sophisticated mechanisms of prediction and recognition of unknown entities. Can transform and normalize data to produce a highly-refined final value. In other words, extraction involves recognizing and extracting unknown instances of well defined types of data, as well as aggregating and normalizing data in order to make them ready for further processing and, possibly, storage. To facilitate the extraction task and provide a structure for the extracted data, the process is based on a templates system . Templates \"attract\" and aggregate data. A template is composed of one or more fields, each representing a data type. As mentioned in the introduction, when activated, an extraction rule fills the template field (or fields) declared inside the rule and generates a record. A single rule can fill more than one field. Extracting more than one field in the same rule represents a strong relationship between fields and brings to the generation of a multi-field record. This mechanism is called by-rule aggregation . Note A rule cannot refer to the same field more than once, except in the case of composition. On the other hand, the same field can be filled by more than one rule since several rules are often required to identify the field in all the expected forms defined by the project requirements. Field attributes and merge options merge basic records to reach a higher level of data aggregation. A single rule can be activated multiple times by the input text, thus extracting multiple instances of the same value. In these cases, an automatic reduction mechanism called \"bundling\" combines the basic records, by effectively extracting a single value while preserving the single occurrences as lower-level detail data. The output resulting from an input document processed by an extraction engine consists of one or more records per document, each containing the data extracted by means of extraction rules and aggregated using one of the available options or the by-rule aggregation technique. In these records, each extracted value is associated to its field, which is in turn associated to the template it belongs to. In other words, the table structure previously defined through the creation of template(s) and field(s) is returned in the output filled with the specific pieces of data retrieved in the document. As an optional part in the extraction process, the data retrieved from a document can be kept in its original form or can be refined and manipulated through the use of different normalization or transformation approaches. Please see the dedicated pages for detailed information.","title":"Extraction peculiarities"},{"location":"extraction/composition/","text":"Composition overview In extraction rule writing, composition is one of the two optional features in the association of an extracted token and a field which controls which data is transferred into the field. In particular, composition manipulates and assembles the information in the text to return complete, uniform and non redundant extraction data. In fact, composition allows the user to combine several elements of a positional or logical sequence, and decide in which order they will become part of the extraction output. Composition can be employed only in rules using one of the sequence operators available in the Rules language. Composition is able to return two or more elements included in a sequence with the option to decide the position of each element in the final output. The syntax of a composition is as follows: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute1]|[COMPOSITION] sequence operator @field_1[Attribute2]|[COMPOSITION] } } where COMPOSITION refers to a number sign followed by a whole number. For example: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute1]|[#1] sequence operator @field_1[Attribute2]|[#2] } } In each rule, the numbering must begin with the number one (1), without any gap in the sequence. Each so-called sequence \"chunk\" ( #1 , #2 , etc.) must share the same destination field and be part of a sequence. Each \"chunk\" symbol must be written between brackets and positioned at the end of a full and correct extraction rule; a vertical bar (called pipe, | ) separates the rule from the composition. The number associated to each chunk represents the position that the extracted value will take in the final output. Composition is the only syntax that allows the repetition of the same extraction field within the same rule. When using this option, it is not mandatory to use the extraction syntax on every element of the sequence; in other words, it is possible to choose which elements will be part of the extraction and which elements will only act as constraints in the rule. When composition is used, the extracted text is always the same as the text found in the original document unless a composition is associated with the transformation. Composition is useful when a complex management of extraction data is required. In particular, it is beneficial when the information is not concentrated in one single part of a document. A possible use of a composition normalization is when final values are found in different parts of the text, but must be selected and combined together to reach the final output. Consider the following example: SCOPE SENTENCE { IDENTIFY(PORTS) { @Port[SYNCON(39541)]|[#1] // 39541: port, city port, seaport >> KEYWORD(\"of\") >> @Port[ANCESTOR(39663)]|[#2] // 39663: city, town } } This rule is meant to extract the concept of port ( SYNCON(39541) ), strictly followed by (double greater than sign, >> ) keyword of , followed in turn by the chain of concepts starting from the syncon for city ( ANCESTOR(39663) ). The extraction syntax is applied only to the first and the last element of the rule, in order to extract just the values matched by these two attributes. Note that the two attributes enclosed in the extraction syntax extract different values for the same field ( @Port ) and that the composition syntax is applied to each one of them. The #1 and #2 declarations determine the following behavior: every element extracted for the attribute marked with #1 will be the first element of the final output while every element extracted for the attribute marked with #2 will be the second element of the final output. If the rule above is run against the following sample text: Poll: Should Seattle Port CEO choose between jobs? Port of Seattle CEO Tay Yoshitani is coming under growing criticism for trying to hold onto his $367,000 job and a seat on the board of Expeditors International. For sitting on the board, Yoshitani receives a $30,000-a-year retainer, $1,000 per-diem for board meetings or other company work and $200,000 in restricted stock each year. Ka-ching! Yoshitani says the dual roles do not represent a conflict of interest, an opinion apparently concurred with by the Port's top attorney. But commissioners are becoming increasingly vocal about this discomfort with his roles. Thirteen state legislators wrote to Port commissioners expressing concern about Yoshitani's jobs. The text contains one combination of values - Port of Seattle - matching the sample rule because Port is recognized as an expression of syncon 39541 ( port ) and Seattle is recognized as a descendant of syncon 39663 ( city ). Port is then chosen as chunk #1 of the composition and Seattle as chunk #2. The final extraction is Port Seattle , the of keyword is not part of the composition. Now consider the same rule with a slight adjustment to the composition chunks: SCOPE SENTENCE { IDENTIFY(PORTS) { @Port[SYNCON(39541)]|[#2] // 39541: port, city port, seaport >> KEYWORD(\"of\") >> @Port[ANCESTOR(39663)]|[#1] // 39663: city, town } } The only difference is that the order of the two chunks has been inverted, so the extracted value for field @Port becomes Seattle Port . The sample document contains another reference to the same port ( Seattle Port ). If a second rule is added to the set in order to extract this instance, an interesting behavior of the engine could be observed: SCOPE SENTENCE { IDENTIFY(PORTS) { @Port[ANCESTOR(39541) + TYPE(NPR)] // 39541: port, city port, seaport } } This rule is meant to extract any proper noun ( TYPE(NPR) ) - both known or unknown to the Knowledge Graph - recognized to be the name of a port ( ANCESTOR(39541) ). Since the second rule would extract Seattle Port and this value coincides with the outcome of the first rule, the engine creates only one output record, thus assuring non redundant and normalized results. In other words, the two rules manage two different forms of the same concept and extract different elements in different positions. However, the composition syntax allows the user to deconstruct and recompose the elements so that data normalization can be applied.","title":"Composition overview"},{"location":"extraction/composition/#composition-overview","text":"In extraction rule writing, composition is one of the two optional features in the association of an extracted token and a field which controls which data is transferred into the field. In particular, composition manipulates and assembles the information in the text to return complete, uniform and non redundant extraction data. In fact, composition allows the user to combine several elements of a positional or logical sequence, and decide in which order they will become part of the extraction output. Composition can be employed only in rules using one of the sequence operators available in the Rules language. Composition is able to return two or more elements included in a sequence with the option to decide the position of each element in the final output. The syntax of a composition is as follows: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute1]|[COMPOSITION] sequence operator @field_1[Attribute2]|[COMPOSITION] } } where COMPOSITION refers to a number sign followed by a whole number. For example: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute1]|[#1] sequence operator @field_1[Attribute2]|[#2] } } In each rule, the numbering must begin with the number one (1), without any gap in the sequence. Each so-called sequence \"chunk\" ( #1 , #2 , etc.) must share the same destination field and be part of a sequence. Each \"chunk\" symbol must be written between brackets and positioned at the end of a full and correct extraction rule; a vertical bar (called pipe, | ) separates the rule from the composition. The number associated to each chunk represents the position that the extracted value will take in the final output. Composition is the only syntax that allows the repetition of the same extraction field within the same rule. When using this option, it is not mandatory to use the extraction syntax on every element of the sequence; in other words, it is possible to choose which elements will be part of the extraction and which elements will only act as constraints in the rule. When composition is used, the extracted text is always the same as the text found in the original document unless a composition is associated with the transformation. Composition is useful when a complex management of extraction data is required. In particular, it is beneficial when the information is not concentrated in one single part of a document. A possible use of a composition normalization is when final values are found in different parts of the text, but must be selected and combined together to reach the final output. Consider the following example: SCOPE SENTENCE { IDENTIFY(PORTS) { @Port[SYNCON(39541)]|[#1] // 39541: port, city port, seaport >> KEYWORD(\"of\") >> @Port[ANCESTOR(39663)]|[#2] // 39663: city, town } } This rule is meant to extract the concept of port ( SYNCON(39541) ), strictly followed by (double greater than sign, >> ) keyword of , followed in turn by the chain of concepts starting from the syncon for city ( ANCESTOR(39663) ). The extraction syntax is applied only to the first and the last element of the rule, in order to extract just the values matched by these two attributes. Note that the two attributes enclosed in the extraction syntax extract different values for the same field ( @Port ) and that the composition syntax is applied to each one of them. The #1 and #2 declarations determine the following behavior: every element extracted for the attribute marked with #1 will be the first element of the final output while every element extracted for the attribute marked with #2 will be the second element of the final output. If the rule above is run against the following sample text: Poll: Should Seattle Port CEO choose between jobs? Port of Seattle CEO Tay Yoshitani is coming under growing criticism for trying to hold onto his $367,000 job and a seat on the board of Expeditors International. For sitting on the board, Yoshitani receives a $30,000-a-year retainer, $1,000 per-diem for board meetings or other company work and $200,000 in restricted stock each year. Ka-ching! Yoshitani says the dual roles do not represent a conflict of interest, an opinion apparently concurred with by the Port's top attorney. But commissioners are becoming increasingly vocal about this discomfort with his roles. Thirteen state legislators wrote to Port commissioners expressing concern about Yoshitani's jobs. The text contains one combination of values - Port of Seattle - matching the sample rule because Port is recognized as an expression of syncon 39541 ( port ) and Seattle is recognized as a descendant of syncon 39663 ( city ). Port is then chosen as chunk #1 of the composition and Seattle as chunk #2. The final extraction is Port Seattle , the of keyword is not part of the composition. Now consider the same rule with a slight adjustment to the composition chunks: SCOPE SENTENCE { IDENTIFY(PORTS) { @Port[SYNCON(39541)]|[#2] // 39541: port, city port, seaport >> KEYWORD(\"of\") >> @Port[ANCESTOR(39663)]|[#1] // 39663: city, town } } The only difference is that the order of the two chunks has been inverted, so the extracted value for field @Port becomes Seattle Port . The sample document contains another reference to the same port ( Seattle Port ). If a second rule is added to the set in order to extract this instance, an interesting behavior of the engine could be observed: SCOPE SENTENCE { IDENTIFY(PORTS) { @Port[ANCESTOR(39541) + TYPE(NPR)] // 39541: port, city port, seaport } } This rule is meant to extract any proper noun ( TYPE(NPR) ) - both known or unknown to the Knowledge Graph - recognized to be the name of a port ( ANCESTOR(39541) ). Since the second rule would extract Seattle Port and this value coincides with the outcome of the first rule, the engine creates only one output record, thus assuring non redundant and normalized results. In other words, the two rules manage two different forms of the same concept and extract different elements in different positions. However, the composition syntax allows the user to deconstruct and recompose the elements so that data normalization can be applied.","title":"Composition overview"},{"location":"extraction/process/","text":"Advanced topic: record generation process Introduction and definitions The process that begins with capturing data and then returning it as an aggregated record is complex and comprised of many steps. What follows is the description of the process that transforms single extracted tokens into complex records. Each extracted token is coded with information about its: Value : what was extracted in a given field. Position : where in the input document it was extracted. Template and Field : the data \"destination\". Groups of values extracted by a single rule represent the first available records. When a single rule identifies several values for a single field in a table, more records are generated. Simple record At this stage, a record consists of a set of identical values which refer to the same template and the same field. Each value is associated to a list of positions that identify all occurrences of the given value that was aggregated to the given field. For the sake of convenience, the records are divided into different groups based on their characteristics: Different records with the same destination template are called cognate . Cognate records that do not have any field in common are called compatible . Cognate records that have one field in common are named consistent if such field contains the same value, or inconsistent in the opposite case. Cognate records that have several fields in common are consistent if all the fields in common have the same value. If a record is consistent with another record and all of its fields are in common with the latter, then the former is said to be reducible to the other record. If a reducible record has at least one position for each field in common with the reducing record, it is said to be contained within the reducing record. The process The phases The extraction records elaboration process is divided into three main phases: Bundling Joining Attraction Phase 1 - Bundling Definition The first phase of the extraction records elaboration process is called bundling . During this phase, the number of records is reduced whenever possible. This is true when the fields of a record are a sub-set of another record's field; in this case, the first record can be bundled into the second one. At the end of this process, the list of positions of the second record is the combination of the positions of the two original records. In case of ambiguity, when there are two records that can potentially receive the data of another one, it is not possible to perform any reduction. In the following example, record 3 may be bundled both with 1 and 2, therefore it is not deleted. The algorithm Bundling is carried out following these steps: Records are arranged from the longest to the shortest. Starting from the first, each record is compared to the previous one, thus calculating a list of the records which can \"absorb\" other records. A record can be absorbed when it contains a number of fields less than\u2014or equal to\u2014the number of the fields contained in the other record, and both the fields and the associated values are the same. Due to the order of progression, two records from the same \"absorption\" list can never absorb each other. If the list is not empty and only has one value, it means that the record is absorbable, and therefore it is absorbed. This will cause the record to be deleted, while the absorbing record adds all of the positions from the absorbed record to its list. If the list has several values, the positions must be verified, especially when one of the absorbing records has the same positions as the absorbable record. If no absorbing record has this feature, nothing will happen. Otherwise, all absorbing records will absorb the absorbable records. When all the fields of the second record have the \u201cS\u201d attribute, then the condition of the positions must be verified for the record to be absorbed. The following table illustrates how the impossibility of bundling record 3 to one of the other two records is only apparent, since they are indeed identical and can be reduced to a single record by removing all forms of ambiguity. Another form under which an apparent ambiguity may occur is the following: Record 3 cannot be immediately bundled with 1 or 2 because they can both absorb it. Actually, record 1 can also be bundled with 2, thus eliminating every ambiguity, and then also be bundled with record 3 later. Please consider the following situation as well: Record 4 is really ambiguous, because it could be bundled both with 2 and 3, therefore, it can not be absorbed by either of them. However, if an additional record such as the following is also present: Everything could be solved because, first, records 2 and 3 would be absorbed by record 1, and then, the latter will also absorb record 4. Phase 2 - Joining Definition The second phase of the extraction records elaboration process is called joining . During this phase pairs of records are combined in respect to the cardinal fields. At the end of this process, every new generated record will contain the combination of fields with their values, each with all the positions that were present in the original instances. In this process, the programmed scope does not play any role: if a combination is possible, it will be done regardless of the positions of the values. A generated record can also be re-used as a generating one. The algorithm All records that can be joined are joined. This may lead to the generation of double records or records contained in bigger ones. Therefore, at the end of the joining phase, a new bundling process is performed and only the remaining records are maintained. Assume that the following four records have been extracted: The following joining processes are generated: AB + BC = ABC BC + CD1 = BCD1 BC + CD2 = BCD2 ABC + CD1 = ABCD1 ABC + CD2 = ABCD2 By performing a new bundling process, the two longest combinations from the list of records (both original and generated) will progressively \"absorb\" the others until only they remain. The list of records: ABCD1 ABCD2 BCD1 BCD2 ABC AB BC CD1 CD2 Undergoes a bundling process so that the final output is: ABCD1 ABCD2 In this context, the bundling mechanism uses the positions associated with the fields less strictly. In the current implementation it is still used when stating that a record can be absorbed when there is no intersection between the lists of field positions in both records. Distribution The use of cardinal fields is distributive. Consider the following example, where: produces: AB + BC = ABC BC + A1C = A1BC BC is joined to both of the two other blocks. The following example: produces: ABD + BC = ABCD ABD + CD = ABCD which can be bundled. On the other hand, two values for D would have produced two records ABCD1 and ABCD2 . Phase 3 - Attraction Definition The third phase of the extraction records elaboration process is called attraction . During this phase, weak records (those characterized by a single field) are retrieved. In other words, during this phase a new joining process is carried out; however, in this case the list of positions associated with fields becomes crucial. To perform this activity for a given template, the following conditions must be satisfied: At least one attraction rule must be defined for a given template. -Rules must have generated one or more records for an attractable field ; such records are called attractable . Rules must have generated at least one record containing one or more fields, in which all the attracting fields have a value. In addition, the record must not contain a value for the attractable field; the record is called attractor . By examining the list of positions associated to the attracting fields , it is possible to identify certain extensions within which attraction may occur. If the attractor record does not contain the attractable field, but in the attractable record at least one of the positions associated to this value is found within the identified extensions, then the field of the attractable record will be added to the attractor record along with the entire list of its positions. Attraction with several attractor records This instance can generate distributions such as the one in the following example, where record 3 can be combined both with record 1 and record 2. If NAME attracts TELEPHONE , then record 3 can be combined both with record 1 and 2, as long as there is an extension that allows this. Attraction with several attracted records We assume that NAME attracts TELEPHONE . Nothing happens because the attractor record cannot be duplicated. In such cases it is necessary to check the list of positions: if one of the attracted records does not have an extension in common with the attractor, all ambiguities will disappear and records could be merged. The algorithm The attraction algorithm is equivalent to the joining algorithm, the only difference is in the method to establish if a combination can occur or not. With the attraction algorithm, the fields to be considered can be more than one, and the list of positions is actually taken into consideration. Complex Record At the end of this process, a record consists of a set of values which refer to one or more different fields belonging to the same template and have consistently been aggregated based on the options previously set for the fields and the template themselves. For example, for a sample called PERSONAL_DATA (whose aim is to aggregate personal information related to names of people present in a document), the records could look like the following: PERSONAL_DATA NAME ADDRESS TELEPHONE Record 1 John Smith 34 Park Avenue 555 123-4567 Record 2 Sarah Parker 1280 Stanstead Rd 555 369-369 Record 3 Laura Diaz 12 Square Garden Note All fields defined in a template may not always be present (along with their value) in the final record. The ones which must necessarily appear in a record or not are determined by the options used to define the fields and the template. Semantics of attraction rules Problems may occur when a template has several attraction rules. The following examples assume that there are one-field records such as A , B , C and so on. For instance, in: LINK B WITH A WHEN PARAGRAPH LINK C WITH A WHEN PARAGRAPH we can get: A B C A + B = AB A + C = AC AB + C = ABC AC + B = ABC Reduction is necessary to bundle everything into: ABC Circular references The following groups of attraction rules are invalid because they contain circular references. In fact they contain fields, which act both as attracting and attractable fields. LINK B WITH A WHEN PARAGRAPH LINK C WITH B WHEN PARAGRAPH LINK A WITH C WHEN PARAGRAPH or: LINK B WITH A WHEN PARAGRAPH LINK A WITH B WHEN PARAGRAPH There are cases in which such circles do not occur, but, since they depend on the extracted data, it is better to prevent them from occurring.","title":"Advanced topic: record generation process"},{"location":"extraction/process/#advanced-topic-record-generation-process","text":"","title":"Advanced topic: record generation process"},{"location":"extraction/process/#introduction-and-definitions","text":"The process that begins with capturing data and then returning it as an aggregated record is complex and comprised of many steps. What follows is the description of the process that transforms single extracted tokens into complex records. Each extracted token is coded with information about its: Value : what was extracted in a given field. Position : where in the input document it was extracted. Template and Field : the data \"destination\". Groups of values extracted by a single rule represent the first available records. When a single rule identifies several values for a single field in a table, more records are generated.","title":"Introduction and definitions"},{"location":"extraction/process/#simple-record","text":"At this stage, a record consists of a set of identical values which refer to the same template and the same field. Each value is associated to a list of positions that identify all occurrences of the given value that was aggregated to the given field. For the sake of convenience, the records are divided into different groups based on their characteristics: Different records with the same destination template are called cognate . Cognate records that do not have any field in common are called compatible . Cognate records that have one field in common are named consistent if such field contains the same value, or inconsistent in the opposite case. Cognate records that have several fields in common are consistent if all the fields in common have the same value. If a record is consistent with another record and all of its fields are in common with the latter, then the former is said to be reducible to the other record. If a reducible record has at least one position for each field in common with the reducing record, it is said to be contained within the reducing record.","title":"Simple record"},{"location":"extraction/process/#the-process","text":"","title":"The process"},{"location":"extraction/process/#the-phases","text":"The extraction records elaboration process is divided into three main phases: Bundling Joining Attraction","title":"The phases"},{"location":"extraction/process/#phase-1-bundling","text":"","title":"Phase 1 - Bundling"},{"location":"extraction/process/#definition","text":"The first phase of the extraction records elaboration process is called bundling . During this phase, the number of records is reduced whenever possible. This is true when the fields of a record are a sub-set of another record's field; in this case, the first record can be bundled into the second one. At the end of this process, the list of positions of the second record is the combination of the positions of the two original records. In case of ambiguity, when there are two records that can potentially receive the data of another one, it is not possible to perform any reduction. In the following example, record 3 may be bundled both with 1 and 2, therefore it is not deleted.","title":"Definition"},{"location":"extraction/process/#the-algorithm","text":"Bundling is carried out following these steps: Records are arranged from the longest to the shortest. Starting from the first, each record is compared to the previous one, thus calculating a list of the records which can \"absorb\" other records. A record can be absorbed when it contains a number of fields less than\u2014or equal to\u2014the number of the fields contained in the other record, and both the fields and the associated values are the same. Due to the order of progression, two records from the same \"absorption\" list can never absorb each other. If the list is not empty and only has one value, it means that the record is absorbable, and therefore it is absorbed. This will cause the record to be deleted, while the absorbing record adds all of the positions from the absorbed record to its list. If the list has several values, the positions must be verified, especially when one of the absorbing records has the same positions as the absorbable record. If no absorbing record has this feature, nothing will happen. Otherwise, all absorbing records will absorb the absorbable records. When all the fields of the second record have the \u201cS\u201d attribute, then the condition of the positions must be verified for the record to be absorbed. The following table illustrates how the impossibility of bundling record 3 to one of the other two records is only apparent, since they are indeed identical and can be reduced to a single record by removing all forms of ambiguity. Another form under which an apparent ambiguity may occur is the following: Record 3 cannot be immediately bundled with 1 or 2 because they can both absorb it. Actually, record 1 can also be bundled with 2, thus eliminating every ambiguity, and then also be bundled with record 3 later. Please consider the following situation as well: Record 4 is really ambiguous, because it could be bundled both with 2 and 3, therefore, it can not be absorbed by either of them. However, if an additional record such as the following is also present: Everything could be solved because, first, records 2 and 3 would be absorbed by record 1, and then, the latter will also absorb record 4.","title":"The algorithm"},{"location":"extraction/process/#phase-2-joining","text":"","title":"Phase 2 - Joining"},{"location":"extraction/process/#definition_1","text":"The second phase of the extraction records elaboration process is called joining . During this phase pairs of records are combined in respect to the cardinal fields. At the end of this process, every new generated record will contain the combination of fields with their values, each with all the positions that were present in the original instances. In this process, the programmed scope does not play any role: if a combination is possible, it will be done regardless of the positions of the values. A generated record can also be re-used as a generating one.","title":"Definition"},{"location":"extraction/process/#the-algorithm_1","text":"All records that can be joined are joined. This may lead to the generation of double records or records contained in bigger ones. Therefore, at the end of the joining phase, a new bundling process is performed and only the remaining records are maintained. Assume that the following four records have been extracted: The following joining processes are generated: AB + BC = ABC BC + CD1 = BCD1 BC + CD2 = BCD2 ABC + CD1 = ABCD1 ABC + CD2 = ABCD2 By performing a new bundling process, the two longest combinations from the list of records (both original and generated) will progressively \"absorb\" the others until only they remain. The list of records: ABCD1 ABCD2 BCD1 BCD2 ABC AB BC CD1 CD2 Undergoes a bundling process so that the final output is: ABCD1 ABCD2 In this context, the bundling mechanism uses the positions associated with the fields less strictly. In the current implementation it is still used when stating that a record can be absorbed when there is no intersection between the lists of field positions in both records.","title":"The algorithm"},{"location":"extraction/process/#distribution","text":"The use of cardinal fields is distributive. Consider the following example, where: produces: AB + BC = ABC BC + A1C = A1BC BC is joined to both of the two other blocks. The following example: produces: ABD + BC = ABCD ABD + CD = ABCD which can be bundled. On the other hand, two values for D would have produced two records ABCD1 and ABCD2 .","title":"Distribution"},{"location":"extraction/process/#phase-3-attraction","text":"","title":"Phase 3 - Attraction"},{"location":"extraction/process/#definition_2","text":"The third phase of the extraction records elaboration process is called attraction . During this phase, weak records (those characterized by a single field) are retrieved. In other words, during this phase a new joining process is carried out; however, in this case the list of positions associated with fields becomes crucial. To perform this activity for a given template, the following conditions must be satisfied: At least one attraction rule must be defined for a given template. -Rules must have generated one or more records for an attractable field ; such records are called attractable . Rules must have generated at least one record containing one or more fields, in which all the attracting fields have a value. In addition, the record must not contain a value for the attractable field; the record is called attractor . By examining the list of positions associated to the attracting fields , it is possible to identify certain extensions within which attraction may occur. If the attractor record does not contain the attractable field, but in the attractable record at least one of the positions associated to this value is found within the identified extensions, then the field of the attractable record will be added to the attractor record along with the entire list of its positions.","title":"Definition"},{"location":"extraction/process/#attraction-with-several-attractor-records","text":"This instance can generate distributions such as the one in the following example, where record 3 can be combined both with record 1 and record 2. If NAME attracts TELEPHONE , then record 3 can be combined both with record 1 and 2, as long as there is an extension that allows this.","title":"Attraction with several attractor records"},{"location":"extraction/process/#attraction-with-several-attracted-records","text":"We assume that NAME attracts TELEPHONE . Nothing happens because the attractor record cannot be duplicated. In such cases it is necessary to check the list of positions: if one of the attracted records does not have an extension in common with the attractor, all ambiguities will disappear and records could be merged.","title":"Attraction with several attracted records"},{"location":"extraction/process/#the-algorithm_2","text":"The attraction algorithm is equivalent to the joining algorithm, the only difference is in the method to establish if a combination can occur or not. With the attraction algorithm, the fields to be considered can be more than one, and the list of positions is actually taken into consideration.","title":"The algorithm"},{"location":"extraction/process/#complex-record","text":"At the end of this process, a record consists of a set of values which refer to one or more different fields belonging to the same template and have consistently been aggregated based on the options previously set for the fields and the template themselves. For example, for a sample called PERSONAL_DATA (whose aim is to aggregate personal information related to names of people present in a document), the records could look like the following: PERSONAL_DATA NAME ADDRESS TELEPHONE Record 1 John Smith 34 Park Avenue 555 123-4567 Record 2 Sarah Parker 1280 Stanstead Rd 555 369-369 Record 3 Laura Diaz 12 Square Garden Note All fields defined in a template may not always be present (along with their value) in the final record. The ones which must necessarily appear in a record or not are determined by the options used to define the fields and the template.","title":"Complex Record"},{"location":"extraction/process/#semantics-of-attraction-rules","text":"Problems may occur when a template has several attraction rules. The following examples assume that there are one-field records such as A , B , C and so on. For instance, in: LINK B WITH A WHEN PARAGRAPH LINK C WITH A WHEN PARAGRAPH we can get: A B C A + B = AB A + C = AC AB + C = ABC AC + B = ABC Reduction is necessary to bundle everything into: ABC","title":"Semantics of attraction rules"},{"location":"extraction/process/#circular-references","text":"The following groups of attraction rules are invalid because they contain circular references. In fact they contain fields, which act both as attracting and attractable fields. LINK B WITH A WHEN PARAGRAPH LINK C WITH B WHEN PARAGRAPH LINK A WITH C WHEN PARAGRAPH or: LINK B WITH A WHEN PARAGRAPH LINK A WITH B WHEN PARAGRAPH There are cases in which such circles do not occur, but, since they depend on the extracted data, it is better to prevent them from occurring.","title":"Circular references"},{"location":"extraction/rules/","text":"Extraction rules syntax Overview The syntax of an extraction rule is: IDENTIFY[[rule label]](template) { condition } The parts between brackets ( [...] ) are optional. IDENTIFY is a language keyword and must be written in uppercase. rule label is a label that helps identify the rule. template is the name of the template for which extraction records are to be generated. condition is the rule's condition . The rule must be contained in a scope specifier: SCOPE scope_option { IDENTIFY [[ rule label ]]( template ) { condition } } For example, given the following template: TEMPLATE (PERSONAL_DATA) { @Name, @Telephone, @Address } this extraction rule: SCOPE SENTENCE { IDENTIFY(PERSONAL_DATA) { @Name[TYPE(NPH)] } } is activated by person names found in a sentence scope and, for every activation, it generates an extraction record with the PERSONAL_DATA template filling the field @Name with the person name. More rules can be put inside the same scope specifier: SCOPE scope_option { //Rule #1 IDENTIFY[[rule label]](template) { condition } //Rule #2 IDENTIFY[[rule label]](template) { condition } ... } Condition peculiarities The structure of the condition is the same for categorization rules and extraction rules however there's a fundamental peculiarity in the condition of extraction rules: the condition also specifies which template's fields are to be filled and how . The field names must be followed by operands contained in square brackets, like so: @field[operand] Note In this case brackets are mandatory, they do not indicate an optional part of the syntax. The meaning of this syntax is: the text of the token matched by the operand will be used to fill (set the value of) the field @field . Field-prefixed operands can be combined with other simple or field-prefixed operands by means of Boolean or positional sequence operators to create complex conditions: operand operator @field1[operand] operator @field2[operand] operator operand There must be at least one field-prefixed operand in the condition, meaning that every rule must extract at least one field. A field cannot be referenced more than once in the same condition except in the case of composition. The overall condition is evaluated to determine if the rule must be activated, but fields \"receive\" their value only from the single operands they are associated with. It's not possible to include operators, and therefore define expressions or use sub-rules, within a field-prefixed operand. Conditions featuring more than one field determine the so called \"by-rule aggregation\", that is the generation of records with multiple fields.","title":"Extraction rules syntax"},{"location":"extraction/rules/#extraction-rules-syntax","text":"","title":"Extraction rules syntax"},{"location":"extraction/rules/#overview","text":"The syntax of an extraction rule is: IDENTIFY[[rule label]](template) { condition } The parts between brackets ( [...] ) are optional. IDENTIFY is a language keyword and must be written in uppercase. rule label is a label that helps identify the rule. template is the name of the template for which extraction records are to be generated. condition is the rule's condition . The rule must be contained in a scope specifier: SCOPE scope_option { IDENTIFY [[ rule label ]]( template ) { condition } } For example, given the following template: TEMPLATE (PERSONAL_DATA) { @Name, @Telephone, @Address } this extraction rule: SCOPE SENTENCE { IDENTIFY(PERSONAL_DATA) { @Name[TYPE(NPH)] } } is activated by person names found in a sentence scope and, for every activation, it generates an extraction record with the PERSONAL_DATA template filling the field @Name with the person name. More rules can be put inside the same scope specifier: SCOPE scope_option { //Rule #1 IDENTIFY[[rule label]](template) { condition } //Rule #2 IDENTIFY[[rule label]](template) { condition } ... }","title":"Overview"},{"location":"extraction/rules/#condition-peculiarities","text":"The structure of the condition is the same for categorization rules and extraction rules however there's a fundamental peculiarity in the condition of extraction rules: the condition also specifies which template's fields are to be filled and how . The field names must be followed by operands contained in square brackets, like so: @field[operand] Note In this case brackets are mandatory, they do not indicate an optional part of the syntax. The meaning of this syntax is: the text of the token matched by the operand will be used to fill (set the value of) the field @field . Field-prefixed operands can be combined with other simple or field-prefixed operands by means of Boolean or positional sequence operators to create complex conditions: operand operator @field1[operand] operator @field2[operand] operator operand There must be at least one field-prefixed operand in the condition, meaning that every rule must extract at least one field. A field cannot be referenced more than once in the same condition except in the case of composition. The overall condition is evaluated to determine if the rule must be activated, but fields \"receive\" their value only from the single operands they are associated with. It's not possible to include operators, and therefore define expressions or use sub-rules, within a field-prefixed operand. Conditions featuring more than one field determine the so called \"by-rule aggregation\", that is the generation of records with multiple fields.","title":"Condition peculiarities"},{"location":"extraction/rules/byrule-aggregation/","text":"By-rule aggregation Syntax By-rule aggregation is the basic way to obtain records containing two or more fields. It consists of having more than one field-prefixed operand in a rule's condition, like the following example: //optional comment describing the rule SCOPE scope_option { IDENTIFY(template_name) { @field1_name[operand] operator @field2_name[operand] ... } } Rules with this syntax will extract fields only if their value occurs in the context of the condition and aggregate them in the output record. Single field records The syntax of the simplest ( non aggregating ) extraction rule is: SCOPE scope_option { IDENTIFY(template_name) { @field_name[operand] } } For example, given this template: TEMPLATE (PERSONAL_DATA) { @Name } this rule: SCOPE SENTENCE { IDENTIFY(PERSONAL_DATA) { @Name[TYPE(NPH)] } } would extract people's names ( TYPE(NPH) ) in the @Name field belonging to the PERSONAL_DATA template. If the rule is run against this text: Doug Smith lives at 1540 Chicago Avenue, Baltimore. the output is a record containing only the @Name field. Template : PERSONAL_DATA @Name Doug Smith If the same rule is run against this text: Doug Smith lives at 1540 Chicago Avenue, Baltimore. He lives there with his wife Norah and their two children. the output is a pair of records, each containing only the @Name field. Template : PERSONAL_DATA @Name Doug Smith Template : PERSONAL_DATA @Name Norah Two people's names are identified and extracted into two different records. Note In fact, if a rule is activated by several tokens, the engine will generate a separate record for each of them. Multiple field records Usually, extraction projects require templates with more than one field, for example: TEMPLATE (PERSONAL_DATA) { @Name, @Telephone, @Address } In this scenario, it's still possible to use single-field rules like these: SCOPE SENTENCE { IDENTIFY(PERSONAL_DATA) { @Name[TYPE(NPH)] } IDENTIFY(PERSONAL_DATA) { @Telephone[ANCESTOR(29700)]// 29700, phone number } IDENTIFY(PERSONAL_DATA) { @Address[TYPE(ADR)] } } If the above rules are used to analyze the following sample text: Doug Smith lives at 1540 Chicago Avenue, Baltimore. Doug's number is 555-234-567. the output will contain three different records: Template : PERSONAL_DATA @Name Doug Smith Template : PERSONAL_DATA @Telephone 555-234-567 Template : PERSONAL_DATA @Address 1540, Chicago Avenue - Baltimore This schema has a serious weakness: it has the highest probability of extracting unrelated data . In the example case, nothing guarantees that the phone number and the address refer to a particular person; it is not even guaranteed that they refer to a person. This can be acceptable when it is certain that input documents only contain related data. In the case of personal data, it would mean that each document always contains one ( and only one ) person's name and other data that is related to that person. Therefore, the documents would be personal records indeed, which are uncommon occurrences. On a much more frequent basis, the documents will contain both related and unrelated data and the project aim is to extract the related data only. Multi-field templates are an implicit declaration of a wanted correlation, in which it is desirable to produce output records containing related data. One way to reach this goal is to write conditions that model the relation between fields. Possible relations between fields are declared in the rule, hence the concept of by-rule aggregation . This relation can be simple co-occurrence in the same scope, positional (e.g., \"the value for field2 usually comes after the value for field1\") or syntactic (e.g \"usually the value for field1 is the subject and the value for field2 is the object). For example, this rule: SCOPE SENTENCE { IDENTIFY(PERSONAL_DATA) { @Name[TYPE(NPH)] AND @Telephone[ANCESTOR(29700)]// 29700, phone number } } extracts both @Name and @Telephone fields based on the assumption that, to be considered related, it is sufficient that they co-occur in the same sentence. When triggered, this rule produces a record that contains a pair of supposedly related fields: Template : PERSONAL_DATA @Name @Telephone Doug Smith 555-234-567 When applicable, positional sequences combined with the use of additional non-extraction constraints make conditions less loose and increase the probability of capturing real relations between extracted data. Consider the following sample rules: SCOPE SENTENCE { IDENTIFY(PERSONAL_DATA) { @Name[TYPE(NPH)] > LEMMA(\"live\") > @Address[TYPE(ADR)] } IDENTIFY(PERSONAL_DATA) { @Name[TYPE(NPH)] >> KEYWORD(\"'s\") <1:3> @Telephone[ANCESTOR(29700)]// 29700, phone number } } Here the mere presence of a person's name and other personal data in the same sentence is not enough to consider the data as related. The reciprocal position and the relation of the data are defined using positional sequences and other elements, unrelated to extraction, as additional constraints. For example, in the first rule, a person's name is extracted along with an address only if the sentence explicitly states that the person lives at a given address. By-rule aggregation leads to a higher accuracy at the price of reducing recall, so adopting it is a choice to be made based on the project goals. This is true in general whenever stricter rules are used: they potentially \"capture\" less, so more rules need to be written in order to model all the foreseeable combinations. In the example case, it could be more common to have sentences containing a couple pieces of data (for example, the person's name and his/her address) than having values for all the template fields specified in the same sentences. The rules will typically aggregate two fields at a time and the cardinal field definition and the merge option could also be used to create compound records containing the maximum number of supposedly related fields found in the same scope.","title":"By-rule aggregation"},{"location":"extraction/rules/byrule-aggregation/#by-rule-aggregation","text":"","title":"By-rule aggregation"},{"location":"extraction/rules/byrule-aggregation/#syntax","text":"By-rule aggregation is the basic way to obtain records containing two or more fields. It consists of having more than one field-prefixed operand in a rule's condition, like the following example: //optional comment describing the rule SCOPE scope_option { IDENTIFY(template_name) { @field1_name[operand] operator @field2_name[operand] ... } } Rules with this syntax will extract fields only if their value occurs in the context of the condition and aggregate them in the output record.","title":"Syntax"},{"location":"extraction/rules/byrule-aggregation/#single-field-records","text":"The syntax of the simplest ( non aggregating ) extraction rule is: SCOPE scope_option { IDENTIFY(template_name) { @field_name[operand] } } For example, given this template: TEMPLATE (PERSONAL_DATA) { @Name } this rule: SCOPE SENTENCE { IDENTIFY(PERSONAL_DATA) { @Name[TYPE(NPH)] } } would extract people's names ( TYPE(NPH) ) in the @Name field belonging to the PERSONAL_DATA template. If the rule is run against this text: Doug Smith lives at 1540 Chicago Avenue, Baltimore. the output is a record containing only the @Name field. Template : PERSONAL_DATA @Name Doug Smith If the same rule is run against this text: Doug Smith lives at 1540 Chicago Avenue, Baltimore. He lives there with his wife Norah and their two children. the output is a pair of records, each containing only the @Name field. Template : PERSONAL_DATA @Name Doug Smith Template : PERSONAL_DATA @Name Norah Two people's names are identified and extracted into two different records. Note In fact, if a rule is activated by several tokens, the engine will generate a separate record for each of them.","title":"Single field records"},{"location":"extraction/rules/byrule-aggregation/#multiple-field-records","text":"Usually, extraction projects require templates with more than one field, for example: TEMPLATE (PERSONAL_DATA) { @Name, @Telephone, @Address } In this scenario, it's still possible to use single-field rules like these: SCOPE SENTENCE { IDENTIFY(PERSONAL_DATA) { @Name[TYPE(NPH)] } IDENTIFY(PERSONAL_DATA) { @Telephone[ANCESTOR(29700)]// 29700, phone number } IDENTIFY(PERSONAL_DATA) { @Address[TYPE(ADR)] } } If the above rules are used to analyze the following sample text: Doug Smith lives at 1540 Chicago Avenue, Baltimore. Doug's number is 555-234-567. the output will contain three different records: Template : PERSONAL_DATA @Name Doug Smith Template : PERSONAL_DATA @Telephone 555-234-567 Template : PERSONAL_DATA @Address 1540, Chicago Avenue - Baltimore This schema has a serious weakness: it has the highest probability of extracting unrelated data . In the example case, nothing guarantees that the phone number and the address refer to a particular person; it is not even guaranteed that they refer to a person. This can be acceptable when it is certain that input documents only contain related data. In the case of personal data, it would mean that each document always contains one ( and only one ) person's name and other data that is related to that person. Therefore, the documents would be personal records indeed, which are uncommon occurrences. On a much more frequent basis, the documents will contain both related and unrelated data and the project aim is to extract the related data only. Multi-field templates are an implicit declaration of a wanted correlation, in which it is desirable to produce output records containing related data. One way to reach this goal is to write conditions that model the relation between fields. Possible relations between fields are declared in the rule, hence the concept of by-rule aggregation . This relation can be simple co-occurrence in the same scope, positional (e.g., \"the value for field2 usually comes after the value for field1\") or syntactic (e.g \"usually the value for field1 is the subject and the value for field2 is the object). For example, this rule: SCOPE SENTENCE { IDENTIFY(PERSONAL_DATA) { @Name[TYPE(NPH)] AND @Telephone[ANCESTOR(29700)]// 29700, phone number } } extracts both @Name and @Telephone fields based on the assumption that, to be considered related, it is sufficient that they co-occur in the same sentence. When triggered, this rule produces a record that contains a pair of supposedly related fields: Template : PERSONAL_DATA @Name @Telephone Doug Smith 555-234-567 When applicable, positional sequences combined with the use of additional non-extraction constraints make conditions less loose and increase the probability of capturing real relations between extracted data. Consider the following sample rules: SCOPE SENTENCE { IDENTIFY(PERSONAL_DATA) { @Name[TYPE(NPH)] > LEMMA(\"live\") > @Address[TYPE(ADR)] } IDENTIFY(PERSONAL_DATA) { @Name[TYPE(NPH)] >> KEYWORD(\"'s\") <1:3> @Telephone[ANCESTOR(29700)]// 29700, phone number } } Here the mere presence of a person's name and other personal data in the same sentence is not enough to consider the data as related. The reciprocal position and the relation of the data are defined using positional sequences and other elements, unrelated to extraction, as additional constraints. For example, in the first rule, a person's name is extracted along with an address only if the sentence explicitly states that the person lives at a given address. By-rule aggregation leads to a higher accuracy at the price of reducing recall, so adopting it is a choice to be made based on the project goals. This is true in general whenever stricter rules are used: they potentially \"capture\" less, so more rules need to be written in order to model all the foreseeable combinations. In the example case, it could be more common to have sentences containing a couple pieces of data (for example, the person's name and his/her address) than having values for all the template fields specified in the same sentences. The rules will typically aggregate two fields at a time and the cardinal field definition and the merge option could also be used to create compound records containing the maximum number of supposedly related fields found in the same scope.","title":"Multiple field records"},{"location":"extraction/structures/","text":"Templates and fields Templates and fields make up the underlying structure of the extraction process. The syntax to define a template is: TEMPLATE(template_name) { @field1_name, @field2_name, ... } TEMPLATE is a language keyword and must be written in uppercase. template_name and field#_name can be any sequence of alphanumerical characters and underscores ( _ ); accented letters are not allowed. The template name must be unique within the project. All field names must be preceded by the at sign ( @ ). Each template must contain at least one field. If a template contains several fields, these will have to be separated by a comma. It is not possible to have two fields with the same name within the same template; however, it will be possible to define two fields with the same name, if they are in different templates. Templates must be defined in the special Config.cr source file. A template can be compared to a table with columns and rows, where fields correspond to the column headers. Templates are data receptors meaning that their fields get filled with extracted data. Every extraction project must have dedicated templates which reflect the requirements. The table below represents a sample template filled with data. Name Telephone Address Jane Doe 555-0199 123 Blue Street, New York John Smith 020 7946 0123 456 Park Lane, London Its definition is: TEMPLATE(PERSONAL_DATA) { @Name, @Telephone, @Address } This template is named PERSONAL_DATA and it contains three fields, @Name , @Telephone and @Address . It is like defining a table named PERSONAL_DATA with three columns named @Name , @Telephone and @Address . Beyond the basic syntax, advanced options can be used to control and aggregate extracted data from various fields. Some are related to single fields while others impact the behavior of the entire template. Advanced options can aggregate data into complex records, while isolated values are generated by default. Advanced options are: Field attributes: used to characterize some or all the fields in a template in a special way. Merge options: used to define if and when a template can aggregate all the extracted values and generate compound records. It is possible to use one or more of these options at the same time in the same template.","title":"Templates and fields"},{"location":"extraction/structures/#templates-and-fields","text":"Templates and fields make up the underlying structure of the extraction process. The syntax to define a template is: TEMPLATE(template_name) { @field1_name, @field2_name, ... } TEMPLATE is a language keyword and must be written in uppercase. template_name and field#_name can be any sequence of alphanumerical characters and underscores ( _ ); accented letters are not allowed. The template name must be unique within the project. All field names must be preceded by the at sign ( @ ). Each template must contain at least one field. If a template contains several fields, these will have to be separated by a comma. It is not possible to have two fields with the same name within the same template; however, it will be possible to define two fields with the same name, if they are in different templates. Templates must be defined in the special Config.cr source file. A template can be compared to a table with columns and rows, where fields correspond to the column headers. Templates are data receptors meaning that their fields get filled with extracted data. Every extraction project must have dedicated templates which reflect the requirements. The table below represents a sample template filled with data. Name Telephone Address Jane Doe 555-0199 123 Blue Street, New York John Smith 020 7946 0123 456 Park Lane, London Its definition is: TEMPLATE(PERSONAL_DATA) { @Name, @Telephone, @Address } This template is named PERSONAL_DATA and it contains three fields, @Name , @Telephone and @Address . It is like defining a table named PERSONAL_DATA with three columns named @Name , @Telephone and @Address . Beyond the basic syntax, advanced options can be used to control and aggregate extracted data from various fields. Some are related to single fields while others impact the behavior of the entire template. Advanced options can aggregate data into complex records, while isolated values are generated by default. Advanced options are: Field attributes: used to characterize some or all the fields in a template in a special way. Merge options: used to define if and when a template can aggregate all the extracted values and generate compound records. It is possible to use one or more of these options at the same time in the same template.","title":"Templates and fields"},{"location":"extraction/structures/field-attributes/","text":"Field attributes Overview Within the template definition, fields names can optionally be followed by one or more attributes \u2014not to be confused with the attributes used in categorization and extraction rules. These attributes are used to characterize fields that have special features. The syntax is: @field_name(attribute1, ...) Possible field attributes are: Attribute Meaning C Cardinal S Solitary Attributes are language keywords and must be typed in uppercase. If several attributes are specified, they will have to be separated by commas. Cardinal field The C attribute is used to mark the cardinal (i.e., fundamental) field in a template. It will affect the MERGE option causing the merging of two or more simple records, if they contain the same value for the cardinal field. The attribute must then be used in conjunction with the MERGE option and by-rule aggregation has to be used to link the cardinal field to non-cardinal fields. For example, consider the following template and extraction rules: TEMPLATE(PERSONAL_DATA) { @Name, @Telephone, @Address } SCOPE SENTENCE { IDENTIFY(PERSONAL_DATA) { @Name[TYPE(NPH)] AND @Telephone[ANCESTOR(29700)]// 29700: phone number } IDENTIFY(PERSONAL_DATA) { @Name[TYPE(NPH)] AND @Address[TYPE(ADR)] } } Both rules exhibit the so called \"by-rule aggregation\", i.e. they give value to more than one field to create multi-field records. In this case, the records are incomplete with respect to the template, because the template has three fields. If the rules are run against this text: Doug Smith lives at 1540 Chicago Avenue, Baltimore and his number is 555-234-567. the first rule will be triggered by Doug Smith and 555-234-567 and the second by Doug Smith and 1540 Chicago Avenue, Baltimore , so the first rule will generate this record: Template : PERSONAL_DATA @Name @Telephone Doug Smith 555-234-567 and the second will generate this one: Template : PERSONAL_DATA @Name @Address Doug Smith 1540, Chicago Avenue - Baltimore 1 However, if the template is changed in this way: TEMPLATE(PERSONAL_DATA) { @Name(C), @Telephone, @Address MERGE WHEN DOCUMENT } the @Name field has been marked as cardinal and the MERGE option has been added. The effect of this change can be seen in the table below: the two simple records are merged in one compound record, because they both contain the cardinal field and the value of the cardinal field is the same. After the merge, the simple contributing records are discarded. Template : PERSONAL_DATA @Name @Telephone @Address Doug Smith 555-234-567 1540, Chicago Avenue - Baltimore Non-cardinal fields must be combined with the cardinal field in the extraction rules to make their relationship explicit. Typically, rules are written to extract pairs of fields such as: Cardinal + Non-Cardinal #1 , Cardinal + Non-Cardinal #2 . To summarize, in order to merge several records which have the value of one field in common: That field must be declared as cardinal. The MERGE option must be activated. Extraction rules must extract the cardinal field and one or more non-cardinal fields. Solitary field If the attribute S is appended to one or more fields in a template, it will inhibit the normal \"bundling\" mechanism acting on the extraction records. This process is responsible for returning a single extraction value when the same token is identified multiple times, in different positions, inside a single document. In other words, tokens identified as being the same entity or concept are usually grouped together, and a single value is returned representing all instances. For example, consider the same text, the following template and extraction rule: TEMPLATE(PERSONAL_DATA) { @Name } SCOPE SENTENCE { IDENTIFY(PERSONAL_DATA) { @Name[TYPE(NPH)] } } It will generate records containing only human proper names (TYPE(NPH)) . Instead, if the field @Name is marked as solitary (S): TEMPLATE(PERSONAL_DATA) { @Name(S) } the engine will generate two separate records for each instance of the name. @Name Doug Smith Doug Smith Please note how the second instance matches his in the text, but returns Doug Smith thanks to Studio anaphora recognizer capabilities. Note that the field value is slightly different from the text. This is because the values of typed entities go through a default normalization process. \u21a9","title":"Field attributes"},{"location":"extraction/structures/field-attributes/#field-attributes","text":"","title":"Field attributes"},{"location":"extraction/structures/field-attributes/#overview","text":"Within the template definition, fields names can optionally be followed by one or more attributes \u2014not to be confused with the attributes used in categorization and extraction rules. These attributes are used to characterize fields that have special features. The syntax is: @field_name(attribute1, ...) Possible field attributes are: Attribute Meaning C Cardinal S Solitary Attributes are language keywords and must be typed in uppercase. If several attributes are specified, they will have to be separated by commas.","title":"Overview"},{"location":"extraction/structures/field-attributes/#cardinal-field","text":"The C attribute is used to mark the cardinal (i.e., fundamental) field in a template. It will affect the MERGE option causing the merging of two or more simple records, if they contain the same value for the cardinal field. The attribute must then be used in conjunction with the MERGE option and by-rule aggregation has to be used to link the cardinal field to non-cardinal fields. For example, consider the following template and extraction rules: TEMPLATE(PERSONAL_DATA) { @Name, @Telephone, @Address } SCOPE SENTENCE { IDENTIFY(PERSONAL_DATA) { @Name[TYPE(NPH)] AND @Telephone[ANCESTOR(29700)]// 29700: phone number } IDENTIFY(PERSONAL_DATA) { @Name[TYPE(NPH)] AND @Address[TYPE(ADR)] } } Both rules exhibit the so called \"by-rule aggregation\", i.e. they give value to more than one field to create multi-field records. In this case, the records are incomplete with respect to the template, because the template has three fields. If the rules are run against this text: Doug Smith lives at 1540 Chicago Avenue, Baltimore and his number is 555-234-567. the first rule will be triggered by Doug Smith and 555-234-567 and the second by Doug Smith and 1540 Chicago Avenue, Baltimore , so the first rule will generate this record: Template : PERSONAL_DATA @Name @Telephone Doug Smith 555-234-567 and the second will generate this one: Template : PERSONAL_DATA @Name @Address Doug Smith 1540, Chicago Avenue - Baltimore 1 However, if the template is changed in this way: TEMPLATE(PERSONAL_DATA) { @Name(C), @Telephone, @Address MERGE WHEN DOCUMENT } the @Name field has been marked as cardinal and the MERGE option has been added. The effect of this change can be seen in the table below: the two simple records are merged in one compound record, because they both contain the cardinal field and the value of the cardinal field is the same. After the merge, the simple contributing records are discarded. Template : PERSONAL_DATA @Name @Telephone @Address Doug Smith 555-234-567 1540, Chicago Avenue - Baltimore Non-cardinal fields must be combined with the cardinal field in the extraction rules to make their relationship explicit. Typically, rules are written to extract pairs of fields such as: Cardinal + Non-Cardinal #1 , Cardinal + Non-Cardinal #2 . To summarize, in order to merge several records which have the value of one field in common: That field must be declared as cardinal. The MERGE option must be activated. Extraction rules must extract the cardinal field and one or more non-cardinal fields.","title":"Cardinal field"},{"location":"extraction/structures/field-attributes/#solitary-field","text":"If the attribute S is appended to one or more fields in a template, it will inhibit the normal \"bundling\" mechanism acting on the extraction records. This process is responsible for returning a single extraction value when the same token is identified multiple times, in different positions, inside a single document. In other words, tokens identified as being the same entity or concept are usually grouped together, and a single value is returned representing all instances. For example, consider the same text, the following template and extraction rule: TEMPLATE(PERSONAL_DATA) { @Name } SCOPE SENTENCE { IDENTIFY(PERSONAL_DATA) { @Name[TYPE(NPH)] } } It will generate records containing only human proper names (TYPE(NPH)) . Instead, if the field @Name is marked as solitary (S): TEMPLATE(PERSONAL_DATA) { @Name(S) } the engine will generate two separate records for each instance of the name. @Name Doug Smith Doug Smith Please note how the second instance matches his in the text, but returns Doug Smith thanks to Studio anaphora recognizer capabilities. Note that the field value is slightly different from the text. This is because the values of typed entities go through a default normalization process. \u21a9","title":"Solitary field"},{"location":"extraction/structures/merge-option/","text":"Merge option The merge mechanism The merge option determines the merging of simple extraction records into compound records . Simple records disappear in the merge, so only compound records are returned as engine output. The syntax is: TEMPLATE(template_name) { @field1_name, @field2_name, ... MERGE WHEN scope } MERGE and WHEN are language keywords and must be written in uppercase. A template can have at most one merge option. scope defines the range of the merge, that is the portion of text that has to be considered as the source of simple records to be merged. If, for example, the scope is SENTENCE , all simple records originating from the same sentence will be merged in a compound record, i.e., a separate compound record is generated for every sentence from which something was extracted. Instead, if the scope is DOCUMENT , all simple records extracted from the whole document will be merged into one (literally, just one, and potentially large) compound record. Note Merging represents a notable exception to the template-table similarity, because it creates compound records that can contain more than one instance of the same field. Possible scopes are: SENTENCE CLAUSE PARAGRAPH SEGMENT SECTION DOCUMENT The scopes are illustrated in the next section of this topic. Merging is useful when extracted data can be considered related, because it's located in the same portion of text. The followings are examples of the merge option used with different scopes. Consider this template and corresponding extraction rules: TEMPLATE(PERSONAL_DATA) { @Name, @Telephone, @Address } SCOPE SENTENCE { IDENTIFY(PERSONAL_DATA) { @Name[TYPE(NPH)] } IDENTIFY(PERSONAL_DATA) { @Telephone[ANCESTOR(29700)]// 29700, phone number, } IDENTIFY(PERSONAL_DATA) { @Address[TYPE(ADR)] } } If the rules are run against this text: Doug Smith lives at 1540 Chicago Avenue, Baltimore. Doug's number is 555-234-567 the text intelligence engine will generate three records for the PERSONAL_DATA template, one with the @Name field (with two instances of value Doug Smith ), another with the @Telephone field and the last with the @Address field. Template : PERSONAL_DATA @Name Doug Smith Template : PERSONAL_DATA @Telephone 555-234-567 Template : PERSONAL_DATA @Address 1540, Chicago Avenue - Baltimore There is no aggregation: each rule fills a single field, so the generated records only contain one field. If the merge option is added to the template definition: TEMPLATE(PERSONAL_DATA) { @Name, @Telephone, @Address MERGE WHEN SENTENCE } it will cause the generation of two compound records, one per sentence: The first contains a person's name and an address. The second contains a person's name and a telephone number. Template : PERSONAL_DATA @Name @Address Doug Smith 1540, Chicago Avenue - Baltimore Template : PERSONAL_DATA @Name @Telephone Doug Smith 555-234-567 If the merge option is modified like this: MERGE WHEN DOCUMENT a single compound record will be generated for the entire document and it will have all the template fields set. Template : PERSONAL_DATA @Name @Telephone @Address Doug Smith 555-234-567 1540, Chicago Avenue - Baltimore The merge option also has a role when a cardinal field is defined in a template. In fact, the combination of the merge option with the cardinal attribute will aggregate two separate records into one, if both have the same value for the cardinal field. The syntax is: TEMPLATE (template_name) { @field1_name (C), @field2_name2, ... MERGE WHEN scope } For a more complete description of the cardinal attribute, please see the dedicated topic . Scope peculiarities Overview Just like categorization and extraction rules, merge scopes correspond to subdivisions of the input text either generated by the disambiguator ( DOCUMENT , SENTENCE and PARAGRAPH ) or defined for a specific project ( SECTION and SEGMENT ). With the exception of DOCUMENT , which is specific to the merge option, the other scopes are the same as those of categorization and extraction rules, although the merge option syntax is simpler. DOCUMENT As the name says, the DOCUMENT scope is the whole input document. Its effect is that all simple records are merged into one potentially big record , no matter the position of the text that was extracted to fill their fields. SENTENCE, CLAUSE and PARAGRAPH With SENTENCE , CLAUSE and PARAGRAPH scopes, a compound record is generated out of simple records in which the fields have been set with the values extracted from the same sentence, clause or paragraph. Unlike in categorization and extraction rules, it is not possible to use the multiplier ( * ) to extend the scope to two or more consecutive sentences or paragraphs. As in categorization and extraction rules, SENTENCE , CLAUSE and PARAGRAPH scopes can be combined with SECTION and SEGMENT scopes just like in categorization and extraction rules. The syntax is: WHEN [ SENTENCE | PARAGRAPH | CLAUSE [(type)] ] IN [ SECTION | SEGMENT ] (name) where name is the name of the section or segment, type is the optional clause type. For example, the followings are valid scopes: WHEN SENTENCE WHEN PARAGRAPH WHEN PARAGRAPH IN SEGMENT(COVER_PAGE) WHEN SENTENCE IN SECTION(TITLE) SECTION and SEGMENT With SECTION and SEGMENT , a compound record is generated out of simple records in which the fields have been set with the values extracted from the same section or segment. As in categorization and extraction rules, advanced combinations of SECTION and SEGMENT scopes can be be defined. These include: The intersection of a section with one or more segments. The intersection of two or more segments. The syntax is: WHEN SECTION (section_name:segment_name) WHEN SEGMENT (segment_name:segment_name) For example, the followings are valid scopes: WHEN SECTION (BODY) WHEN SEGMENT (SENDER) WHEN SECTION (BODY:BYLINE) WHEN SEGMENT (SENDER:ADDRESSES) WHEN SECTION (SENDER, RECEIVER:ADDRESSES) Warning MERGE WHEN SEGMENT merges records from the same segment, regardless of the scope of the underlying extraction rules. Therefore, if two segments, for example S1 and S2 , overlap and records are generated out of the overlapping zone, because the rules have one of the two segments such as S1 as its scope, the records will be merged in the exact same way twice , one for each segment, because the records will have been generated by the second segment too. Therefore, the compound records will be identical.","title":"Merge option"},{"location":"extraction/structures/merge-option/#merge-option","text":"","title":"Merge option"},{"location":"extraction/structures/merge-option/#the-merge-mechanism","text":"The merge option determines the merging of simple extraction records into compound records . Simple records disappear in the merge, so only compound records are returned as engine output. The syntax is: TEMPLATE(template_name) { @field1_name, @field2_name, ... MERGE WHEN scope } MERGE and WHEN are language keywords and must be written in uppercase. A template can have at most one merge option. scope defines the range of the merge, that is the portion of text that has to be considered as the source of simple records to be merged. If, for example, the scope is SENTENCE , all simple records originating from the same sentence will be merged in a compound record, i.e., a separate compound record is generated for every sentence from which something was extracted. Instead, if the scope is DOCUMENT , all simple records extracted from the whole document will be merged into one (literally, just one, and potentially large) compound record. Note Merging represents a notable exception to the template-table similarity, because it creates compound records that can contain more than one instance of the same field. Possible scopes are: SENTENCE CLAUSE PARAGRAPH SEGMENT SECTION DOCUMENT The scopes are illustrated in the next section of this topic. Merging is useful when extracted data can be considered related, because it's located in the same portion of text. The followings are examples of the merge option used with different scopes. Consider this template and corresponding extraction rules: TEMPLATE(PERSONAL_DATA) { @Name, @Telephone, @Address } SCOPE SENTENCE { IDENTIFY(PERSONAL_DATA) { @Name[TYPE(NPH)] } IDENTIFY(PERSONAL_DATA) { @Telephone[ANCESTOR(29700)]// 29700, phone number, } IDENTIFY(PERSONAL_DATA) { @Address[TYPE(ADR)] } } If the rules are run against this text: Doug Smith lives at 1540 Chicago Avenue, Baltimore. Doug's number is 555-234-567 the text intelligence engine will generate three records for the PERSONAL_DATA template, one with the @Name field (with two instances of value Doug Smith ), another with the @Telephone field and the last with the @Address field. Template : PERSONAL_DATA @Name Doug Smith Template : PERSONAL_DATA @Telephone 555-234-567 Template : PERSONAL_DATA @Address 1540, Chicago Avenue - Baltimore There is no aggregation: each rule fills a single field, so the generated records only contain one field. If the merge option is added to the template definition: TEMPLATE(PERSONAL_DATA) { @Name, @Telephone, @Address MERGE WHEN SENTENCE } it will cause the generation of two compound records, one per sentence: The first contains a person's name and an address. The second contains a person's name and a telephone number. Template : PERSONAL_DATA @Name @Address Doug Smith 1540, Chicago Avenue - Baltimore Template : PERSONAL_DATA @Name @Telephone Doug Smith 555-234-567 If the merge option is modified like this: MERGE WHEN DOCUMENT a single compound record will be generated for the entire document and it will have all the template fields set. Template : PERSONAL_DATA @Name @Telephone @Address Doug Smith 555-234-567 1540, Chicago Avenue - Baltimore The merge option also has a role when a cardinal field is defined in a template. In fact, the combination of the merge option with the cardinal attribute will aggregate two separate records into one, if both have the same value for the cardinal field. The syntax is: TEMPLATE (template_name) { @field1_name (C), @field2_name2, ... MERGE WHEN scope } For a more complete description of the cardinal attribute, please see the dedicated topic .","title":"The merge mechanism"},{"location":"extraction/structures/merge-option/#scope-peculiarities","text":"","title":"Scope peculiarities"},{"location":"extraction/structures/merge-option/#overview","text":"Just like categorization and extraction rules, merge scopes correspond to subdivisions of the input text either generated by the disambiguator ( DOCUMENT , SENTENCE and PARAGRAPH ) or defined for a specific project ( SECTION and SEGMENT ). With the exception of DOCUMENT , which is specific to the merge option, the other scopes are the same as those of categorization and extraction rules, although the merge option syntax is simpler.","title":"Overview"},{"location":"extraction/structures/merge-option/#document","text":"As the name says, the DOCUMENT scope is the whole input document. Its effect is that all simple records are merged into one potentially big record , no matter the position of the text that was extracted to fill their fields.","title":"DOCUMENT"},{"location":"extraction/structures/merge-option/#sentence-clause-and-paragraph","text":"With SENTENCE , CLAUSE and PARAGRAPH scopes, a compound record is generated out of simple records in which the fields have been set with the values extracted from the same sentence, clause or paragraph. Unlike in categorization and extraction rules, it is not possible to use the multiplier ( * ) to extend the scope to two or more consecutive sentences or paragraphs. As in categorization and extraction rules, SENTENCE , CLAUSE and PARAGRAPH scopes can be combined with SECTION and SEGMENT scopes just like in categorization and extraction rules. The syntax is: WHEN [ SENTENCE | PARAGRAPH | CLAUSE [(type)] ] IN [ SECTION | SEGMENT ] (name) where name is the name of the section or segment, type is the optional clause type. For example, the followings are valid scopes: WHEN SENTENCE WHEN PARAGRAPH WHEN PARAGRAPH IN SEGMENT(COVER_PAGE) WHEN SENTENCE IN SECTION(TITLE)","title":"SENTENCE, CLAUSE and PARAGRAPH"},{"location":"extraction/structures/merge-option/#section-and-segment","text":"With SECTION and SEGMENT , a compound record is generated out of simple records in which the fields have been set with the values extracted from the same section or segment. As in categorization and extraction rules, advanced combinations of SECTION and SEGMENT scopes can be be defined. These include: The intersection of a section with one or more segments. The intersection of two or more segments. The syntax is: WHEN SECTION (section_name:segment_name) WHEN SEGMENT (segment_name:segment_name) For example, the followings are valid scopes: WHEN SECTION (BODY) WHEN SEGMENT (SENDER) WHEN SECTION (BODY:BYLINE) WHEN SEGMENT (SENDER:ADDRESSES) WHEN SECTION (SENDER, RECEIVER:ADDRESSES) Warning MERGE WHEN SEGMENT merges records from the same segment, regardless of the scope of the underlying extraction rules. Therefore, if two segments, for example S1 and S2 , overlap and records are generated out of the overlapping zone, because the rules have one of the two segments such as S1 as its scope, the records will be merged in the exact same way twice , one for each segment, because the records will have been generated by the second segment too. Therefore, the compound records will be identical.","title":"SECTION and SEGMENT"},{"location":"extraction/transformation/","text":"Transformation overview By default, when an extraction rule is activated, each field is set with the text of the token matched by the operand it is associated with. With an optional transformation , instead, the field is set to data which still has to do with the matched token, but differs because it can be chosen from the disambiguation output or from the Knowledge Graph. The syntax is: @field_name[operand]|[transformation] Possible transformations are: BASE ATOM TEXT TOKEN ENTRY SMARTENTRY SYNCON PHRASE CLAUSE SENTENCE PARAGRAPH SEGMENT SECTION EXTENSION SEQUENCE SECTOR NORM Transformation names are language keywords and must be written in uppercase. The transformation name must be typed in brackets and put at the end of the field-prefixed operand preceded by a pipe character (vertical bar, | ). Each transformation is described in a dedicated topic. Note When using a transformation, the beginning and the end positions in the extraction output are those of the token matched by the operand, not those of the transformation itself. If, for example, the transformation is SENTENCE , the field will contain the whole sentence in which the token was found, but the beginning and end positions will be those of the original token, which can be a single word. If it is necessary to know the position of the whole extracted text (for example to highlight the extraction in the document text), this information could be found in the lower levels of the detailed extraction output, at the \"word\" level.","title":"Transformation overview"},{"location":"extraction/transformation/#transformation-overview","text":"By default, when an extraction rule is activated, each field is set with the text of the token matched by the operand it is associated with. With an optional transformation , instead, the field is set to data which still has to do with the matched token, but differs because it can be chosen from the disambiguation output or from the Knowledge Graph. The syntax is: @field_name[operand]|[transformation] Possible transformations are: BASE ATOM TEXT TOKEN ENTRY SMARTENTRY SYNCON PHRASE CLAUSE SENTENCE PARAGRAPH SEGMENT SECTION EXTENSION SEQUENCE SECTOR NORM Transformation names are language keywords and must be written in uppercase. The transformation name must be typed in brackets and put at the end of the field-prefixed operand preceded by a pipe character (vertical bar, | ). Each transformation is described in a dedicated topic. Note When using a transformation, the beginning and the end positions in the extraction output are those of the token matched by the operand, not those of the transformation itself. If, for example, the transformation is SENTENCE , the field will contain the whole sentence in which the token was found, but the beginning and end positions will be those of the original token, which can be a single word. If it is necessary to know the position of the whole extracted text (for example to highlight the extraction in the document text), this information could be found in the lower levels of the detailed extraction output, at the \"word\" level.","title":"Transformation overview"},{"location":"extraction/transformation/atom/","text":"ATOM ATOM transformation transforms what is matched by the field-prefixed operand into the base form of the atoms that are matched. It is suggested to use this transformation only for fields identified by the KEYWORD attribute. The syntax is: @field_name[operand]|[ATOM] This transformation is based on the concept of atom , one of the subdivisions of the input text resulting from the disambiguation process. An atom is an indivisible particle; in expert.ai terminology it is the smallest linguistic unit which is able to convey meaning. Generally, rules act on the word level of disambiguation which includes single or composite terms resulting from the semantic analysis of texts. At the word level, it's possible to have tokens corresponding to lemmas contained in the Knowledge Graph (such as emergency team ), sequences of words recognized as entities like Sept. 15, 2008 , recognized as a date, and multi-word proper nouns like. Lehman Brothers Holdings Inc. . The atom level, on the other hand, contains only single word tokens; it can be considered as a primitive disambiguation output that precedes word aggregation. The KEYWORD attribute matches tokens at the atom level . The ATOM transformation sets the field to the base form of the atom matched by the operand. To fully understand it, it's useful to compare it with the BASE transformation. Consider this sample rule: SCOPE SENTENCE { IDENTIFY(TEST) { @FIELD1[KEYWORD(\"emergency\")]|[BASE] } } The purpose of this rule is to extract the base form of anything matched by keyword emergency . In the following sentence: Emergency teams battled more than 130 fires across New South Wales. Emergency teams is recognized as an inflection of lemma emergency team . It is composed of two atoms: Emergency teams The rule above is triggered by atom Emergency that's matched by KEYWORD(\"emergency\") . While the KEYWORD attribute matches tokens at the atom level, the BASE transformation operates at the word level , as if the operand had matched the upper level token, so @FIELD1 is set with the base form of the word level token Emergency teams therefore with emergency team . If the rule condition is changed as follows: @FIELD1[KEYWORD(\"emergency\")]|[ATOM] the rule will be triggered for the same reason, but the transformation takes place at the atom level, so the base form of the atom matched by the operand ( emergency ) will be used to set the field.","title":"ATOM"},{"location":"extraction/transformation/atom/#atom","text":"ATOM transformation transforms what is matched by the field-prefixed operand into the base form of the atoms that are matched. It is suggested to use this transformation only for fields identified by the KEYWORD attribute. The syntax is: @field_name[operand]|[ATOM] This transformation is based on the concept of atom , one of the subdivisions of the input text resulting from the disambiguation process. An atom is an indivisible particle; in expert.ai terminology it is the smallest linguistic unit which is able to convey meaning. Generally, rules act on the word level of disambiguation which includes single or composite terms resulting from the semantic analysis of texts. At the word level, it's possible to have tokens corresponding to lemmas contained in the Knowledge Graph (such as emergency team ), sequences of words recognized as entities like Sept. 15, 2008 , recognized as a date, and multi-word proper nouns like. Lehman Brothers Holdings Inc. . The atom level, on the other hand, contains only single word tokens; it can be considered as a primitive disambiguation output that precedes word aggregation. The KEYWORD attribute matches tokens at the atom level . The ATOM transformation sets the field to the base form of the atom matched by the operand. To fully understand it, it's useful to compare it with the BASE transformation. Consider this sample rule: SCOPE SENTENCE { IDENTIFY(TEST) { @FIELD1[KEYWORD(\"emergency\")]|[BASE] } } The purpose of this rule is to extract the base form of anything matched by keyword emergency . In the following sentence: Emergency teams battled more than 130 fires across New South Wales. Emergency teams is recognized as an inflection of lemma emergency team . It is composed of two atoms: Emergency teams The rule above is triggered by atom Emergency that's matched by KEYWORD(\"emergency\") . While the KEYWORD attribute matches tokens at the atom level, the BASE transformation operates at the word level , as if the operand had matched the upper level token, so @FIELD1 is set with the base form of the word level token Emergency teams therefore with emergency team . If the rule condition is changed as follows: @FIELD1[KEYWORD(\"emergency\")]|[ATOM] the rule will be triggered for the same reason, but the transformation takes place at the atom level, so the base form of the atom matched by the operand ( emergency ) will be used to set the field.","title":"ATOM"},{"location":"extraction/transformation/base/","text":"BASE Overview BASE transformation transforms what is matched by the field-prefixed operand into its base form. With the exception of the KEYWORD attribute, for which the default is the TEXT transformation, and PATTERN , which always returns what is matched by the regular expression, the base form transformation is the default behavior. The syntax is: @field_name[operand ]|[BASE] The concept of base form changes depending on the value matched by the sub-condition being contained in the Knowledge Graph or not. If contained in the Knowledge Graph, the base form will be the singular form for nouns (e.g., children will be transformed into child ), the bare infinitive for verbs (for example went , goes and going will be transformed into go ), the positive form for adverbs and adjectives (for example easier will be transformed into easy ) and the most significant form for proper nouns. Consider the following example: SCOPE SENTENCE { IDENTIFY(TEST) { @FIELD1[TYPE (NOU, VER)]|[BASE] } } The purpose of this rule is to extract nouns and verbs ( TYPE (NOU, VER) ) and to transform these values into their base form. If the rule above is run against the following sentence: Emergency teams battled more than 130 fires across New South Wales. the following transformations will take place: Token text Type Final field value emergency teams Noun emergency team battled Verb battle fires Noun fire Transformation of unknown tokens If the matched token is not contained in the Knowledge Graph, a further distinction will be applied based on whether or not a virtual supernomen has been assigned to the token. In the first scenario, the base form returned depends on the type of entity and the text content. Consider this rule: SCOPE SENTENCE { IDENTIFY(PERSON) { @Person[TYPE(NPH)]|[BASE] } } It is meant to extract people's names and return their base form. Now consider this text: To stand your ground in the face of relentless criticism from a double Nobel prize-winning scientist takes a lot of guts. For engineer and materials scientist Dan Shechtman, however, years of self-belief in the face of the eminent Linus Pauling's criticisms led him to the ultimate accolade: his own Nobel prize. Shechtman was the sole winner of the Nobel prize for chemistry in 2011, for his discovery of seemingly impossible crystal structures in metal alloys. Two elements, Dan Shechtman and Linus Pauling , are not contained in the Knowledge Graph, it can be said that they are \"unknown\". They are both disambiguated as people's names so the corresponding output tokens have their meaning set to syncon ID 78452 ( person ). This is matched by operand TYPE(NPH) so the condition is met, the rule is triggered and the field @Person is extracted. The first entity appears in two different forms, Dan Shechtman and Shechtman . Given the context in which they appear, the disambiguator deduces that they refer to the same entity and chooses Dan Shechtman as the most significant form for both tokens. This representation is used as the base form and returned as the outcome of the transformation. If the same rule is applied to this text: Shechtman was born in Tel Aviv in 1941 and received his PhD from Technion, the Israel Institute of Technology in Haifa, in 1972. Shechtman again will be recognized as a person's name, but unlike the first sample sentence, only one form is contained in the text and no \"more significant\" form exists. The base form is then Shechtman , which is identical to the matched text. For the same reason, in a scenario where the extracted value is not contained in the Knowledge Graph and is not assigned a virtual supernomen, the same (the matched text) would be returned by the TEXT transformation.","title":"BASE"},{"location":"extraction/transformation/base/#base","text":"","title":"BASE"},{"location":"extraction/transformation/base/#overview","text":"BASE transformation transforms what is matched by the field-prefixed operand into its base form. With the exception of the KEYWORD attribute, for which the default is the TEXT transformation, and PATTERN , which always returns what is matched by the regular expression, the base form transformation is the default behavior. The syntax is: @field_name[operand ]|[BASE] The concept of base form changes depending on the value matched by the sub-condition being contained in the Knowledge Graph or not. If contained in the Knowledge Graph, the base form will be the singular form for nouns (e.g., children will be transformed into child ), the bare infinitive for verbs (for example went , goes and going will be transformed into go ), the positive form for adverbs and adjectives (for example easier will be transformed into easy ) and the most significant form for proper nouns. Consider the following example: SCOPE SENTENCE { IDENTIFY(TEST) { @FIELD1[TYPE (NOU, VER)]|[BASE] } } The purpose of this rule is to extract nouns and verbs ( TYPE (NOU, VER) ) and to transform these values into their base form. If the rule above is run against the following sentence: Emergency teams battled more than 130 fires across New South Wales. the following transformations will take place: Token text Type Final field value emergency teams Noun emergency team battled Verb battle fires Noun fire","title":"Overview"},{"location":"extraction/transformation/base/#transformation-of-unknown-tokens","text":"If the matched token is not contained in the Knowledge Graph, a further distinction will be applied based on whether or not a virtual supernomen has been assigned to the token. In the first scenario, the base form returned depends on the type of entity and the text content. Consider this rule: SCOPE SENTENCE { IDENTIFY(PERSON) { @Person[TYPE(NPH)]|[BASE] } } It is meant to extract people's names and return their base form. Now consider this text: To stand your ground in the face of relentless criticism from a double Nobel prize-winning scientist takes a lot of guts. For engineer and materials scientist Dan Shechtman, however, years of self-belief in the face of the eminent Linus Pauling's criticisms led him to the ultimate accolade: his own Nobel prize. Shechtman was the sole winner of the Nobel prize for chemistry in 2011, for his discovery of seemingly impossible crystal structures in metal alloys. Two elements, Dan Shechtman and Linus Pauling , are not contained in the Knowledge Graph, it can be said that they are \"unknown\". They are both disambiguated as people's names so the corresponding output tokens have their meaning set to syncon ID 78452 ( person ). This is matched by operand TYPE(NPH) so the condition is met, the rule is triggered and the field @Person is extracted. The first entity appears in two different forms, Dan Shechtman and Shechtman . Given the context in which they appear, the disambiguator deduces that they refer to the same entity and chooses Dan Shechtman as the most significant form for both tokens. This representation is used as the base form and returned as the outcome of the transformation. If the same rule is applied to this text: Shechtman was born in Tel Aviv in 1941 and received his PhD from Technion, the Israel Institute of Technology in Haifa, in 1972. Shechtman again will be recognized as a person's name, but unlike the first sample sentence, only one form is contained in the text and no \"more significant\" form exists. The base form is then Shechtman , which is identical to the matched text. For the same reason, in a scenario where the extracted value is not contained in the Knowledge Graph and is not assigned a virtual supernomen, the same (the matched text) would be returned by the TEXT transformation.","title":"Transformation of unknown tokens"},{"location":"extraction/transformation/clause/","text":"CLAUSE CLAUSE is an extraction transformation option that can be described as a completion feature of the matched value rather than a normalization. It adds elements surrounding the original matched data to the final extracted value. Its action is based on the concept of \"clause\", which is the smallest grammatical unit that can express a complete proposition. The recognition of one or more clauses in a sentence takes place during the disambiguation process. The CLAUSE option returns the whole clause containing the value matched by an attribute. The syntax of the CLAUSE option is the following: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute]|[CLAUSE] } } This option is useful in situations where it's necessary to expand the extraction output revolving around a matched element. Consider the following example: SCOPE SENTENCE { IDENTIFY(SUBJECT) { @Subject[TYPE(NPH) + ROLE(SUBJECT)]|[CLAUSE] } } This rule's purpose is to extract human proper nouns ( TYPE(NPH) ) only if the names identified are the subjects of a sentence or a clause ( + ROLE(SUBJECT) ). If this condition is verified, the CLAUSE transformation option will ensure that every extracted value will be expanded to the clause where the people's names are found as subjects. Consider the extraction output if the rule above is run against the following sample sentence: Assistant Commissioner Simon Byrne described detectives as \"constables in T-shirts and jeans\" and said he wanted to end the division between uniformed officers and detectives. The text contains one value matching the sample rule: Simon Byrne . This concept is recognized as a person's name and is the subject of the first clause of the sentence. Due to the CLAUSE transformation, the whole clause in which the person's name was found is extracted: Assistant Commissioner Simon Byrne described detectives as \"constables in T-shirts and jeans . Note If a sentence does not contain any verb, no clause will be recognized by the disambiguator. In that case, no extraction normalized with CLAUSE is returned.","title":"CLAUSE"},{"location":"extraction/transformation/clause/#clause","text":"CLAUSE is an extraction transformation option that can be described as a completion feature of the matched value rather than a normalization. It adds elements surrounding the original matched data to the final extracted value. Its action is based on the concept of \"clause\", which is the smallest grammatical unit that can express a complete proposition. The recognition of one or more clauses in a sentence takes place during the disambiguation process. The CLAUSE option returns the whole clause containing the value matched by an attribute. The syntax of the CLAUSE option is the following: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute]|[CLAUSE] } } This option is useful in situations where it's necessary to expand the extraction output revolving around a matched element. Consider the following example: SCOPE SENTENCE { IDENTIFY(SUBJECT) { @Subject[TYPE(NPH) + ROLE(SUBJECT)]|[CLAUSE] } } This rule's purpose is to extract human proper nouns ( TYPE(NPH) ) only if the names identified are the subjects of a sentence or a clause ( + ROLE(SUBJECT) ). If this condition is verified, the CLAUSE transformation option will ensure that every extracted value will be expanded to the clause where the people's names are found as subjects. Consider the extraction output if the rule above is run against the following sample sentence: Assistant Commissioner Simon Byrne described detectives as \"constables in T-shirts and jeans\" and said he wanted to end the division between uniformed officers and detectives. The text contains one value matching the sample rule: Simon Byrne . This concept is recognized as a person's name and is the subject of the first clause of the sentence. Due to the CLAUSE transformation, the whole clause in which the person's name was found is extracted: Assistant Commissioner Simon Byrne described detectives as \"constables in T-shirts and jeans . Note If a sentence does not contain any verb, no clause will be recognized by the disambiguator. In that case, no extraction normalized with CLAUSE is returned.","title":"CLAUSE"},{"location":"extraction/transformation/entry/","text":"ENTRY ENTRY is the extraction transformation option that transforms what is matched by the attribute into its most significant base form. This option is used to normalize extraction values known to the Knowledge Graph. The syntax of the ENTRY option is the following: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute]|[ENTRY] } } To use this option, the attribute chosen should be among those capable of recognizing elements found within the Knowledge Graph (see SYNCON , ANCESTOR , LIST excluding the UNKNOWN elements: syncon and ancestor). The ENTRY option returns a constant form when a concept (syncon) is identified in a text in all its possible forms and variations contained in the Knowledge Graph. The constant form returned corresponds to the base form of the syncon's \"main lemma\". The main lemma is the most representative word for a given concept. In other words, in a syncon that contains many lemmas, the main lemma is the most commonly referred to word. This parameter is associated to a lemma during the Knowledge Graph enrichment phase; the main lemma, therefore, can be considered as a predefined attribute of a lemma. Consider the following example: SCOPE SENTENCE { IDENTIFY(COMPANY) { @COMPANY_NAME[ANCESTOR(37475) + TYPE(NPR) - SYNCON(UNKNOWN)]|[ENTRY] } } The purpose of this rule is to extract a chain of proper noun concepts ( + TYPE(NPR) ) starting from syncon 37475 ( company ), only if the identified concepts are not \"unknown\" to the Knowledge Graph ( - SYNCON(UNKNOWN) ); in other words, extract proper names of companies found within the Knowledge Graph. If this condition is verified, the ENTRY transformation option will ensure that every form that a company's name can take is transformed into the syncon main lemma. This allows a concept to have one consistent extraction value even though the concept may appear in several different forms in a text. Consider the extraction output if the above rule is run against the following sample sentence: The equities index is 20 percent above its level on Sept. 15, 2008, the first trading day after Lehman Brothers Holdings Inc. filed the world's biggest bankruptcy and prompted a 46 percent drop through March 9, 2009. Lehman Brothers is having a great year. The bank, which almost destroyed the global economy four years ago this week, recently emerged from bankruptcy, resolved a third of its debts and executed the largest U.S. real estate deal of the year. The text contains two values matching the sample rule: Lehman Brothers Holdings Inc. and Lehman Brothers , which are both analyzed as companies. The disambiguator also recognizes these two names as the same company associated to syncon 317862; in the Knowledge Graph, this syncon contains five different forms referring to the same concept. The extraction panel shows that the extracted value is its main lemma, Lehman Brothers , while the text record shows the two instances found in the text: Lehman Brothers Holdings Inc. and Lehman Brothers . This means that the extracted values have been transformed\u2014and normalized\u2014into the main lemma thanks to the ENTRY option. Please note that if the ENTRY option is used with a value not contained in the Knowledge Graph, but which received a virtual supernomen, the value returned will be the main lemma of its virtual supernomen (see SMARTENTRY for more details).","title":"ENTRY"},{"location":"extraction/transformation/entry/#entry","text":"ENTRY is the extraction transformation option that transforms what is matched by the attribute into its most significant base form. This option is used to normalize extraction values known to the Knowledge Graph. The syntax of the ENTRY option is the following: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute]|[ENTRY] } } To use this option, the attribute chosen should be among those capable of recognizing elements found within the Knowledge Graph (see SYNCON , ANCESTOR , LIST excluding the UNKNOWN elements: syncon and ancestor). The ENTRY option returns a constant form when a concept (syncon) is identified in a text in all its possible forms and variations contained in the Knowledge Graph. The constant form returned corresponds to the base form of the syncon's \"main lemma\". The main lemma is the most representative word for a given concept. In other words, in a syncon that contains many lemmas, the main lemma is the most commonly referred to word. This parameter is associated to a lemma during the Knowledge Graph enrichment phase; the main lemma, therefore, can be considered as a predefined attribute of a lemma. Consider the following example: SCOPE SENTENCE { IDENTIFY(COMPANY) { @COMPANY_NAME[ANCESTOR(37475) + TYPE(NPR) - SYNCON(UNKNOWN)]|[ENTRY] } } The purpose of this rule is to extract a chain of proper noun concepts ( + TYPE(NPR) ) starting from syncon 37475 ( company ), only if the identified concepts are not \"unknown\" to the Knowledge Graph ( - SYNCON(UNKNOWN) ); in other words, extract proper names of companies found within the Knowledge Graph. If this condition is verified, the ENTRY transformation option will ensure that every form that a company's name can take is transformed into the syncon main lemma. This allows a concept to have one consistent extraction value even though the concept may appear in several different forms in a text. Consider the extraction output if the above rule is run against the following sample sentence: The equities index is 20 percent above its level on Sept. 15, 2008, the first trading day after Lehman Brothers Holdings Inc. filed the world's biggest bankruptcy and prompted a 46 percent drop through March 9, 2009. Lehman Brothers is having a great year. The bank, which almost destroyed the global economy four years ago this week, recently emerged from bankruptcy, resolved a third of its debts and executed the largest U.S. real estate deal of the year. The text contains two values matching the sample rule: Lehman Brothers Holdings Inc. and Lehman Brothers , which are both analyzed as companies. The disambiguator also recognizes these two names as the same company associated to syncon 317862; in the Knowledge Graph, this syncon contains five different forms referring to the same concept. The extraction panel shows that the extracted value is its main lemma, Lehman Brothers , while the text record shows the two instances found in the text: Lehman Brothers Holdings Inc. and Lehman Brothers . This means that the extracted values have been transformed\u2014and normalized\u2014into the main lemma thanks to the ENTRY option. Please note that if the ENTRY option is used with a value not contained in the Knowledge Graph, but which received a virtual supernomen, the value returned will be the main lemma of its virtual supernomen (see SMARTENTRY for more details).","title":"ENTRY"},{"location":"extraction/transformation/extension/","text":"EXTENSION EXTENSION is an extraction transformation option that can be described as a completion feature of the matched value rather than a normalization. It adds elements surrounding the original matched data to the final extracted value. Its action is based on the concept of \"extension\", which in turn is linked to the SCOPE statement (the part of the rule's syntax in which the portion of text to which a rule applies is specified). The EXTENSION option reads what the SCOPE statement (found at the beginning of the rule) contains and returns the portion of text containing the value matched by an attribute as defined in the SCOPE statement. In this way, EXTENSION dynamically reproduces what the options PHRASE , CLAUSE , SENTENCE , PARAGRAPH , SEGMENT and SECTION do without having to choose the extension beforehand. The syntax of the EXTENSION option is the following: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute]|[EXTENSION] } } This option is useful in cases where it's necessary to expand the extraction output revolving around a matched element, to include exactly what the extension of the rule is; whereas the options PHRASE , CLAUSE , SENTENCE , PARAGRAPH , SEGMENT and SECTION allow the extension of the rule to be different from the transformation option. Consider the following example: SCOPE CLAUSE { IDENTIFY(SUBJECT) { @Subject[TYPE(NPH) + ROLE(SUBJECT)]|[EXTENSION] } } The purpose of this rule is to extract people's names ( TYPE (NPH) ), only if the names identified are the subjects of a clause ( + ROLE (SUBJECT) ). If this condition is verified, the EXTENSION transformation option will ensure that every extracted value will be expanded to the defined extension in the SCOPE statement of the rule, in this case, a clause. Consider the extraction output if the rule above is run against the following sample sentence: Assistant Commissioner Simon Byrne described detectives as \"constables in T-shirts and jeans\" and said he wanted to end the division between uniformed officers and detectives. The text contains one value matching the sample rule: Simon Byrne . Simon Byrne is recognized as both a person's name and as the subject in the first clause of the sentence, so it causes the rule to trigger while the EXTENSION transformation determines the extraction of the extension defined in the SCOPE statement of the rule, the clause to which the person's name belongs, which is Assistant Commissioner Simon Byrne described detectives as \"constables in T-shirts and jeans\" . If the rule's scope is changed to PHRASE and the transformation remains unchanged, the extraction will become the phrase that contains the person's name: Assistant Commissioner Simon Byrne .","title":"EXTENSION"},{"location":"extraction/transformation/extension/#extension","text":"EXTENSION is an extraction transformation option that can be described as a completion feature of the matched value rather than a normalization. It adds elements surrounding the original matched data to the final extracted value. Its action is based on the concept of \"extension\", which in turn is linked to the SCOPE statement (the part of the rule's syntax in which the portion of text to which a rule applies is specified). The EXTENSION option reads what the SCOPE statement (found at the beginning of the rule) contains and returns the portion of text containing the value matched by an attribute as defined in the SCOPE statement. In this way, EXTENSION dynamically reproduces what the options PHRASE , CLAUSE , SENTENCE , PARAGRAPH , SEGMENT and SECTION do without having to choose the extension beforehand. The syntax of the EXTENSION option is the following: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute]|[EXTENSION] } } This option is useful in cases where it's necessary to expand the extraction output revolving around a matched element, to include exactly what the extension of the rule is; whereas the options PHRASE , CLAUSE , SENTENCE , PARAGRAPH , SEGMENT and SECTION allow the extension of the rule to be different from the transformation option. Consider the following example: SCOPE CLAUSE { IDENTIFY(SUBJECT) { @Subject[TYPE(NPH) + ROLE(SUBJECT)]|[EXTENSION] } } The purpose of this rule is to extract people's names ( TYPE (NPH) ), only if the names identified are the subjects of a clause ( + ROLE (SUBJECT) ). If this condition is verified, the EXTENSION transformation option will ensure that every extracted value will be expanded to the defined extension in the SCOPE statement of the rule, in this case, a clause. Consider the extraction output if the rule above is run against the following sample sentence: Assistant Commissioner Simon Byrne described detectives as \"constables in T-shirts and jeans\" and said he wanted to end the division between uniformed officers and detectives. The text contains one value matching the sample rule: Simon Byrne . Simon Byrne is recognized as both a person's name and as the subject in the first clause of the sentence, so it causes the rule to trigger while the EXTENSION transformation determines the extraction of the extension defined in the SCOPE statement of the rule, the clause to which the person's name belongs, which is Assistant Commissioner Simon Byrne described detectives as \"constables in T-shirts and jeans\" . If the rule's scope is changed to PHRASE and the transformation remains unchanged, the extraction will become the phrase that contains the person's name: Assistant Commissioner Simon Byrne .","title":"EXTENSION"},{"location":"extraction/transformation/norm/","text":"NORM NORM is the extraction transformation option that transforms what is matched by the attribute into the entry of one of its ancestors. Its action is based on the principle that every syncon in the Knowledge Graph belongs to at least one chain of concepts (is linked to one or more syncons). As a standard (for example with the ANCESTOR attribute), various links are navigated, going from the most general level to the most specific one. Using the NORM transformation option, it is possible to \"invert\" the direction and explore the available links upwards in the hierarchy. In this way, a generalization may be achieved using the extracted data. The syntax of the NORM option is the following: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute]|[NORM ID:levels:linkname] } } NORM keyword is followed by: ID : refers to the univocal identifier that belongs to each and every syncon contained in the Knowledge Graph. Results of the NORM option could be observed, only if the element used in the extraction part of the rule Attribute and the syncon ID specified with the NORM option are semantically linked (they belong to the same chain of concepts). levels : refers to the number of levels that must be navigated upwards in the concepts hierarchy when searching for the syncon that will be returned in the output. The number of levels ranges from 0 to 99, where 0 is the farthest ancestor in the chain and 99 is the default representing the concept extracted by the rule itself (the action of the option NORM is nullified). linkname : refers to the link to be navigated when looking for ancestors. Valid links are those available in the Knowledge Graph, including any custom link added to the Knowledge Graph for a specific project. Consider the following example: SCOPE SENTENCE { IDENTIFY(PLACE) { @Place[ANCESTOR(16226414:99:syncon/geography) + TYPE(NPR)]|[NORM 16226414:1:syncon/geography] } } The purpose of this rule is to to extract proper nouns ( TYPE(NPR) ) of places which are found in Oceania (syncon 16226414) and which are linked to each other with the semantic relationship representing the geographic inclusion on the world map ( syncon/geography ). If this condition is verified, the NORM option will go back up one level in the hierarchy ( :1 ) starting from the concept matched by the ANCESTOR attribute in order to return a value which is more general than the matched one. In other words, the value to be returned will be the father of the matched value. Consider the extraction output if the rule above is run against the following sample text: An Air New Zealand flight from Wellington to Sydney this morning followed an unusual flight pattern after turning back for a \"technical stop\". Air New Zealand flight NZ845 left Wellington Airport bound for Sydney but appeared to turn back off Nelson before circling off the Kapiti Coast and then heading for Auckland. The Airbus A320 was scheduled to depart Wellington at 6:40am, Wellington time, and arrive in Sydney at 8:20am local time - a three-hour flight. The text contains several terms matched by the sample rule: Sydney , Auckland and Wellington , each repeated several times. These terms are recognized as places in Oceania, the ancestor concept specified in the extraction rule, and become extraction \"candidates\". The NORM option, however, normalizes and aggregates the values to be returned: Auckland and Wellington (two values) are transformed into New Zealand (one value) and Sydney is transformed into Australia . In other words, the rule would extract all places in Oceania, but the transformation forces it to only return the names of the countries in which these places are located. The NORM option provides a greater or lesser degree of generalization. Consider the sample rule with a modification to the NORM transformation: SCOPE SENTENCE { IDENTIFY(PLACE) { @Place[ANCESTOR(16226414:99:syncon/geography)+ TYPE(NPR)]|[NORM 16226414:0:syncon/geography] } } The purpose of this transformation is to return the most general common ancestor for all elements recognized by the extraction rule. If the rule is applied to the same sample text, it will only extract Oceania . The examples presented so far use only one type of link for both the extraction rule and the NORM transformation. Different links, however, may also be used. Consider the following rule: SCOPE SENTENCE { IDENTIFY(PLACE) { @Place[ANCESTOR(16226414:99:syncon/geography)+ TYPE(NPR)]|[NORM 78660:2:supernomen/subnomen] } } The purpose of this rule is extract proper nouns ( TYPE(NPR) ) of places in Oceania (syncon 16226414) that are linked to each other by the semantic relationship representing the geographic inclusion on the world map (syncon/geography). If this condition is verified, the NORM option will navigate the supernomen/subnomen chain (\"type of\" relationship) starting from the concept of geographic place (syncon 78660), then it will go up two levels in the hierarchy ( :2 ) starting from the concept matched by the ANCESTOR attribute and will not return the value which is more general than the matched one - as in the previous examples - but a word that can be considered as a class (\"type of\" link) to which the extracted entity belongs. In other words, the value to be returned will be the grandfather of the matched value in a concept hierarchy, different from the one specified in the extraction sub-condition. If the rule above is run against the same sample text, Sydney and Wellington will be transformed into urban area . Inside the supernomen/subnomen hierarchy, in fact, Sydney is a capital , which is in turn a type of city , and a city is a type of urban area , so urban area , which is two levels up in the chain starting from Sydney , is the concept returned as the final extraction output.","title":"NORM"},{"location":"extraction/transformation/norm/#norm","text":"NORM is the extraction transformation option that transforms what is matched by the attribute into the entry of one of its ancestors. Its action is based on the principle that every syncon in the Knowledge Graph belongs to at least one chain of concepts (is linked to one or more syncons). As a standard (for example with the ANCESTOR attribute), various links are navigated, going from the most general level to the most specific one. Using the NORM transformation option, it is possible to \"invert\" the direction and explore the available links upwards in the hierarchy. In this way, a generalization may be achieved using the extracted data. The syntax of the NORM option is the following: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute]|[NORM ID:levels:linkname] } } NORM keyword is followed by: ID : refers to the univocal identifier that belongs to each and every syncon contained in the Knowledge Graph. Results of the NORM option could be observed, only if the element used in the extraction part of the rule Attribute and the syncon ID specified with the NORM option are semantically linked (they belong to the same chain of concepts). levels : refers to the number of levels that must be navigated upwards in the concepts hierarchy when searching for the syncon that will be returned in the output. The number of levels ranges from 0 to 99, where 0 is the farthest ancestor in the chain and 99 is the default representing the concept extracted by the rule itself (the action of the option NORM is nullified). linkname : refers to the link to be navigated when looking for ancestors. Valid links are those available in the Knowledge Graph, including any custom link added to the Knowledge Graph for a specific project. Consider the following example: SCOPE SENTENCE { IDENTIFY(PLACE) { @Place[ANCESTOR(16226414:99:syncon/geography) + TYPE(NPR)]|[NORM 16226414:1:syncon/geography] } } The purpose of this rule is to to extract proper nouns ( TYPE(NPR) ) of places which are found in Oceania (syncon 16226414) and which are linked to each other with the semantic relationship representing the geographic inclusion on the world map ( syncon/geography ). If this condition is verified, the NORM option will go back up one level in the hierarchy ( :1 ) starting from the concept matched by the ANCESTOR attribute in order to return a value which is more general than the matched one. In other words, the value to be returned will be the father of the matched value. Consider the extraction output if the rule above is run against the following sample text: An Air New Zealand flight from Wellington to Sydney this morning followed an unusual flight pattern after turning back for a \"technical stop\". Air New Zealand flight NZ845 left Wellington Airport bound for Sydney but appeared to turn back off Nelson before circling off the Kapiti Coast and then heading for Auckland. The Airbus A320 was scheduled to depart Wellington at 6:40am, Wellington time, and arrive in Sydney at 8:20am local time - a three-hour flight. The text contains several terms matched by the sample rule: Sydney , Auckland and Wellington , each repeated several times. These terms are recognized as places in Oceania, the ancestor concept specified in the extraction rule, and become extraction \"candidates\". The NORM option, however, normalizes and aggregates the values to be returned: Auckland and Wellington (two values) are transformed into New Zealand (one value) and Sydney is transformed into Australia . In other words, the rule would extract all places in Oceania, but the transformation forces it to only return the names of the countries in which these places are located. The NORM option provides a greater or lesser degree of generalization. Consider the sample rule with a modification to the NORM transformation: SCOPE SENTENCE { IDENTIFY(PLACE) { @Place[ANCESTOR(16226414:99:syncon/geography)+ TYPE(NPR)]|[NORM 16226414:0:syncon/geography] } } The purpose of this transformation is to return the most general common ancestor for all elements recognized by the extraction rule. If the rule is applied to the same sample text, it will only extract Oceania . The examples presented so far use only one type of link for both the extraction rule and the NORM transformation. Different links, however, may also be used. Consider the following rule: SCOPE SENTENCE { IDENTIFY(PLACE) { @Place[ANCESTOR(16226414:99:syncon/geography)+ TYPE(NPR)]|[NORM 78660:2:supernomen/subnomen] } } The purpose of this rule is extract proper nouns ( TYPE(NPR) ) of places in Oceania (syncon 16226414) that are linked to each other by the semantic relationship representing the geographic inclusion on the world map (syncon/geography). If this condition is verified, the NORM option will navigate the supernomen/subnomen chain (\"type of\" relationship) starting from the concept of geographic place (syncon 78660), then it will go up two levels in the hierarchy ( :2 ) starting from the concept matched by the ANCESTOR attribute and will not return the value which is more general than the matched one - as in the previous examples - but a word that can be considered as a class (\"type of\" link) to which the extracted entity belongs. In other words, the value to be returned will be the grandfather of the matched value in a concept hierarchy, different from the one specified in the extraction sub-condition. If the rule above is run against the same sample text, Sydney and Wellington will be transformed into urban area . Inside the supernomen/subnomen hierarchy, in fact, Sydney is a capital , which is in turn a type of city , and a city is a type of urban area , so urban area , which is two levels up in the chain starting from Sydney , is the concept returned as the final extraction output.","title":"NORM"},{"location":"extraction/transformation/paragraph/","text":"PARAGRAPH PARAGRAPH is an extraction transformation option that can be described as a completion feature of the matched value rather than a normalization. It adds elements surrounding the original matched data to the final extracted value. Its action is based on the concept of \"paragraph\", a unit of a discourse in writing, dealing with a particular idea. It consists of one or more sentences and its start is typically indicated by the beginning of a new line. The recognition of paragraphs takes place during the disambiguation process. The PARAGRAPH option returns the whole paragraph containing the value matched by an attribute. The syntax of the PARAGRAPH option is the following: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute]|[PARAGRAPH] } } This option is useful in situations where it's necessary to expand the extraction output revolving around a matched element. Consider the following example: SCOPE SENTENCE { IDENTIFY(SUBJECT) { @Subject[TYPE(NPH) + ROLE(SUBJECT)]|[PARAGRAPH] } } The purpose of this rule is to extract people's names ( TYPE(NPH) ), only if the names identified are the subjects of a sentence or clause ( + ROLE(SUBJECT) ). If this condition is verified, the PARAGRAPH transformation option will ensure that every extracted value will be expanded to the paragraph where the people's names are found as a subject. Consider the extraction output if the rule above is run against the following sample text: Scotland Yard is to close 65 police stations to the public across London and move its front desks into post offices and supermarkets as part of proposals to make \u00a3500m budget cuts. In a blueprint for the future that will see the role of the detective at the Yard - once considered to have the finest investigators in the world - apparently downgraded, 1,200 more constables will be put into boroughs, and neighborhood teams will be boosted by 2,600 officers. Closing police stations mapped. Click image to explore it Eight hundred of the 1,200 extra constables will be detectives who are to be taken out of specialist squads, such as the burglary squad, and put back into uniform and on to the streets. The aim is to hand investigative powers to neighborhood constables for low-level crime. They will be led by a \"sheriff\" in each London borough, and will be supported by teams of special constables, PCSOs and some detectives within each of the 32 boroughs of the force. Assistant Commissioner Simon Byrne described detectives as \"constables in T-shirts and jeans\" and said he wanted to end the division between uniformed officers and detectives. While carrying out what the commissioner has in the past admitted are huge cuts to the budget, the mayor's office wants public confidence in the police to rise from 62% to about 75%, and to reduce crime in seven key areas by 20%. The mayor's office for policing and crime confirmed that the Scotland Yard building in central London would be one of 200 sold off as part of the cuts. Within six months a pilot of putting police officers into post offices will be unveiled. Byrne said: \"This is about fundamental change. I think we can demonstrate that we care about local priorities. The way that neighborhood policing in London is run compared with other examples of good practice is that we can do more. The text contains two values matching the sample rule: Simon Byrne and Byrne . Both are recognized as people's names and are the subject of a clause. The PARAGRAPH transformation causes the extraction of the whole paragraph in which the people's name were found, that is: Assistant Commissioner Simon Byrne described detectives as \"constables in T-shirts and jeans\" and said he wanted to end the division between uniformed officers and detectives. and: Byrne said: \"This is about fundamental change. I think we can demonstrate that we care about local priorities. The way that neighborhood policing in London is run compared with other examples of good practice is that we can do more.","title":"PARAGRAPH"},{"location":"extraction/transformation/paragraph/#paragraph","text":"PARAGRAPH is an extraction transformation option that can be described as a completion feature of the matched value rather than a normalization. It adds elements surrounding the original matched data to the final extracted value. Its action is based on the concept of \"paragraph\", a unit of a discourse in writing, dealing with a particular idea. It consists of one or more sentences and its start is typically indicated by the beginning of a new line. The recognition of paragraphs takes place during the disambiguation process. The PARAGRAPH option returns the whole paragraph containing the value matched by an attribute. The syntax of the PARAGRAPH option is the following: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute]|[PARAGRAPH] } } This option is useful in situations where it's necessary to expand the extraction output revolving around a matched element. Consider the following example: SCOPE SENTENCE { IDENTIFY(SUBJECT) { @Subject[TYPE(NPH) + ROLE(SUBJECT)]|[PARAGRAPH] } } The purpose of this rule is to extract people's names ( TYPE(NPH) ), only if the names identified are the subjects of a sentence or clause ( + ROLE(SUBJECT) ). If this condition is verified, the PARAGRAPH transformation option will ensure that every extracted value will be expanded to the paragraph where the people's names are found as a subject. Consider the extraction output if the rule above is run against the following sample text: Scotland Yard is to close 65 police stations to the public across London and move its front desks into post offices and supermarkets as part of proposals to make \u00a3500m budget cuts. In a blueprint for the future that will see the role of the detective at the Yard - once considered to have the finest investigators in the world - apparently downgraded, 1,200 more constables will be put into boroughs, and neighborhood teams will be boosted by 2,600 officers. Closing police stations mapped. Click image to explore it Eight hundred of the 1,200 extra constables will be detectives who are to be taken out of specialist squads, such as the burglary squad, and put back into uniform and on to the streets. The aim is to hand investigative powers to neighborhood constables for low-level crime. They will be led by a \"sheriff\" in each London borough, and will be supported by teams of special constables, PCSOs and some detectives within each of the 32 boroughs of the force. Assistant Commissioner Simon Byrne described detectives as \"constables in T-shirts and jeans\" and said he wanted to end the division between uniformed officers and detectives. While carrying out what the commissioner has in the past admitted are huge cuts to the budget, the mayor's office wants public confidence in the police to rise from 62% to about 75%, and to reduce crime in seven key areas by 20%. The mayor's office for policing and crime confirmed that the Scotland Yard building in central London would be one of 200 sold off as part of the cuts. Within six months a pilot of putting police officers into post offices will be unveiled. Byrne said: \"This is about fundamental change. I think we can demonstrate that we care about local priorities. The way that neighborhood policing in London is run compared with other examples of good practice is that we can do more. The text contains two values matching the sample rule: Simon Byrne and Byrne . Both are recognized as people's names and are the subject of a clause. The PARAGRAPH transformation causes the extraction of the whole paragraph in which the people's name were found, that is: Assistant Commissioner Simon Byrne described detectives as \"constables in T-shirts and jeans\" and said he wanted to end the division between uniformed officers and detectives. and: Byrne said: \"This is about fundamental change. I think we can demonstrate that we care about local priorities. The way that neighborhood policing in London is run compared with other examples of good practice is that we can do more.","title":"PARAGRAPH"},{"location":"extraction/transformation/phrase/","text":"PHRASE PHRASE is an extraction transformation option that can be described as a completion feature of the matched value rather than a normalization. It adds elements surrounding the original matched data to the final extracted value. Its action is based on the concept of \"phrase\", one of the sentence subdivision levels resulting from the disambiguation process. Phrases are a group of words (or sometimes single words) that form constituents acting as single units in the syntax of a sentence; for example: noun phrases, verb phrases, prepositional phrase, etc. The PHRASE option returns the whole group containing the value matched by an attribute. The syntax of the PHRASE option is the following: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute]|[`PHRASE`] } } This option is useful in situations where it's necessary to expand the extraction output revolving around a matched element. Consider the following example: SCOPE SENTENCE { IDENTIFY(SUBJECT) { @Subject[TYPE(NPH) + ROLE(SUBJECT)]|[PHRASE] } } The purpose of this rule is to extract human proper nouns ( TYPE(NPH) ) only if the names identified are the subjects of a sentence or clause ( + ROLE(SUBJECT) ). If this condition is verified, the PHRASE transformation option will ensure that every extracted value will be expanded to the phrase where the people's names are found as subjects. Consider the extraction output if the rule above is run against the following sample sentence: Assistant Commissioner Simon Byrne described detectives as \"constables in T-shirts and jeans\" and said he wanted to end the division between uniformed officers and detectives. The text contains one value matching the sample rule: Simon Byrne . This concept is recognized as a person's name and is the subject of the first clause of the sentence. Simon Byrne is also part of a broader noun phrase. The extraction would be the whole group in which the person's name was found: Assistant Commissioner Simon Byrne .","title":"PHRASE"},{"location":"extraction/transformation/phrase/#phrase","text":"PHRASE is an extraction transformation option that can be described as a completion feature of the matched value rather than a normalization. It adds elements surrounding the original matched data to the final extracted value. Its action is based on the concept of \"phrase\", one of the sentence subdivision levels resulting from the disambiguation process. Phrases are a group of words (or sometimes single words) that form constituents acting as single units in the syntax of a sentence; for example: noun phrases, verb phrases, prepositional phrase, etc. The PHRASE option returns the whole group containing the value matched by an attribute. The syntax of the PHRASE option is the following: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute]|[`PHRASE`] } } This option is useful in situations where it's necessary to expand the extraction output revolving around a matched element. Consider the following example: SCOPE SENTENCE { IDENTIFY(SUBJECT) { @Subject[TYPE(NPH) + ROLE(SUBJECT)]|[PHRASE] } } The purpose of this rule is to extract human proper nouns ( TYPE(NPH) ) only if the names identified are the subjects of a sentence or clause ( + ROLE(SUBJECT) ). If this condition is verified, the PHRASE transformation option will ensure that every extracted value will be expanded to the phrase where the people's names are found as subjects. Consider the extraction output if the rule above is run against the following sample sentence: Assistant Commissioner Simon Byrne described detectives as \"constables in T-shirts and jeans\" and said he wanted to end the division between uniformed officers and detectives. The text contains one value matching the sample rule: Simon Byrne . This concept is recognized as a person's name and is the subject of the first clause of the sentence. Simon Byrne is also part of a broader noun phrase. The extraction would be the whole group in which the person's name was found: Assistant Commissioner Simon Byrne .","title":"PHRASE"},{"location":"extraction/transformation/section/","text":"SECTION SECTION is a transformation option that can be described as a completion feature of the matched value rather than a normalization. It adds elements surrounding the original matched data to the final extracted value. Its action is based on the concept of \"section\", which is a custom text subdivision that can be optionally defined for a project (for more information, please see the dedicated topic ). The SECTION option returns the whole section containing the value matched by an attribute. This option should only be used if one or more sections have been previously defined in the project. Also, at least one section must be used in the rule scope. The syntax of the SECTION option is the following: SCOPE scope_option IN SECTION (section_name) { IDENTIFY(templatename) { @field_1[Attribute]|[SECTION] } } This option is useful in situations where it's necessary to expand the extraction output revolving around a matched element. Consider the following example: SCOPE SECTION (PUBLICATIONDATE) { IDENTIFY(ARTICLE) { @Date[TYPE(DAT)]|[SECTION] } } The purpose of this rule is to extract dates ( TYPE(DAT) ) within a previously defined section called PUBLICATIONDATE ( SCOPE SECTION (PUBLICATIONDATE) ). If this condition is verified, the SECTION transformation option will ensure that every extracted value will be expanded to the section where the dates are found. Consider the extraction output if the rule above is run against the following sectioned text: TITLE section: Flu Widespread, Leading a Range of Winter's Ills AUTHOR section: By DONALD G. McNEIL Jr. and KATHARINE Q. SEELYE PUBLICATIONDATE section: Published: January 9, 2013 BODY section: It is not your imagination - more people you know are sick this winter, even people who have had flu shots. The country is in the grip of three emerging flu or flulike epidemics: an early start to the annual flu season with an unusually aggressive virus, a surge in a new type of norovirus, and the worst whooping cough outbreak in 60 years. And these are all developing amid the normal winter highs for the many viruses that cause symptoms on the \"colds and flu\" spectrum. Influenza is widespread, and causing local crises. On Wednesday, Boston's mayor declared a public health emergency as cases flooded hospital emergency rooms. Here the section PUBLICATIONDATE allows for two actions: As the scope of the rule, it restricts the extraction of dates to just the portion of text it delimits, in this case, the section called PUBLICATIONDATE . As the transformation option, the section itself is the final output of the extraction process. The rule condition is triggered by January 9, 2013 found within the section PUBLICATIONDATE , but the SECTION transformation extracts the whole section in the @Date field: Published: January 9, 2013 .","title":"SECTION"},{"location":"extraction/transformation/section/#section","text":"SECTION is a transformation option that can be described as a completion feature of the matched value rather than a normalization. It adds elements surrounding the original matched data to the final extracted value. Its action is based on the concept of \"section\", which is a custom text subdivision that can be optionally defined for a project (for more information, please see the dedicated topic ). The SECTION option returns the whole section containing the value matched by an attribute. This option should only be used if one or more sections have been previously defined in the project. Also, at least one section must be used in the rule scope. The syntax of the SECTION option is the following: SCOPE scope_option IN SECTION (section_name) { IDENTIFY(templatename) { @field_1[Attribute]|[SECTION] } } This option is useful in situations where it's necessary to expand the extraction output revolving around a matched element. Consider the following example: SCOPE SECTION (PUBLICATIONDATE) { IDENTIFY(ARTICLE) { @Date[TYPE(DAT)]|[SECTION] } } The purpose of this rule is to extract dates ( TYPE(DAT) ) within a previously defined section called PUBLICATIONDATE ( SCOPE SECTION (PUBLICATIONDATE) ). If this condition is verified, the SECTION transformation option will ensure that every extracted value will be expanded to the section where the dates are found. Consider the extraction output if the rule above is run against the following sectioned text: TITLE section: Flu Widespread, Leading a Range of Winter's Ills AUTHOR section: By DONALD G. McNEIL Jr. and KATHARINE Q. SEELYE PUBLICATIONDATE section: Published: January 9, 2013 BODY section: It is not your imagination - more people you know are sick this winter, even people who have had flu shots. The country is in the grip of three emerging flu or flulike epidemics: an early start to the annual flu season with an unusually aggressive virus, a surge in a new type of norovirus, and the worst whooping cough outbreak in 60 years. And these are all developing amid the normal winter highs for the many viruses that cause symptoms on the \"colds and flu\" spectrum. Influenza is widespread, and causing local crises. On Wednesday, Boston's mayor declared a public health emergency as cases flooded hospital emergency rooms. Here the section PUBLICATIONDATE allows for two actions: As the scope of the rule, it restricts the extraction of dates to just the portion of text it delimits, in this case, the section called PUBLICATIONDATE . As the transformation option, the section itself is the final output of the extraction process. The rule condition is triggered by January 9, 2013 found within the section PUBLICATIONDATE , but the SECTION transformation extracts the whole section in the @Date field: Published: January 9, 2013 .","title":"SECTION"},{"location":"extraction/transformation/sector/","text":"SECTOR SECTOR is an extraction transformation option that can be described as a \"completion feature\" applied to the matched value rather than a normalization feature. It adds the elements that surround the original matched data in the input sentence to the final extracted value. Its action is similar to the SEQUENCE option and it is based on the concepts of \"phrase\" and \"sequence\" (one of the classes of operators available in the Rules language). The SEQUENCE option returns all the elements included in a rule's sequence along with the value matched by the attribute enclosed in the extraction syntax. The SECTOR option not only returns the sequence, but it also expands the extraction output to the phrase containing the attributes that are part of the sequence. The syntax of the SECTOR option is the following: SCOPE scope_option { IDENTIFY(templatename) { Attribute1 sequence operator @field_1[Attribute 2]|[SECTOR] } } sequence operator refers to one of the positional or logical sequence operators available in the Rules language. The operators and the attributes other than the one enclosed in the extraction syntax can be positioned before or after the field-prefixed operand and as many operators and attributes may be used as needed. The SECTOR option must be used in a rule containing at least one sequence. Consider the following example: SCOPE SENTENCE { IDENTIFY(TEST) { TYPE(ADJ) >> @field_1[LEMMA(\"virus\")]|[SECTOR] } } The purpose of this rule is to extract the lemma virus , in singular or plural form, only if it appears in a text strictly preceded (double greater than sign , >> ) by an adjective ( TYPE(ADJ) ). If this condition is verified, the SECTOR transformation option will ensure that the extraction value will be expanded to include all elements pertaining to the sequence specified in the rule. Consider the extraction output if the rule above is run against the following sample text: Flu Widespread, Leading a Range of Winter's Ills By DONALD G. McNEIL Jr. and KATHARINE Q. SEELYE Published: January 9, 2013 It is not your imagination - more people you know are sick this winter, even people who have had flu shots. The country is in the grip of three emerging flu or flulike epidemics: an early start to the annual flu season with an unusually aggressive virus, a surge in a new type of norovirus, and the worst whooping cough outbreak in 60 years. And these are all developing amid the normal winter highs for the many viruses that cause symptoms on the \"colds and flu\" spectrum. Influenza is widespread, and causing local crises. On Wednesday, Boston's mayor declared a public health emergency as cases flooded hospital emergency rooms..[...] The text contains two combinations of values that the rule condition would match: many viruses and aggressive virus . Both strings are composed of an adjective preceding the lemma virus . Sequences are contained in these preposition phrases: with an unusually aggressive virus for the many viruses and the SECTOR transformation causes the extraction of the same phrases.","title":"SECTOR"},{"location":"extraction/transformation/sector/#sector","text":"SECTOR is an extraction transformation option that can be described as a \"completion feature\" applied to the matched value rather than a normalization feature. It adds the elements that surround the original matched data in the input sentence to the final extracted value. Its action is similar to the SEQUENCE option and it is based on the concepts of \"phrase\" and \"sequence\" (one of the classes of operators available in the Rules language). The SEQUENCE option returns all the elements included in a rule's sequence along with the value matched by the attribute enclosed in the extraction syntax. The SECTOR option not only returns the sequence, but it also expands the extraction output to the phrase containing the attributes that are part of the sequence. The syntax of the SECTOR option is the following: SCOPE scope_option { IDENTIFY(templatename) { Attribute1 sequence operator @field_1[Attribute 2]|[SECTOR] } } sequence operator refers to one of the positional or logical sequence operators available in the Rules language. The operators and the attributes other than the one enclosed in the extraction syntax can be positioned before or after the field-prefixed operand and as many operators and attributes may be used as needed. The SECTOR option must be used in a rule containing at least one sequence. Consider the following example: SCOPE SENTENCE { IDENTIFY(TEST) { TYPE(ADJ) >> @field_1[LEMMA(\"virus\")]|[SECTOR] } } The purpose of this rule is to extract the lemma virus , in singular or plural form, only if it appears in a text strictly preceded (double greater than sign , >> ) by an adjective ( TYPE(ADJ) ). If this condition is verified, the SECTOR transformation option will ensure that the extraction value will be expanded to include all elements pertaining to the sequence specified in the rule. Consider the extraction output if the rule above is run against the following sample text: Flu Widespread, Leading a Range of Winter's Ills By DONALD G. McNEIL Jr. and KATHARINE Q. SEELYE Published: January 9, 2013 It is not your imagination - more people you know are sick this winter, even people who have had flu shots. The country is in the grip of three emerging flu or flulike epidemics: an early start to the annual flu season with an unusually aggressive virus, a surge in a new type of norovirus, and the worst whooping cough outbreak in 60 years. And these are all developing amid the normal winter highs for the many viruses that cause symptoms on the \"colds and flu\" spectrum. Influenza is widespread, and causing local crises. On Wednesday, Boston's mayor declared a public health emergency as cases flooded hospital emergency rooms..[...] The text contains two combinations of values that the rule condition would match: many viruses and aggressive virus . Both strings are composed of an adjective preceding the lemma virus . Sequences are contained in these preposition phrases: with an unusually aggressive virus for the many viruses and the SECTOR transformation causes the extraction of the same phrases.","title":"SECTOR"},{"location":"extraction/transformation/segment/","text":"SEGMENT SEGMENT is an extraction transformation option that can be described as a completion feature of the matched value rather than a normalization. It adds elements surrounding the original matched data to the final extracted value. Its action is based on the concept of \"segment\", which is a custom text subdivision that can be optionally defined for a project (for more information, please see the dedicated topic ). The SEGMENT option returns the whole segment containing the value matched by an attribute. This option should only be used if one or more segments have been previously defined in the project. Also, at least one segment must be specified in the rule scope. The syntax of the SEGMENT option is the following: SCOPE scope_option IN SEGMENT (segment_name) { IDENTIFY(templatename) { @field_1[Attribute]|[SEGMENT] } } This option is useful in situations where it's necessary to expand the extraction output revolving around a matched element. Consider the following example: SCOPE SEGMENT (TITLE) { IDENTIFY(ARTICLE) { @Title[TYPE(NOU)]|[SEGMENT] } } The purpose of this rule is to extract nouns ( TYPE(NOU) ) within a previously defined segment called TITLE ( SCOPE SEGMENT (TITLE) ). If this condition is verified, the SEGMENT transformation option will ensure that every extracted value will be expanded to the segment where the nouns are found. Consider the extraction output if the rule above is run against the following sample text: Flu Widespread, Leading a Range of Winter's Ills By DONALD G. McNEIL Jr. and KATHARINE Q. SEELYE Published: January 9, 2013 It is not your imagination - more people you know are sick this winter, even people who have had flu shots. The country is in the grip of three emerging flu or flulike epidemics: an early start to the annual flu season with an unusually aggressive virus, a surge in a new type of norovirus, and the worst whooping cough outbreak in 60 years. And these are all developing amid the normal winter highs for the many viruses that cause symptoms on the \"colds and flu\" spectrum. Influenza is widespread, and causing local crises. On Wednesday, Boston's mayor declared a public health emergency as cases flooded hospital emergency rooms. Let's suppose the segment TITLE was detected based on a positional criterion (e.g., the first line of the text): Flu Widespread, Leading a Range of Winter's Ills This is the first condition that must be verified for the rule to be triggered. Here, the segment TITLE allows two actions: As the scope of the rule, it restricts the extraction of nouns to just the portion of text it delimits, in this case, the title. As the transformation option, the segment itself is the final output of the extraction process. The final result is the extraction of one instance of the segment text. This is because the rule was triggered by the four nouns contained in the segment ( Flu , Range , Winter , Ills ), but the engine was then able to recognize that each noun was found within the same segment and thus returns a single record instead of four identical records.","title":"SEGMENT"},{"location":"extraction/transformation/segment/#segment","text":"SEGMENT is an extraction transformation option that can be described as a completion feature of the matched value rather than a normalization. It adds elements surrounding the original matched data to the final extracted value. Its action is based on the concept of \"segment\", which is a custom text subdivision that can be optionally defined for a project (for more information, please see the dedicated topic ). The SEGMENT option returns the whole segment containing the value matched by an attribute. This option should only be used if one or more segments have been previously defined in the project. Also, at least one segment must be specified in the rule scope. The syntax of the SEGMENT option is the following: SCOPE scope_option IN SEGMENT (segment_name) { IDENTIFY(templatename) { @field_1[Attribute]|[SEGMENT] } } This option is useful in situations where it's necessary to expand the extraction output revolving around a matched element. Consider the following example: SCOPE SEGMENT (TITLE) { IDENTIFY(ARTICLE) { @Title[TYPE(NOU)]|[SEGMENT] } } The purpose of this rule is to extract nouns ( TYPE(NOU) ) within a previously defined segment called TITLE ( SCOPE SEGMENT (TITLE) ). If this condition is verified, the SEGMENT transformation option will ensure that every extracted value will be expanded to the segment where the nouns are found. Consider the extraction output if the rule above is run against the following sample text: Flu Widespread, Leading a Range of Winter's Ills By DONALD G. McNEIL Jr. and KATHARINE Q. SEELYE Published: January 9, 2013 It is not your imagination - more people you know are sick this winter, even people who have had flu shots. The country is in the grip of three emerging flu or flulike epidemics: an early start to the annual flu season with an unusually aggressive virus, a surge in a new type of norovirus, and the worst whooping cough outbreak in 60 years. And these are all developing amid the normal winter highs for the many viruses that cause symptoms on the \"colds and flu\" spectrum. Influenza is widespread, and causing local crises. On Wednesday, Boston's mayor declared a public health emergency as cases flooded hospital emergency rooms. Let's suppose the segment TITLE was detected based on a positional criterion (e.g., the first line of the text): Flu Widespread, Leading a Range of Winter's Ills This is the first condition that must be verified for the rule to be triggered. Here, the segment TITLE allows two actions: As the scope of the rule, it restricts the extraction of nouns to just the portion of text it delimits, in this case, the title. As the transformation option, the segment itself is the final output of the extraction process. The final result is the extraction of one instance of the segment text. This is because the rule was triggered by the four nouns contained in the segment ( Flu , Range , Winter , Ills ), but the engine was then able to recognize that each noun was found within the same segment and thus returns a single record instead of four identical records.","title":"SEGMENT"},{"location":"extraction/transformation/sentence/","text":"SENTENCE SENTENCE is an extraction transformation option that can be described as a completion feature of the matched value rather than a normalization. It adds elements surrounding the original matched data to the final extracted value. Its action is based on the concept of \"sentence\", a grammatical unit consisting of one or several words linked to each other by a syntactic relation in order to convey meaning. The recognition of sentences takes place during the disambiguation process. The SENTENCE option return the whole sentence containing the value matched by an attribute. The syntax of the SENTENCE option is the following: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute]|[SENTENCE] } } This option is useful in situations where it's necessary to expand the extraction output revolving around a matched element. Consider the following example: SCOPE `SENTENCE` { IDENTIFY(SUBJECT) { @Subject[TYPE(NPH) + ROLE(SUBJECT)]|[SENTENCE] } } This purpose of this rule is to extract people's names ( TYPE(NPH) ) only if the names identified are the subjects of a sentence or clause ( + ROLE(SUBJECT) ). If this condition is verified, the SENTENCE transformation option will ensure that every extracted value will be expanded to the sentence where the people's names are found as subjects. Consider the extraction output if the rule above is run against the following sample sentence: Assistant Commissioner Simon Byrne described detectives as \"constables in T-shirts and jeans\" and said he wanted to end the division between uniformed officers and detectives. The text contains one value matching the sample rule: Simon Byrne . This concept is recognized as a person's name and is the subject of the first clause of the sentence. The SENTENCE transformation extracts the whole sentence in which the person's name was found: Assistant Commissioner Simon Byrne described detectives as \"constables in T-shirts and jeans\" and said he wanted to end the division between uniformed officers and detectives .","title":"SENTENCE"},{"location":"extraction/transformation/sentence/#sentence","text":"SENTENCE is an extraction transformation option that can be described as a completion feature of the matched value rather than a normalization. It adds elements surrounding the original matched data to the final extracted value. Its action is based on the concept of \"sentence\", a grammatical unit consisting of one or several words linked to each other by a syntactic relation in order to convey meaning. The recognition of sentences takes place during the disambiguation process. The SENTENCE option return the whole sentence containing the value matched by an attribute. The syntax of the SENTENCE option is the following: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute]|[SENTENCE] } } This option is useful in situations where it's necessary to expand the extraction output revolving around a matched element. Consider the following example: SCOPE `SENTENCE` { IDENTIFY(SUBJECT) { @Subject[TYPE(NPH) + ROLE(SUBJECT)]|[SENTENCE] } } This purpose of this rule is to extract people's names ( TYPE(NPH) ) only if the names identified are the subjects of a sentence or clause ( + ROLE(SUBJECT) ). If this condition is verified, the SENTENCE transformation option will ensure that every extracted value will be expanded to the sentence where the people's names are found as subjects. Consider the extraction output if the rule above is run against the following sample sentence: Assistant Commissioner Simon Byrne described detectives as \"constables in T-shirts and jeans\" and said he wanted to end the division between uniformed officers and detectives. The text contains one value matching the sample rule: Simon Byrne . This concept is recognized as a person's name and is the subject of the first clause of the sentence. The SENTENCE transformation extracts the whole sentence in which the person's name was found: Assistant Commissioner Simon Byrne described detectives as \"constables in T-shirts and jeans\" and said he wanted to end the division between uniformed officers and detectives .","title":"SENTENCE"},{"location":"extraction/transformation/sequence/","text":"SEQUENCE SEQUENCE is an extraction transformation option that can be described as a completion feature of the matched value rather than a normalization. It adds elements surrounding the original matched data to the final extracted value. Its action is based on the concept of \"sequence\", one of the classes of operators available in the Rules language. Used in extraction rules, sequences define conditions that imply stricter constraints compared to basic extraction rules. The SEQUENCE option returns all the elements included in a rule's sequence along with the value matched by an attribute enclosed in the extraction syntax. The syntax of the SEQUENCE option is the following: SCOPE scope_option { IDENTIFY(templatename) { Attribute1 sequence operator @field_1[Attribute 2]|[SEQUENCE] } } sequence operator refers to one of the positional or logical sequence operators available. The operators and the attributes, other than the one enclosed in the extraction syntax, can be positioned before or after the extraction syntax and as many operators and attributes may be used as needed. By definition, the SEQUENCE option must be used in a rule containing at least one sequence. Basic extraction rules are commonly made up of an attribute (or combination of attributes). In some cases, a condition implies that other elements must be taken into account in addition to the attribute specified within the extraction syntax. A rule may in fact contain one or more constraining attributes, as well as the attribute enclosed in the extraction syntax, all of which are combined using one of the available positional or logical operators, for example: SCOPE SENTENCE { IDENTIFY(TEST) { KEYWORD(\"flu\", \"flulike\", \"flu-like\") >> @field_1[LEMMA(\"epidemic\")] } } The purpose of this rule is to extract only the lemma epidemic , in singular or plural form, if it appears in a text strictly preceded (double greater than sign, >> ) by one of the keywords flu , flulike or flu-like . The SEQUENCE option must be used when it is required for a sequence rule to extract not only the attribute enclosed in the extraction syntax but also any other element included in the sequence. Consider the same rule above with the addition of the SEQUENCE transformation: SCOPE SENTENCE { IDENTIFY(TEST) { KEYWORD(\"flu\", \"flulike\", \"flu-like\") >> @field_1[LEMMA(\"epidemic\")]|[SEQUENCE] } } Now, if the condition is verified, the SEQUENCE transformation option will ensure that the extraction value will be expanded to include all elements pertaining to the sequence specified in the rule. Consider the extraction output if the rule above is run against the following sample text: Flu Widespread, Leading a Range of Winter's Ills By DONALD G. McNEIL Jr. and KATHARINE Q. SEELYE Published: January 9, 2013 It is not your imagination - more people you know are sick this winter, even people who have had flu shots. The country is in the grip of three emerging flu or flulike epidemics: an early start to the annual flu season with an unusually aggressive virus, a surge in a new type of norovirus, and the worst whooping cough outbreak in 60 years. And these are all developing amid the normal winter highs for the many viruses that cause symptoms on the \"colds and flu\" spectrum. Influenza is widespread, and causing local crises. On Wednesday, Boston's mayor declared a public health emergency as cases flooded hospital emergency rooms. The text contains only one combination of values matching the sample rule - flulike epidemics - and the SEQUENCE operator causes the extraction of the entire sequence, and not just of the token (lemma epidemic ) matched by the field-prefixed operand.","title":"SEQUENCE"},{"location":"extraction/transformation/sequence/#sequence","text":"SEQUENCE is an extraction transformation option that can be described as a completion feature of the matched value rather than a normalization. It adds elements surrounding the original matched data to the final extracted value. Its action is based on the concept of \"sequence\", one of the classes of operators available in the Rules language. Used in extraction rules, sequences define conditions that imply stricter constraints compared to basic extraction rules. The SEQUENCE option returns all the elements included in a rule's sequence along with the value matched by an attribute enclosed in the extraction syntax. The syntax of the SEQUENCE option is the following: SCOPE scope_option { IDENTIFY(templatename) { Attribute1 sequence operator @field_1[Attribute 2]|[SEQUENCE] } } sequence operator refers to one of the positional or logical sequence operators available. The operators and the attributes, other than the one enclosed in the extraction syntax, can be positioned before or after the extraction syntax and as many operators and attributes may be used as needed. By definition, the SEQUENCE option must be used in a rule containing at least one sequence. Basic extraction rules are commonly made up of an attribute (or combination of attributes). In some cases, a condition implies that other elements must be taken into account in addition to the attribute specified within the extraction syntax. A rule may in fact contain one or more constraining attributes, as well as the attribute enclosed in the extraction syntax, all of which are combined using one of the available positional or logical operators, for example: SCOPE SENTENCE { IDENTIFY(TEST) { KEYWORD(\"flu\", \"flulike\", \"flu-like\") >> @field_1[LEMMA(\"epidemic\")] } } The purpose of this rule is to extract only the lemma epidemic , in singular or plural form, if it appears in a text strictly preceded (double greater than sign, >> ) by one of the keywords flu , flulike or flu-like . The SEQUENCE option must be used when it is required for a sequence rule to extract not only the attribute enclosed in the extraction syntax but also any other element included in the sequence. Consider the same rule above with the addition of the SEQUENCE transformation: SCOPE SENTENCE { IDENTIFY(TEST) { KEYWORD(\"flu\", \"flulike\", \"flu-like\") >> @field_1[LEMMA(\"epidemic\")]|[SEQUENCE] } } Now, if the condition is verified, the SEQUENCE transformation option will ensure that the extraction value will be expanded to include all elements pertaining to the sequence specified in the rule. Consider the extraction output if the rule above is run against the following sample text: Flu Widespread, Leading a Range of Winter's Ills By DONALD G. McNEIL Jr. and KATHARINE Q. SEELYE Published: January 9, 2013 It is not your imagination - more people you know are sick this winter, even people who have had flu shots. The country is in the grip of three emerging flu or flulike epidemics: an early start to the annual flu season with an unusually aggressive virus, a surge in a new type of norovirus, and the worst whooping cough outbreak in 60 years. And these are all developing amid the normal winter highs for the many viruses that cause symptoms on the \"colds and flu\" spectrum. Influenza is widespread, and causing local crises. On Wednesday, Boston's mayor declared a public health emergency as cases flooded hospital emergency rooms. The text contains only one combination of values matching the sample rule - flulike epidemics - and the SEQUENCE operator causes the extraction of the entire sequence, and not just of the token (lemma epidemic ) matched by the field-prefixed operand.","title":"SEQUENCE"},{"location":"extraction/transformation/smartentry/","text":"SMARTENTRY SMARTENTRY is a transformation option whose action is similar to the ENTRY option. In fact, SMARTENTRY can be described as an evolution of the ENTRY option. The syntax of the SMARTENTRY option is the following: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute]|[SMARTENTRY] } } The purpose of the SMARTENTRY transformation is to transform what is matched by the attribute into its most significant base form. But whereas the ENTRY option is useful only for attributes capable of recognizing concepts found in the Knowledge Graph, SMARTENTRY can be used with any attribute without the need to evaluate if a concept is found in the Knowledge Graph or not. The SMARTENTRY option automatically recognizes if a concept is contained in the Knowledge Graph or not, and will return a syncon \"main lemma\", if the concept is known, or will return its guessed base form, if the concept is unknown. SMARTENTRY transformation is useful in many cases, especially for named entities processing. To understand the behavior of this option, consider the following use case: Named entities are classified by the semantic disambiguator as proper nouns. The Knowledge Graph contains a number of proper noun syncons, but not all existing proper nouns in the language are part of the semantic network. For example, the Knowledge Graph may contain a syncon for the concept of Lehman Brothers , but it does not contain a syncon for Transocean Ltd. . The disambiguator, however, is able to recognize that Transocean Ltd . is the proper noun of a company even though this concept is not present in the Knowledge Graph. Transocean Ltd. , in fact, is recognized as a virtual child of syncon 37887 ( limited liability company ). Before the introduction of the SMARTENTRY option, if a rule was required to extract proper names of companies from texts and to normalize the output value with a constant form when the company name is contained in the Knowledge Graph, different rules using different transformation options should have been used based on the company being \"known\" or \"unknown\" to the Knowledge Graph. The transformation option used to extract known syncons is ENTRY which finds a concept in a text, identifies it in all its possible forms and variations (contained in the Knowledge Graph) and returns a constant form corresponding to the \"main lemma\" (previously set in the Knowledge Graph). Consider the following example: SCOPE SENTENCE { IDENTIFY(COMPANY) { @COMPANY_NAME[ANCESTOR(37475) + TYPE(NPR) - SYNCON(UNKNOWN)]|[ENTRY] } } The purpose of this rule is to extract a chain of proper noun concepts ( + TYPE(NPR) ) starting from syncon 37475 ( company ), only if the identified concepts are not \"unknown\" to the Knowledge Graph ( - SYNCON (UNKNOWN) ); in other words, extract proper nouns of companies if they are contained in the Knowledge Graph. If this condition is verified, then the ENTRY transformation option will ensure that every form that a company name can take will be transformed into the syncon main lemma. This allows for a concept to have one consistent extraction value even though the concept appears in several different forms in a text. Consider for example the extraction output if the modified sample rule is run against the following sentence: The equities index is 20 percent above its level on Sept. 15, 2008, the first trading day after Lehman Brothers Holdings Inc. filed the world's biggest bankruptcy and prompted a 46 percent drop through March 9, 2009. Lehman Brothers is having a great year. The bank, which almost destroyed the global economy four years ago this week, recently emerged from bankruptcy, resolved a third of its debts and executed the largest U.S. real estate deal of the year The text contains two values matching the sample rule: Lehman Brothers Holdings Inc. and Lehman Brothers , which are both analyzed as companies. The disambiguator also recognizes these two names as the same company associated to syncon 317862; in the Knowledge Graph, this syncon contains five different forms referring to the same concept. The extraction panel shows that the value extracted is its main lemma, Lehman Brothers , while the text record shows the two instances found in the text: Lehman Brothers Holdings Inc. and Lehman Brothers . This means that the extracted values have been transformed and normalized into the main lemma thanks to the ENTRY option. The transformation option used to extract unknown syncons is TEXT which extracts the exact value identified in the text. Consider the following example: SCOPE SENTENCE { IDENTIFY(COMPANY) { @COMPANY_NAME[ANCESTOR(37475) + TYPE(NPR) + SYNCON(UNKNOWN)]|[TEXT] } } If this condition is verified, the TEXT transformation option will ensure that every extracted value will be kept in its original form as it appears in the text. Consider the extraction output if the sample rule is run against the following sentence: Transocean Ltd. announces Definitive Agreements to Sell 38 Shallow Water drilling Rigs to Shelf Drilling. The text contains the value Transocean Ltd. , which is recognized as a company, but it is not present in the Knowledge Graph as a syncon. It is however recognized as a company with the virtual supernomen limited liability company . The extracted value was taken as it appears in the text thanks to the TEXT transformation, therefore the output is Transocean Ltd. . If an ENTRY option was used for both known and unknown concepts in the Knowledge Graph, unknown company names would be transformed into the main lemma associated to the token virtual parent. In the the example, Transocean Ltd. would be transformed into the main lemma associated to the token's virtual parent, therefore limited liability company . Instead, if only one rule with the TEXT option was used, the option of normalizing concepts found in the Knowledge Graph would be lost. The SMARTENTRY option provides the possibility to write just one rule for concepts which are both known and unknown to the Knowledge Graph and to obtain clean and correct results which were previously obtained using two separate rules. SMARTENTRY recognizes when an entity is contained or not in the Knowledge Graph and consequently normalizes the values.","title":"SMARTENTRY"},{"location":"extraction/transformation/smartentry/#smartentry","text":"SMARTENTRY is a transformation option whose action is similar to the ENTRY option. In fact, SMARTENTRY can be described as an evolution of the ENTRY option. The syntax of the SMARTENTRY option is the following: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute]|[SMARTENTRY] } } The purpose of the SMARTENTRY transformation is to transform what is matched by the attribute into its most significant base form. But whereas the ENTRY option is useful only for attributes capable of recognizing concepts found in the Knowledge Graph, SMARTENTRY can be used with any attribute without the need to evaluate if a concept is found in the Knowledge Graph or not. The SMARTENTRY option automatically recognizes if a concept is contained in the Knowledge Graph or not, and will return a syncon \"main lemma\", if the concept is known, or will return its guessed base form, if the concept is unknown. SMARTENTRY transformation is useful in many cases, especially for named entities processing. To understand the behavior of this option, consider the following use case: Named entities are classified by the semantic disambiguator as proper nouns. The Knowledge Graph contains a number of proper noun syncons, but not all existing proper nouns in the language are part of the semantic network. For example, the Knowledge Graph may contain a syncon for the concept of Lehman Brothers , but it does not contain a syncon for Transocean Ltd. . The disambiguator, however, is able to recognize that Transocean Ltd . is the proper noun of a company even though this concept is not present in the Knowledge Graph. Transocean Ltd. , in fact, is recognized as a virtual child of syncon 37887 ( limited liability company ). Before the introduction of the SMARTENTRY option, if a rule was required to extract proper names of companies from texts and to normalize the output value with a constant form when the company name is contained in the Knowledge Graph, different rules using different transformation options should have been used based on the company being \"known\" or \"unknown\" to the Knowledge Graph. The transformation option used to extract known syncons is ENTRY which finds a concept in a text, identifies it in all its possible forms and variations (contained in the Knowledge Graph) and returns a constant form corresponding to the \"main lemma\" (previously set in the Knowledge Graph). Consider the following example: SCOPE SENTENCE { IDENTIFY(COMPANY) { @COMPANY_NAME[ANCESTOR(37475) + TYPE(NPR) - SYNCON(UNKNOWN)]|[ENTRY] } } The purpose of this rule is to extract a chain of proper noun concepts ( + TYPE(NPR) ) starting from syncon 37475 ( company ), only if the identified concepts are not \"unknown\" to the Knowledge Graph ( - SYNCON (UNKNOWN) ); in other words, extract proper nouns of companies if they are contained in the Knowledge Graph. If this condition is verified, then the ENTRY transformation option will ensure that every form that a company name can take will be transformed into the syncon main lemma. This allows for a concept to have one consistent extraction value even though the concept appears in several different forms in a text. Consider for example the extraction output if the modified sample rule is run against the following sentence: The equities index is 20 percent above its level on Sept. 15, 2008, the first trading day after Lehman Brothers Holdings Inc. filed the world's biggest bankruptcy and prompted a 46 percent drop through March 9, 2009. Lehman Brothers is having a great year. The bank, which almost destroyed the global economy four years ago this week, recently emerged from bankruptcy, resolved a third of its debts and executed the largest U.S. real estate deal of the year The text contains two values matching the sample rule: Lehman Brothers Holdings Inc. and Lehman Brothers , which are both analyzed as companies. The disambiguator also recognizes these two names as the same company associated to syncon 317862; in the Knowledge Graph, this syncon contains five different forms referring to the same concept. The extraction panel shows that the value extracted is its main lemma, Lehman Brothers , while the text record shows the two instances found in the text: Lehman Brothers Holdings Inc. and Lehman Brothers . This means that the extracted values have been transformed and normalized into the main lemma thanks to the ENTRY option. The transformation option used to extract unknown syncons is TEXT which extracts the exact value identified in the text. Consider the following example: SCOPE SENTENCE { IDENTIFY(COMPANY) { @COMPANY_NAME[ANCESTOR(37475) + TYPE(NPR) + SYNCON(UNKNOWN)]|[TEXT] } } If this condition is verified, the TEXT transformation option will ensure that every extracted value will be kept in its original form as it appears in the text. Consider the extraction output if the sample rule is run against the following sentence: Transocean Ltd. announces Definitive Agreements to Sell 38 Shallow Water drilling Rigs to Shelf Drilling. The text contains the value Transocean Ltd. , which is recognized as a company, but it is not present in the Knowledge Graph as a syncon. It is however recognized as a company with the virtual supernomen limited liability company . The extracted value was taken as it appears in the text thanks to the TEXT transformation, therefore the output is Transocean Ltd. . If an ENTRY option was used for both known and unknown concepts in the Knowledge Graph, unknown company names would be transformed into the main lemma associated to the token virtual parent. In the the example, Transocean Ltd. would be transformed into the main lemma associated to the token's virtual parent, therefore limited liability company . Instead, if only one rule with the TEXT option was used, the option of normalizing concepts found in the Knowledge Graph would be lost. The SMARTENTRY option provides the possibility to write just one rule for concepts which are both known and unknown to the Knowledge Graph and to obtain clean and correct results which were previously obtained using two separate rules. SMARTENTRY recognizes when an entity is contained or not in the Knowledge Graph and consequently normalizes the values.","title":"SMARTENTRY"},{"location":"extraction/transformation/syncon/","text":"SYNCON SYNCON is the extraction transformation option that transforms what is matched by the attribute into its syncon ID. This option should be used to normalize only extraction values known to the Knowledge Graph. The syntax of the option SYNCON is the following: SCOPE scope-option { IDENTIFY(templatename) { @field_1[Attribute]|[SYNCON] } } To use this option, the attribute chosen should be among those capable of recognizing elements found within the Knowledge Graph ( see SYNCON , ANCESTOR , LIST excluding the UNKNOWN elements: syncon and ancestor). Consider the following example: SCOPE SENTENCE { IDENTIFY(PLACE) { @Place[TYPE(NPR) - SYNCON(UNKNOWN)]|[SYNCON] } } The purpose of this rule is to extract proper noun concepts (TYPE(NPR)) only if they are not \"unknown\" to the Knowledge Graph ( - SYNCON(UNKNOWN) ); in other words, extract proper nouns if they are contained in the Knowledge Graph. If this condition is verified, the SYNCON transformation option will ensure that every form that a proper noun can take in the text will be recognized and transformed into its syncon ID. Consider the extraction output if the rule above is run against the following sample sentence: Emergency teams battled more than 130 fires across New South Wales, the country's most populous state, on Tuesday, with at least 40 burning out of control. Fires also continued to burn in Tasmania, after blazes at the weekend destroyed more than 20,000 hectares of land and dozens of properties. The text contains two values matching the sample rule: New South Wales and Tasmania , which are both recognized as proper nouns. The value extracted is the syncon ID of each entity.","title":"SYNCON"},{"location":"extraction/transformation/syncon/#syncon","text":"SYNCON is the extraction transformation option that transforms what is matched by the attribute into its syncon ID. This option should be used to normalize only extraction values known to the Knowledge Graph. The syntax of the option SYNCON is the following: SCOPE scope-option { IDENTIFY(templatename) { @field_1[Attribute]|[SYNCON] } } To use this option, the attribute chosen should be among those capable of recognizing elements found within the Knowledge Graph ( see SYNCON , ANCESTOR , LIST excluding the UNKNOWN elements: syncon and ancestor). Consider the following example: SCOPE SENTENCE { IDENTIFY(PLACE) { @Place[TYPE(NPR) - SYNCON(UNKNOWN)]|[SYNCON] } } The purpose of this rule is to extract proper noun concepts (TYPE(NPR)) only if they are not \"unknown\" to the Knowledge Graph ( - SYNCON(UNKNOWN) ); in other words, extract proper nouns if they are contained in the Knowledge Graph. If this condition is verified, the SYNCON transformation option will ensure that every form that a proper noun can take in the text will be recognized and transformed into its syncon ID. Consider the extraction output if the rule above is run against the following sample sentence: Emergency teams battled more than 130 fires across New South Wales, the country's most populous state, on Tuesday, with at least 40 burning out of control. Fires also continued to burn in Tasmania, after blazes at the weekend destroyed more than 20,000 hectares of land and dozens of properties. The text contains two values matching the sample rule: New South Wales and Tasmania , which are both recognized as proper nouns. The value extracted is the syncon ID of each entity.","title":"SYNCON"},{"location":"extraction/transformation/text/","text":"TEXT TEXT transformation fills extraction fields with the original text matched by the rule operands. This is the default transformation for the KEYWORD and the PATTERN attributes, while the default for all the other attributes is BASE . The syntax is: @field_name[operand]|[TEXT] This option is useful in cases where strict correspondence to the text is necessary. For example, consider a project with a customized Knowledge Graph containing the concepts of many companies linked to the parent syncon with ID 37475 ( company ). For some companies, several variants of their names have been defined. To extract the exact citation of a company from a text, we can use this rule: SCOPE SENTENCE { IDENTIFY(COMPANY) { @COMPANY_NAME[ANCESTOR(37475) + TYPE(NPR) - SYNCON(UNKNOWN)]|[TEXT] } } The operand associated with the @COMPANY_NAME field matches concepts that derive from the syncon 37475 excluding the common names of types of companies ( + TYPE (NPR) ) and the companies heuristically recognized as such by the disambiguator and that therefore have the syncon 37475 as their virtual supernomen. Now consider the following sample text: The equities index is 20 percent above its level on Sept. 15, 2008, the first trading day after Lehman Brothers Holdings Inc. filed the world's biggest bankruptcy and prompted a 46 percent drop through March 9, 2009. Lehman Brothers is having a great year. The bank, which almost destroyed the global economy four years ago this week, recently emerged from bankruptcy, resolved a third of its debts and executed the largest U.S. real estate deal of the year. The operand above matches Lehman Brothers Holdings Inc. and Lehman Brothers as they are lemmas of a syncon that has Lehman Brothers as its base form. If no transformation or the BASE transformation were specified, the base form would be extracted. With the TEXT transformation the literal value matched by the operand is extracted instead.","title":"TEXT"},{"location":"extraction/transformation/text/#text","text":"TEXT transformation fills extraction fields with the original text matched by the rule operands. This is the default transformation for the KEYWORD and the PATTERN attributes, while the default for all the other attributes is BASE . The syntax is: @field_name[operand]|[TEXT] This option is useful in cases where strict correspondence to the text is necessary. For example, consider a project with a customized Knowledge Graph containing the concepts of many companies linked to the parent syncon with ID 37475 ( company ). For some companies, several variants of their names have been defined. To extract the exact citation of a company from a text, we can use this rule: SCOPE SENTENCE { IDENTIFY(COMPANY) { @COMPANY_NAME[ANCESTOR(37475) + TYPE(NPR) - SYNCON(UNKNOWN)]|[TEXT] } } The operand associated with the @COMPANY_NAME field matches concepts that derive from the syncon 37475 excluding the common names of types of companies ( + TYPE (NPR) ) and the companies heuristically recognized as such by the disambiguator and that therefore have the syncon 37475 as their virtual supernomen. Now consider the following sample text: The equities index is 20 percent above its level on Sept. 15, 2008, the first trading day after Lehman Brothers Holdings Inc. filed the world's biggest bankruptcy and prompted a 46 percent drop through March 9, 2009. Lehman Brothers is having a great year. The bank, which almost destroyed the global economy four years ago this week, recently emerged from bankruptcy, resolved a third of its debts and executed the largest U.S. real estate deal of the year. The operand above matches Lehman Brothers Holdings Inc. and Lehman Brothers as they are lemmas of a syncon that has Lehman Brothers as its base form. If no transformation or the BASE transformation were specified, the base form would be extracted. With the TEXT transformation the literal value matched by the operand is extracted instead.","title":"TEXT"},{"location":"extraction/transformation/token/","text":"TOKEN TOKEN is an extraction transformation option which can be described as the equivalent of the TEXT option on the word level of text analysis. The TOKEN option, however, acts on the atom level and maintains what is matched by the attribute in its original form. The syntax of the TOKEN option is the following: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute]|[TOKEN] } } To fully understand the TOKEN option, it is useful to compare it to the TEXT option. Consider the following example: SCOPE SENTENCE { IDENTIFY(TEST) { @FIELD1[KEYWORD(\"emergency\")]|[TEXT] } } This rule aims is to extract the keyword emergency and to keep its value in its original form. Consider the extraction output if the above rule is run against the following sample sentence: Emergency teams battled more than 130 fires across New South Wales. The text contains only one value that matches the sample rule; the base form on the atom level for this value is emergency . Due to the TEXT transformation, the field is set to the actual instance found in the text at the atom level, which is the keyword Emergency . Compare this result with the TOKEN transformation run against the same sample sentence: SCOPE SENTENCE { IDENTIFY(TEST) { @FIELD1[KEYWORD(\"emergency\")]|[TOKEN] } } Here, the keyword emergency is part of a noun lemma emergency teams at the word level, which is composed of two atoms: emergency and team . The base form at the word level for this value is emergency team . The effect of the TOKEN transformation is the extraction of the text form on the word level: Emergency teams .","title":"TOKEN"},{"location":"extraction/transformation/token/#token","text":"TOKEN is an extraction transformation option which can be described as the equivalent of the TEXT option on the word level of text analysis. The TOKEN option, however, acts on the atom level and maintains what is matched by the attribute in its original form. The syntax of the TOKEN option is the following: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute]|[TOKEN] } } To fully understand the TOKEN option, it is useful to compare it to the TEXT option. Consider the following example: SCOPE SENTENCE { IDENTIFY(TEST) { @FIELD1[KEYWORD(\"emergency\")]|[TEXT] } } This rule aims is to extract the keyword emergency and to keep its value in its original form. Consider the extraction output if the above rule is run against the following sample sentence: Emergency teams battled more than 130 fires across New South Wales. The text contains only one value that matches the sample rule; the base form on the atom level for this value is emergency . Due to the TEXT transformation, the field is set to the actual instance found in the text at the atom level, which is the keyword Emergency . Compare this result with the TOKEN transformation run against the same sample sentence: SCOPE SENTENCE { IDENTIFY(TEST) { @FIELD1[KEYWORD(\"emergency\")]|[TOKEN] } } Here, the keyword emergency is part of a noun lemma emergency teams at the word level, which is composed of two atoms: emergency and team . The base form at the word level for this value is emergency team . The effect of the TOKEN transformation is the extraction of the text form on the word level: Emergency teams .","title":"TOKEN"},{"location":"extraction/transformation-and-composition/","text":"Combined use of transformation and composition In extraction rule writing, transformation and composition options can be used in combination to reach a desired normalized output. The syntax to combine transformation and composition is the following: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute]|[TRANSFORMATION COMPOSITION] } } where TRANSFORMATION refers to one of the transformation options listed below: BASE ATOM TEXT TOKEN ENTRY SMARTENTRY and COMPOSITION refers to a pound sign ( # ) followed by a whole number. For example: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute1][TRANSFORMATION#1] sequence operator @field_1[Attribute2]|[TRANSFORMATION#2] } } Transformation and composition options must be typed in uppercase and in the order indicated above, with no spaces between the two. The two syntaxes are written between brackets and positioned at the end of a full and correct extraction rule; a vertical bar (called pipe, | ) separates the rule and the transformation/composition part. For composition, numbering within each rule should start with the number one (1), without any gap in the sequence. Each so-called sequence \"chunk\" ( #1 , #2 , etc.) shares the same destination field and is part of a sequence. The number associated to each chunk represents the position that the extracted value will assume in the final output. Composition is the only syntax that allows for the repetition of the same extraction field within the same rule. When using this option, even in combination with transformation, it is not necessary to use the extraction syntax on every element of the sequence; in other words, the user can choose which elements will be part of the extraction and which elements will act only as constraints in the rule. Consider the following example: SCOPE SENTENCE { IDENTIFY(CYBERCRIME) { @Event[ANCESTOR(100220)]|[ENTRY#2]// 100220: website, web-site, <1:3> @Event[LEMMA(\"hack\") + TYPE(VER)]|[BASE#1] } IDENTIFY(CYBERCRIME) { @Event[LEMMA(\"hack\") + TYPE(VER)]|[BASE#1] <1:4> @Event[ANCESTOR(100220)]|[ENTRY#2]// 100220: website, web-site } } These two rules are meant to extract strings indicating the hacking of websites. The two rules look for the verb to hack (lemma hack , + TYPE (VER) ) either preceded (first rule) or followed (second rule, <1:3> and <1:4> , see positional sequences )) by the chain of concepts starting from the syncon website ( ANCESTOR(100220) ). This means that both active ( hack a site ) and passive voices ( a site has been hacked ) mentioning the same concepts will be recognized. Please note that the attributes enclosed in the extraction syntax extract different values for the same field @Event and that the composition syntax is applied to each. The #1 and #2 declarations determine the following behavior: every element extracted for the attribute marked with #1 will be the first element to compose the final output; every element extracted for the attribute marked with #2 will be the second element to compose the final output. Transformation options are applied which return the base form for the verb and the entry for the Ancestor. Consider the extraction output if the rule above is run against the following sample text: Culture Ministry website hacked Published: 16 Jan 2013 at 11.23 Online news: Local News The Culture Ministry's Thai website has been hacked by a group demanding the ministry return the banned political soap opera \"Nua Mek 2\" to the public. The hackers, who called themselves \"The Bad Piggies Team\", posted a \"Nua Mek 2\" banner and two messages: \"Return Nua Mek 2 to Us\" and \"HACKED by THE BAD PIGGIES TEAM\" showing intermittently with a cartoon avatar of a green pig head. They were posted on Wednesday morning. The website was shut down shortly afterwards. The ministry's website in English was not hacked. (story continues below) The Bad Piggies Team hacked the Culture Ministry's website on Jan 16, 2013. The text contains three strings matching the rules above: Culture Ministry website hacked website has been hacked hacked the Culture Ministry's website The first two (passive voices) are matched by the first rule, while the last instance (active voice) is matched by the second. Transformation option BASE transforms hacked into the lemma's base form hack . The option ENTRY transforms every instance of the concept website into the main lemma's base form. This also occurs for the string Culture Ministry website , which is disambiguated as a proper noun, unknown to the Knowledge Graph, but virtually recognized as a child of the concept website (its virtual supernomen). Since virtual concepts do not have an entry form, then the entry of their virtual supernomen is chosen; in this case it is the lemma website . Finally, the composition syntax allows the lemma hack to always be the first element in the final output (chunk #1 ) and the ancestor website to always be the second (chunk #2 ). This occurs independently from their actual position in the text. This entire process results in a final normalized output of one single record, hack website , for three originally different instances of the same concept.","title":"Combined use of transformation and composition"},{"location":"extraction/transformation-and-composition/#combined-use-of-transformation-and-composition","text":"In extraction rule writing, transformation and composition options can be used in combination to reach a desired normalized output. The syntax to combine transformation and composition is the following: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute]|[TRANSFORMATION COMPOSITION] } } where TRANSFORMATION refers to one of the transformation options listed below: BASE ATOM TEXT TOKEN ENTRY SMARTENTRY and COMPOSITION refers to a pound sign ( # ) followed by a whole number. For example: SCOPE scope_option { IDENTIFY(templatename) { @field_1[Attribute1][TRANSFORMATION#1] sequence operator @field_1[Attribute2]|[TRANSFORMATION#2] } } Transformation and composition options must be typed in uppercase and in the order indicated above, with no spaces between the two. The two syntaxes are written between brackets and positioned at the end of a full and correct extraction rule; a vertical bar (called pipe, | ) separates the rule and the transformation/composition part. For composition, numbering within each rule should start with the number one (1), without any gap in the sequence. Each so-called sequence \"chunk\" ( #1 , #2 , etc.) shares the same destination field and is part of a sequence. The number associated to each chunk represents the position that the extracted value will assume in the final output. Composition is the only syntax that allows for the repetition of the same extraction field within the same rule. When using this option, even in combination with transformation, it is not necessary to use the extraction syntax on every element of the sequence; in other words, the user can choose which elements will be part of the extraction and which elements will act only as constraints in the rule. Consider the following example: SCOPE SENTENCE { IDENTIFY(CYBERCRIME) { @Event[ANCESTOR(100220)]|[ENTRY#2]// 100220: website, web-site, <1:3> @Event[LEMMA(\"hack\") + TYPE(VER)]|[BASE#1] } IDENTIFY(CYBERCRIME) { @Event[LEMMA(\"hack\") + TYPE(VER)]|[BASE#1] <1:4> @Event[ANCESTOR(100220)]|[ENTRY#2]// 100220: website, web-site } } These two rules are meant to extract strings indicating the hacking of websites. The two rules look for the verb to hack (lemma hack , + TYPE (VER) ) either preceded (first rule) or followed (second rule, <1:3> and <1:4> , see positional sequences )) by the chain of concepts starting from the syncon website ( ANCESTOR(100220) ). This means that both active ( hack a site ) and passive voices ( a site has been hacked ) mentioning the same concepts will be recognized. Please note that the attributes enclosed in the extraction syntax extract different values for the same field @Event and that the composition syntax is applied to each. The #1 and #2 declarations determine the following behavior: every element extracted for the attribute marked with #1 will be the first element to compose the final output; every element extracted for the attribute marked with #2 will be the second element to compose the final output. Transformation options are applied which return the base form for the verb and the entry for the Ancestor. Consider the extraction output if the rule above is run against the following sample text: Culture Ministry website hacked Published: 16 Jan 2013 at 11.23 Online news: Local News The Culture Ministry's Thai website has been hacked by a group demanding the ministry return the banned political soap opera \"Nua Mek 2\" to the public. The hackers, who called themselves \"The Bad Piggies Team\", posted a \"Nua Mek 2\" banner and two messages: \"Return Nua Mek 2 to Us\" and \"HACKED by THE BAD PIGGIES TEAM\" showing intermittently with a cartoon avatar of a green pig head. They were posted on Wednesday morning. The website was shut down shortly afterwards. The ministry's website in English was not hacked. (story continues below) The Bad Piggies Team hacked the Culture Ministry's website on Jan 16, 2013. The text contains three strings matching the rules above: Culture Ministry website hacked website has been hacked hacked the Culture Ministry's website The first two (passive voices) are matched by the first rule, while the last instance (active voice) is matched by the second. Transformation option BASE transforms hacked into the lemma's base form hack . The option ENTRY transforms every instance of the concept website into the main lemma's base form. This also occurs for the string Culture Ministry website , which is disambiguated as a proper noun, unknown to the Knowledge Graph, but virtually recognized as a child of the concept website (its virtual supernomen). Since virtual concepts do not have an entry form, then the entry of their virtual supernomen is chosen; in this case it is the lemma website . Finally, the composition syntax allows the lemma hack to always be the first element in the final output (chunk #1 ) and the ancestor website to always be the second (chunk #2 ). This occurs independently from their actual position in the text. This entire process results in a final normalized output of one single record, hack website , for three originally different instances of the same concept.","title":"Combined use of transformation and composition"},{"location":"extraction/value-normalization/","text":"Value normalization overview In an extraction project, the values returned at the end of the process can be kept in their original form or they can be modified using different approaches. The aim of such a process is to remove or reduce redundancies in the data, to present it in a standardized format and to improve completeness and correctness. Normalization options can be described as belonging to two main approaches: Transformation Composition These two methods can be used separately or in combination. They are the optional part in the association of an extracted token and a field, and they control which data is transferred into the field.","title":"Value normalization overview"},{"location":"extraction/value-normalization/#value-normalization-overview","text":"In an extraction project, the values returned at the end of the process can be kept in their original form or they can be modified using different approaches. The aim of such a process is to remove or reduce redundancies in the data, to present it in a standardized format and to improve completeness and correctness. Normalization options can be described as belonging to two main approaches: Transformation Composition These two methods can be used separately or in combination. They are the optional part in the association of an extracted token and a field, and they control which data is transferred into the field.","title":"Value normalization overview"},{"location":"extraction/values-transformation/","text":"Extraction values transformation overview In addition to extraction values normalization, the Rules language provides other advanced values transformation methods and procedures. These methods perform complex manipulations of entities and concepts extracted by texts. In particular, they take advantage of the possibility to extract a single value using specific extraction rules and compose, deconstruct, transform and normalize it to fit into several fields of a predefined template. These methods are described in their own dedicated articles: Transformation of structured entities Transform TOKEN","title":"Extraction values transformation overview"},{"location":"extraction/values-transformation/#extraction-values-transformation-overview","text":"In addition to extraction values normalization, the Rules language provides other advanced values transformation methods and procedures. These methods perform complex manipulations of entities and concepts extracted by texts. In particular, they take advantage of the possibility to extract a single value using specific extraction rules and compose, deconstruct, transform and normalize it to fit into several fields of a predefined template. These methods are described in their own dedicated articles: Transformation of structured entities Transform TOKEN","title":"Extraction values transformation overview"},{"location":"extraction/values-transformation/structured-entities/","text":"Transformation of structured entities Overview In extraction rule writing, a transformer is an extraction syntax that allows the user to define the way in which some types of structured entities can be divided into the different elements which compose them. These components are then transferred into different fields of a template. Such structured entities are, in fact, identified by the disambiguator during the disambiguation process as predictable combinations of letters, numbers and also symbols whose overall semantic value is stronger than the semantic value of the components considered individually. For example, the disambiguator is able to recognize that a string composed by a number and the proper name of a street is an address. A date can be recognized as such because it consists of a particular and predictable combination of numbers (for example, 02 , 2009 , '70 ), words (for example, Jul , July ) and other non-alphanumeric characters (for example, / , - ). This part of the disambiguation process generates types of entities such as: NPH (person's name), ADR (address), DAT (date), etc. These types of entities can be either treated as a whole (a complete address, a complete date etc.) or can be broken down into their constituent elements. The complete transformer syntax is the following: TEMPLATE(templatename) { @field_1, @field_2, ... @field_n } DEFINE transformername = TRANSFORM(ID) IN TEMPLATE(templatename) { FIELD (ID) IN field_1 FIELD (ID) IN field_2 ... FIELD (ID) IN field_n } SCOPE scope_option { IDENTIFY(templatename) { @@transformername[Attribute] } } TEMPLATE , DEFINE , TRANSFORM , IN , FIELD , SCOPE , IDENTIFY are language keywords and must be written in uppercase as shown above. ID refers to the unique (unambiguous) identifier that belongs to each and every syncon found in the Knowledge Graph. It is always a whole number made up of one or several digits. templatename , field and transformername refer, respectively, to the name to be assigned to the template, the extraction fields and the transformer. scope_option refers to the part of a rule syntax that allows you to specify the portion of text that the rule must act upon. Attribute can be one of the attributes available in the Rules language. Warning The syntax will accept any of the available attributes, but to make the transformer as effective as possible, it is important to carefully choose the attribute and the related value so that all of the target entities are matched in the input text. This syntax can be split into three main parts: The template definition. The transformer in the strict sense. The extraction rule(s). The TEMPLATE contains the information found in the entity constituent elements. These elements will be organized into template fields. Located in the transformer block, there are the DEFINE line or header and the transformer body. The DEFINE line associates the metadata records with the template. Here, the transformer is given a name ( transformername ) and the action to be performed is defined. The code above can be described as follows: transpose the extracted information associated to the syncon ID to the previously defined template. The transformer body associates each entity record with the template fields. Each field will contain the extracted information associated to the syncon ID corresponding to each entity constituent element. The final elements which complete the syntax are the rules (or rule) extracting the values that must be transformed. The rule identifies the previously created template, then uses one or more attributes to extract data from a text. Unlike standard extraction rules, this particular rule does not extract a value directly to a field ( @fieldname ), but uses the name of the transformer with a double at sign ( @@ ). The transformer is then responsible for splitting the data and associating it to the different template fields. Consider the management of people's names, one of the structured type of entities identified by the disambiguator. By default, people's names are analyzed as virtual children of syncon 784526 ( person ). The names can usually be split into first name and last name and also implicitly contain information about the gender of the person, since it is often possible to recognize if a first name is masculine or feminine. The following paragraph details how to configure the transformer in order to reach the desired result. TEMPLATE(PERSON) { @first_name, @family_name, @gender } The first step is to define a template, in this case PERSON : it contains the information which breaks down people's names into its constituent elements. These elements are organized in the template fields called first_name , family_name and gender . Next, the DEFINE line establishes where the following actions are performed: DEFINE Person = TRANSFORM (78452) IN TEMPLATE (PERSON) The transformer is given a name ( Person ) and the action to be performed is expressed. The action can be described as follows: transpose the information contained in the extracted values associated to syncon 78452 (which is the supernomen , the virtual parent, for any person's name) to the template called PERSON . Finally, the transformer body associates each entity record with the template fields: { FIELD(29307) IN first_name FIELD(29305) IN family_name FIELD(24254) IN gender } The template field first_name contains the extracted information associated with syncon 29307 (the concept of first name in the Knowledge Graph); the field family_name contains the information associated with syncon 29305 (the concept of last name in the Knowledge Graph) and the field gender contains the information associated with syncon 24254 (the concept of gender in the Knowledge Graph) The last element that completes the syntax is the creation of one or more rules extracting the people's names that must be transformed: SCOPE SENTENCE { IDENTIFY(PERSON) { @@Person[TYPE(NPH)] } } Now consider the following example of template, transformer and extraction rule: TEMPLATE(PERSON) { @first_name, @family_name, @gender } DEFINE Person = TRANSFORM (78452) IN TEMPLATE (PERSON) { FIELD(29307) IN first_name FIELD(29305) IN family_name FIELD(24254) IN gender } SCOPE SENTENCE { IDENTIFY(PERSON) { @@Person[TYPE(NPH)] } } along with the sample sentence below: To stand your ground in the face of relentless criticism from a double Nobel prize-winning scientist takes a lot of guts. For engineer and materials scientist Dan Shechtman, however, years of self-belief in the face of the eminent Linus Pauling's criticisms led him to the ultimate accolade: his own Nobel prize. Shechtman was the sole winner of the Nobel prize for chemistry in 2011, for his discovery of seemingly impossible crystal structures in metal alloys. The extraction rule identifies the people's names Linus Pauling and Dan Schechtman and the transformer breaks them down into its components, transposing their content to the fields in the predefined template. The final result is: Template: PERSON @first_name @family_name @gender Linus Pauling M Template: PERSON @first_name @family_name @gender Dan Shechtman M Use of constants Variations to the standard transformer syntax may be applied if necessary. For example, the fields content may be manipulated by adding a constant value instead of a varying value extracted from a text. The syntax is the following: TEMPLATE(templatename) { @field_1, @field_2, ... @field_n } DEFINE transformername = TRANSFORM(ID) IN TEMPLATE(templatename) { constant + FIELD (ID) IN field_1 FIELD (ID) + constant IN field_2 ... FIELD (ID) IN field_n } SCOPE scope_option { IDENTIFY(templatename) { @@transformername[attribute] } } where constant refers to an invariable value that will always be added to the extraction output. The constant must be typed in between quotation marks ( \"...\" ). For example, if it is required to add name: before a person's first name, the sample transformer defined above should be modified as follows: DEFINE Person = TRANSFORM (78452) IN TEMPLATE (PERSON) { \"name: \" + FIELD(29307) IN first_name FIELD(29305) IN family_name FIELD(24254) IN gender } Note Any punctuation mark or spaces which need to appear in the final output must be defined in the constant value. If the rule above was applied to the sample text, the results would be: Template: PERSON @first_name @family_name @gender name: Linus Pauling M Template: PERSON @first_name @family_name @gender name: Dan Shechtman M The plus sign ( + ) can also be used to combine different tokens within the same field. Consider this syntax: TEMPLATE(templatename) { @field_1, @field_2, ... @field_n } DEFINE transformername = TRANSFORM(ID) IN TEMPLATE(templatename) { constant + FIELD (ID) + FIELD (ID) IN field_1 FIELD (ID) IN field_2 ... FIELD (ID) IN field_n } SCOPE scope_option { IDENTIFY(templatename) { @@transformername[Attribute] } } This use of the plus sign can be adopted for the example rule like so: TEMPLATE(PERSON) { @full_name, @gender } DEFINE Person = TRANSFORM (78452) IN TEMPLATE (PERSON) { \"full name: \" + FIELD(29305) + \", \" + FIELD(29307) IN full_name FIELD(24254) IN gender } SCOPE SENTENCE { IDENTIFY(PERSON) { @@Person[TYPE(NPH)] } } In this case, the template has two fields instead of three. The transformer puts the constant full name: before the first token to be extracted, which is the family name (syncon 29305). After the family name, a second constant value will be inserted (a comma) followed by a space. The final token to close the content of the full_name field is the first name (syncon 29307). If the rule above is applied to the sample text, the final output will be a normalized extraction record containing the full name of the person (ordered by family name) along with information about the gender. Template: PERSON @full_name @gender full name: Pauling, Linus M Template: PERSON @full_name @gender full name: Shechtman, Dan M In other cases, it is necessary to deal with the fact that the transformer may not be able to identify all the data stored in a template. In other words, in the event that no token is matched by the transformer, the defined fields can not be processed. In these cases, those fields can be filled with a default string (for example ***) which serves as a placeholder for the blank field. The syntax is: TEMPLATE(templatename) { @field_1, @field_2, ... @field_n } DEFINE transformername = TRANSFORM(ID) IN TEMPLATE(templatename) { FIELD (ID:constant) IN field_1 FIELD (ID) IN field_2 ... FIELD (ID) IN field_n } SCOPE scope_option { IDENTIFY(templatename) { @@transformername[attribute] } } When written between quotation marks, constant becomes the default value when the FIELD(ID) cannot be found. Consider the following example: DEFINE Person = TRANSFORM (78452) IN TEMPLATE (PERSON) { FIELD(29307:\"---\") IN first_name FIELD(29305:\"*-*-\") IN family_name FIELD(24254) IN gender } The first name field will be set to ---, if no first name is found in the text, and field family_name will be set to *-*-, if no value is recognized to be a family name. The output of this configuration will return either an extracted value, when it is present, or the predefined string, if no value is available. Consider a variation of the previous sample text in which the first name Dan is omitted: To stand your ground in the face of relentless criticism from a double Nobel prize-winning scientist takes a lot of guts. For engineer and materials scientist Shechtman, however, years of self-belief in the face of the eminent Linus Pauling's criticisms led him to the ultimate accolade: his own Nobel prize. Shechtman was the sole winner of the Nobel prize for chemistry in 2011, for his discovery of seemingly impossible crystal structures in metal alloys. With the new transformer, the extraction becomes: Template: PERSON @first_name @family_name @gender Linus Pauling M Template: PERSON @first_name @family_name @gender --- Shechtman M Shechtman has been recognized as person's name even though the first name is missing, the entity has been extracted and processed by the transformer to fill in the fields of the PERSON template, and the field @first_name has been set with the default string.","title":"Transformation of structured entities"},{"location":"extraction/values-transformation/structured-entities/#transformation-of-structured-entities","text":"","title":"Transformation of structured entities"},{"location":"extraction/values-transformation/structured-entities/#overview","text":"In extraction rule writing, a transformer is an extraction syntax that allows the user to define the way in which some types of structured entities can be divided into the different elements which compose them. These components are then transferred into different fields of a template. Such structured entities are, in fact, identified by the disambiguator during the disambiguation process as predictable combinations of letters, numbers and also symbols whose overall semantic value is stronger than the semantic value of the components considered individually. For example, the disambiguator is able to recognize that a string composed by a number and the proper name of a street is an address. A date can be recognized as such because it consists of a particular and predictable combination of numbers (for example, 02 , 2009 , '70 ), words (for example, Jul , July ) and other non-alphanumeric characters (for example, / , - ). This part of the disambiguation process generates types of entities such as: NPH (person's name), ADR (address), DAT (date), etc. These types of entities can be either treated as a whole (a complete address, a complete date etc.) or can be broken down into their constituent elements. The complete transformer syntax is the following: TEMPLATE(templatename) { @field_1, @field_2, ... @field_n } DEFINE transformername = TRANSFORM(ID) IN TEMPLATE(templatename) { FIELD (ID) IN field_1 FIELD (ID) IN field_2 ... FIELD (ID) IN field_n } SCOPE scope_option { IDENTIFY(templatename) { @@transformername[Attribute] } } TEMPLATE , DEFINE , TRANSFORM , IN , FIELD , SCOPE , IDENTIFY are language keywords and must be written in uppercase as shown above. ID refers to the unique (unambiguous) identifier that belongs to each and every syncon found in the Knowledge Graph. It is always a whole number made up of one or several digits. templatename , field and transformername refer, respectively, to the name to be assigned to the template, the extraction fields and the transformer. scope_option refers to the part of a rule syntax that allows you to specify the portion of text that the rule must act upon. Attribute can be one of the attributes available in the Rules language. Warning The syntax will accept any of the available attributes, but to make the transformer as effective as possible, it is important to carefully choose the attribute and the related value so that all of the target entities are matched in the input text. This syntax can be split into three main parts: The template definition. The transformer in the strict sense. The extraction rule(s). The TEMPLATE contains the information found in the entity constituent elements. These elements will be organized into template fields. Located in the transformer block, there are the DEFINE line or header and the transformer body. The DEFINE line associates the metadata records with the template. Here, the transformer is given a name ( transformername ) and the action to be performed is defined. The code above can be described as follows: transpose the extracted information associated to the syncon ID to the previously defined template. The transformer body associates each entity record with the template fields. Each field will contain the extracted information associated to the syncon ID corresponding to each entity constituent element. The final elements which complete the syntax are the rules (or rule) extracting the values that must be transformed. The rule identifies the previously created template, then uses one or more attributes to extract data from a text. Unlike standard extraction rules, this particular rule does not extract a value directly to a field ( @fieldname ), but uses the name of the transformer with a double at sign ( @@ ). The transformer is then responsible for splitting the data and associating it to the different template fields. Consider the management of people's names, one of the structured type of entities identified by the disambiguator. By default, people's names are analyzed as virtual children of syncon 784526 ( person ). The names can usually be split into first name and last name and also implicitly contain information about the gender of the person, since it is often possible to recognize if a first name is masculine or feminine. The following paragraph details how to configure the transformer in order to reach the desired result. TEMPLATE(PERSON) { @first_name, @family_name, @gender } The first step is to define a template, in this case PERSON : it contains the information which breaks down people's names into its constituent elements. These elements are organized in the template fields called first_name , family_name and gender . Next, the DEFINE line establishes where the following actions are performed: DEFINE Person = TRANSFORM (78452) IN TEMPLATE (PERSON) The transformer is given a name ( Person ) and the action to be performed is expressed. The action can be described as follows: transpose the information contained in the extracted values associated to syncon 78452 (which is the supernomen , the virtual parent, for any person's name) to the template called PERSON . Finally, the transformer body associates each entity record with the template fields: { FIELD(29307) IN first_name FIELD(29305) IN family_name FIELD(24254) IN gender } The template field first_name contains the extracted information associated with syncon 29307 (the concept of first name in the Knowledge Graph); the field family_name contains the information associated with syncon 29305 (the concept of last name in the Knowledge Graph) and the field gender contains the information associated with syncon 24254 (the concept of gender in the Knowledge Graph) The last element that completes the syntax is the creation of one or more rules extracting the people's names that must be transformed: SCOPE SENTENCE { IDENTIFY(PERSON) { @@Person[TYPE(NPH)] } } Now consider the following example of template, transformer and extraction rule: TEMPLATE(PERSON) { @first_name, @family_name, @gender } DEFINE Person = TRANSFORM (78452) IN TEMPLATE (PERSON) { FIELD(29307) IN first_name FIELD(29305) IN family_name FIELD(24254) IN gender } SCOPE SENTENCE { IDENTIFY(PERSON) { @@Person[TYPE(NPH)] } } along with the sample sentence below: To stand your ground in the face of relentless criticism from a double Nobel prize-winning scientist takes a lot of guts. For engineer and materials scientist Dan Shechtman, however, years of self-belief in the face of the eminent Linus Pauling's criticisms led him to the ultimate accolade: his own Nobel prize. Shechtman was the sole winner of the Nobel prize for chemistry in 2011, for his discovery of seemingly impossible crystal structures in metal alloys. The extraction rule identifies the people's names Linus Pauling and Dan Schechtman and the transformer breaks them down into its components, transposing their content to the fields in the predefined template. The final result is: Template: PERSON @first_name @family_name @gender Linus Pauling M Template: PERSON @first_name @family_name @gender Dan Shechtman M","title":"Overview"},{"location":"extraction/values-transformation/structured-entities/#use-of-constants","text":"Variations to the standard transformer syntax may be applied if necessary. For example, the fields content may be manipulated by adding a constant value instead of a varying value extracted from a text. The syntax is the following: TEMPLATE(templatename) { @field_1, @field_2, ... @field_n } DEFINE transformername = TRANSFORM(ID) IN TEMPLATE(templatename) { constant + FIELD (ID) IN field_1 FIELD (ID) + constant IN field_2 ... FIELD (ID) IN field_n } SCOPE scope_option { IDENTIFY(templatename) { @@transformername[attribute] } } where constant refers to an invariable value that will always be added to the extraction output. The constant must be typed in between quotation marks ( \"...\" ). For example, if it is required to add name: before a person's first name, the sample transformer defined above should be modified as follows: DEFINE Person = TRANSFORM (78452) IN TEMPLATE (PERSON) { \"name: \" + FIELD(29307) IN first_name FIELD(29305) IN family_name FIELD(24254) IN gender } Note Any punctuation mark or spaces which need to appear in the final output must be defined in the constant value. If the rule above was applied to the sample text, the results would be: Template: PERSON @first_name @family_name @gender name: Linus Pauling M Template: PERSON @first_name @family_name @gender name: Dan Shechtman M The plus sign ( + ) can also be used to combine different tokens within the same field. Consider this syntax: TEMPLATE(templatename) { @field_1, @field_2, ... @field_n } DEFINE transformername = TRANSFORM(ID) IN TEMPLATE(templatename) { constant + FIELD (ID) + FIELD (ID) IN field_1 FIELD (ID) IN field_2 ... FIELD (ID) IN field_n } SCOPE scope_option { IDENTIFY(templatename) { @@transformername[Attribute] } } This use of the plus sign can be adopted for the example rule like so: TEMPLATE(PERSON) { @full_name, @gender } DEFINE Person = TRANSFORM (78452) IN TEMPLATE (PERSON) { \"full name: \" + FIELD(29305) + \", \" + FIELD(29307) IN full_name FIELD(24254) IN gender } SCOPE SENTENCE { IDENTIFY(PERSON) { @@Person[TYPE(NPH)] } } In this case, the template has two fields instead of three. The transformer puts the constant full name: before the first token to be extracted, which is the family name (syncon 29305). After the family name, a second constant value will be inserted (a comma) followed by a space. The final token to close the content of the full_name field is the first name (syncon 29307). If the rule above is applied to the sample text, the final output will be a normalized extraction record containing the full name of the person (ordered by family name) along with information about the gender. Template: PERSON @full_name @gender full name: Pauling, Linus M Template: PERSON @full_name @gender full name: Shechtman, Dan M In other cases, it is necessary to deal with the fact that the transformer may not be able to identify all the data stored in a template. In other words, in the event that no token is matched by the transformer, the defined fields can not be processed. In these cases, those fields can be filled with a default string (for example ***) which serves as a placeholder for the blank field. The syntax is: TEMPLATE(templatename) { @field_1, @field_2, ... @field_n } DEFINE transformername = TRANSFORM(ID) IN TEMPLATE(templatename) { FIELD (ID:constant) IN field_1 FIELD (ID) IN field_2 ... FIELD (ID) IN field_n } SCOPE scope_option { IDENTIFY(templatename) { @@transformername[attribute] } } When written between quotation marks, constant becomes the default value when the FIELD(ID) cannot be found. Consider the following example: DEFINE Person = TRANSFORM (78452) IN TEMPLATE (PERSON) { FIELD(29307:\"---\") IN first_name FIELD(29305:\"*-*-\") IN family_name FIELD(24254) IN gender } The first name field will be set to ---, if no first name is found in the text, and field family_name will be set to *-*-, if no value is recognized to be a family name. The output of this configuration will return either an extracted value, when it is present, or the predefined string, if no value is available. Consider a variation of the previous sample text in which the first name Dan is omitted: To stand your ground in the face of relentless criticism from a double Nobel prize-winning scientist takes a lot of guts. For engineer and materials scientist Shechtman, however, years of self-belief in the face of the eminent Linus Pauling's criticisms led him to the ultimate accolade: his own Nobel prize. Shechtman was the sole winner of the Nobel prize for chemistry in 2011, for his discovery of seemingly impossible crystal structures in metal alloys. With the new transformer, the extraction becomes: Template: PERSON @first_name @family_name @gender Linus Pauling M Template: PERSON @first_name @family_name @gender --- Shechtman M Shechtman has been recognized as person's name even though the first name is missing, the entity has been extracted and processed by the transformer to fill in the fields of the PERSON template, and the field @first_name has been set with the default string.","title":"Use of constants"},{"location":"extraction/values-transformation/token/","text":"TRANSFORM TOKEN In extraction rule writing, TRANSFORM TOKEN is a syntax comparable to the simple TRANSFORM syntax. Both syntaxes associate extraction data with a template, but whereas TRANSFORM only performs a transformation of structured entities, TRANSFORM TOKEN can transform any kind of entity or concept. In particular, with the TRANSFORM TOKEN syntax, a value can be assigned to several template fields by applying different transformation methods (options) to an extracted token. The complete TRANSFORM TOKEN syntax is the following: TEMPLATE(templatename) { field 1, field 2, ... field n } DEFINE transformername = TRANSFORM(TOKEN) IN TEMPLATE(templatename) { constant IN field_1 TRANSFORMATION (transformation option) IN field_2 } SCOPE scope_option { IDENTIFY(templatename) { @@transformername[Attribute] } } TEMPLATE , DEFINE , TRANSFORM , TOKEN , IN , TRANSFORMATION , SCOPE and IDENTIFY are language keywords which must be written in uppercase as shown above. constant refers to an invariable value that will always be added to the extraction output. transformation option refers to any of the transformation options available. templatename , field and transformername refer respectively to the name that the user assigns to the template, the extraction fields and the transformer. scope_option refers to the part of a rule syntax which specifies the portion of the text to be considered when evaluating the rule. Attribute refers to any of the attributes available in the Rules language. This syntax can be divided into three main parts: The template definition. The transformer in the strict sense. The extraction rule(s). The template contains the names of the fields to be transformed. The transformation block contains the DEFINE line and the transformation option. The DEFINE line associates the metadata with the template, provides the transformer with a name ( transformername ), and expresses the action to be performed. The line can be described as follows: transpose any extracted information ( TOKEN ) to the previously defined template. The transformer body associates each entity record with the template fields. In detail, each field can contain constant values and/or normalized extraction tokens. The last elements to complete the syntax are the rules (or rule) that extract the values that must be transformed. The rule identifies the previously created template and uses one or more attributes to extract data from text. Unlike standard extraction rules, this particular rule does not extract a value directly to a field ( @fieldname[] ). It uses the name of the transformer with the double at sign ( @@ ), which is then responsible for sorting the data and associating it to the different template fields. For example, consider a project in which proper names of infrastructures (airports, harbors, highways and train stations) must be mined from the text. A standard extraction project would require at least one field for each type of infrastructure, where the field names indicate the type while the entity proper names are the values extracted from the text. However with the TRANSFORM TOKEN syntax, you can: Define just one field for any type of infrastructure and one field for any entity proper name (efficiency). Define just the macro-types of entities to be extracted, thus having the option to easily reduce or expand the project's scope.(scalability). Transform the proper name (token) in a variety of ways and combine different types of token transformation (flexibility). The following paragraphs describe how the transformer should be configured to reach the desired result. The first step to configure a TRANSFORM TOKEN option is to create the template that will receive the extracted values. In this case, it is called INFRASTRUCTURES and it contains two fields: The type of infrastructure extracted ( @type ). The proper name of the extracted infrastructure ( @name ). The template is: TEMPLATE(INFRASTRUCTURES) { @type, @name } The next step is to define two transformers, each will manage a different type of infrastructure. The first transformer is called Airport and its purpose is to transfer the information extracted by the Airport transformer into the INFRASTRUCTURES template. The body of the transformer states that the template field @type must contain the word airport as a constant value every time the transformer Airport extracts a value from a text, while the template field @name will contain the proper name of the infrastructure found in the text and transformed using the transformation option SMARTENTRY . The second transformer performs the same actions and functions just described, but this time with the transformer called Railway . DEFINE Airport = TRANSFORM(TOKEN) IN TEMPLATE(INFRASTRUCTURES) { \"airport\" IN type TRANSFORMATION (SMARTENTRY) IN name } DEFINE Railway = TRANSFORM(TOKEN) IN TEMPLATE(INFRASTRUCTURES) { \"railway\" IN type TRANSFORMATION (SMARTENTRY) IN name } Finally, two extraction rules are defined, one for each transformer. The first extraction rule extracts all the proper names ( + TYPE(NPR) ) of airports ( ANCESTOR(12830) ), while the second extracts any recognized proper name of railway ( ANCESTOR(19647) ). SCOPE SENTENCE { IDENTIFY(INFRASTRUCTURES) { @@Airport[ANCESTOR(12830) + TYPE(NPR)] // 12830: airport, airdrome, aerodrome } IDENTIFY(INFRASTRUCTURES) { @@Railway[ANCESTOR(19647) + TYPE(NPR)] // 19647: railway, rail road, railway line, railway network, } } Using the above rules to extract infrastructure information from the following text: The Canadian Pacific Railway (CPR), formerly also known as CP Rail (reporting mark CP) between 1968 and 1996, is a historic Canadian Class I railroad founded in 1881 and now operated by Canadian Pacific Railway Limited (a subsidiary of Canadian Pacific Limited), which began operations as legal owner in a corporate restructuring in 2001. Historically, Canadian Pacific operated several non-railway businesses. In 1971, these businesses were split off into the separate company Canadian Pacific Limited, and in 2001, that company was further split into five companies. CP no longer provides any of these services. Canadian Pacific Airlines, also called CP Air, operated from 1942 to 1987 and was the main competitor of Canadian government-owned Air Canada. Based at Vancouver International Airport, it served Canadian and international routes until it was purchased by Pacific Western Airlines which merged PWA and CP Air to create Canadian Airlines. The extraction output will contain two records: Template : INFRASTRUCTURES @name @type CPR railway Template : INFRASTRUCTURES @name @type Vancouver International Airport airport The first record contains information related to a railway line, whereas the second contains information related to an airport. Both entities have been mapped on the same template but each one is clearly identified by the field @type . The first entity also shows the result of the SMARTENTRY transformation which finds in the text all the forms in which the concept is expressed ( CPR , Canadian Pacific Railway ) and normalizes them to a single constant form ( CPR ). You can also combine more than one transformation option and/or add a constant value to the extracted value. Consider for instance a new requirement: extract Canadian Pacific Railway with the SMARTENTRY transformation and also include its syncon ID in the extraction record. The previously defined code would be modified as follows: DEFINE Railway = TRANSFORM(TOKEN) IN TEMPLATE(INFRASTRUCTURES) { \"railway\" IN type TRANSFORMATION (SMARTENTRY) + \" \" + TRANSFORMATION(SYNCON) IN name } Following the transformation SMARTENTRY , a space has been added using the plus sign to add it to the final output. The space was added in order to separate the first value from the newly added syncon ID (see the SYNCON transformation ). According to the new transformer definition, the values extracted in the field called @name are now modified as such: Template : INFRASTRUCTURES @name @type CPR 99667 railway To better identify the two elements that now make up this value, another constant value can be added to act as a qualifier . DEFINE Railway = TRANSFORM(TOKEN) IN TEMPLATE(INFRASTRUCTURES) { \"railway\" IN type TRANSFORMATION (SMARTENTRY) +\" - Syncon ID: \" + TRANSFORMATION(SYNCON) IN name } The new command line states that the output should contain the entry for the extracted value, a space, a dash, another space, the constant - Syncon ID: and finally the ID associated to the extracted syncon. According to the new transformer definition, the value extracted in the field @name will now be: Template : INFRASTRUCTURES @name @type CPR - Syncon ID: 99667 railway","title":"TRANSFORM TOKEN"},{"location":"extraction/values-transformation/token/#transform-token","text":"In extraction rule writing, TRANSFORM TOKEN is a syntax comparable to the simple TRANSFORM syntax. Both syntaxes associate extraction data with a template, but whereas TRANSFORM only performs a transformation of structured entities, TRANSFORM TOKEN can transform any kind of entity or concept. In particular, with the TRANSFORM TOKEN syntax, a value can be assigned to several template fields by applying different transformation methods (options) to an extracted token. The complete TRANSFORM TOKEN syntax is the following: TEMPLATE(templatename) { field 1, field 2, ... field n } DEFINE transformername = TRANSFORM(TOKEN) IN TEMPLATE(templatename) { constant IN field_1 TRANSFORMATION (transformation option) IN field_2 } SCOPE scope_option { IDENTIFY(templatename) { @@transformername[Attribute] } } TEMPLATE , DEFINE , TRANSFORM , TOKEN , IN , TRANSFORMATION , SCOPE and IDENTIFY are language keywords which must be written in uppercase as shown above. constant refers to an invariable value that will always be added to the extraction output. transformation option refers to any of the transformation options available. templatename , field and transformername refer respectively to the name that the user assigns to the template, the extraction fields and the transformer. scope_option refers to the part of a rule syntax which specifies the portion of the text to be considered when evaluating the rule. Attribute refers to any of the attributes available in the Rules language. This syntax can be divided into three main parts: The template definition. The transformer in the strict sense. The extraction rule(s). The template contains the names of the fields to be transformed. The transformation block contains the DEFINE line and the transformation option. The DEFINE line associates the metadata with the template, provides the transformer with a name ( transformername ), and expresses the action to be performed. The line can be described as follows: transpose any extracted information ( TOKEN ) to the previously defined template. The transformer body associates each entity record with the template fields. In detail, each field can contain constant values and/or normalized extraction tokens. The last elements to complete the syntax are the rules (or rule) that extract the values that must be transformed. The rule identifies the previously created template and uses one or more attributes to extract data from text. Unlike standard extraction rules, this particular rule does not extract a value directly to a field ( @fieldname[] ). It uses the name of the transformer with the double at sign ( @@ ), which is then responsible for sorting the data and associating it to the different template fields. For example, consider a project in which proper names of infrastructures (airports, harbors, highways and train stations) must be mined from the text. A standard extraction project would require at least one field for each type of infrastructure, where the field names indicate the type while the entity proper names are the values extracted from the text. However with the TRANSFORM TOKEN syntax, you can: Define just one field for any type of infrastructure and one field for any entity proper name (efficiency). Define just the macro-types of entities to be extracted, thus having the option to easily reduce or expand the project's scope.(scalability). Transform the proper name (token) in a variety of ways and combine different types of token transformation (flexibility). The following paragraphs describe how the transformer should be configured to reach the desired result. The first step to configure a TRANSFORM TOKEN option is to create the template that will receive the extracted values. In this case, it is called INFRASTRUCTURES and it contains two fields: The type of infrastructure extracted ( @type ). The proper name of the extracted infrastructure ( @name ). The template is: TEMPLATE(INFRASTRUCTURES) { @type, @name } The next step is to define two transformers, each will manage a different type of infrastructure. The first transformer is called Airport and its purpose is to transfer the information extracted by the Airport transformer into the INFRASTRUCTURES template. The body of the transformer states that the template field @type must contain the word airport as a constant value every time the transformer Airport extracts a value from a text, while the template field @name will contain the proper name of the infrastructure found in the text and transformed using the transformation option SMARTENTRY . The second transformer performs the same actions and functions just described, but this time with the transformer called Railway . DEFINE Airport = TRANSFORM(TOKEN) IN TEMPLATE(INFRASTRUCTURES) { \"airport\" IN type TRANSFORMATION (SMARTENTRY) IN name } DEFINE Railway = TRANSFORM(TOKEN) IN TEMPLATE(INFRASTRUCTURES) { \"railway\" IN type TRANSFORMATION (SMARTENTRY) IN name } Finally, two extraction rules are defined, one for each transformer. The first extraction rule extracts all the proper names ( + TYPE(NPR) ) of airports ( ANCESTOR(12830) ), while the second extracts any recognized proper name of railway ( ANCESTOR(19647) ). SCOPE SENTENCE { IDENTIFY(INFRASTRUCTURES) { @@Airport[ANCESTOR(12830) + TYPE(NPR)] // 12830: airport, airdrome, aerodrome } IDENTIFY(INFRASTRUCTURES) { @@Railway[ANCESTOR(19647) + TYPE(NPR)] // 19647: railway, rail road, railway line, railway network, } } Using the above rules to extract infrastructure information from the following text: The Canadian Pacific Railway (CPR), formerly also known as CP Rail (reporting mark CP) between 1968 and 1996, is a historic Canadian Class I railroad founded in 1881 and now operated by Canadian Pacific Railway Limited (a subsidiary of Canadian Pacific Limited), which began operations as legal owner in a corporate restructuring in 2001. Historically, Canadian Pacific operated several non-railway businesses. In 1971, these businesses were split off into the separate company Canadian Pacific Limited, and in 2001, that company was further split into five companies. CP no longer provides any of these services. Canadian Pacific Airlines, also called CP Air, operated from 1942 to 1987 and was the main competitor of Canadian government-owned Air Canada. Based at Vancouver International Airport, it served Canadian and international routes until it was purchased by Pacific Western Airlines which merged PWA and CP Air to create Canadian Airlines. The extraction output will contain two records: Template : INFRASTRUCTURES @name @type CPR railway Template : INFRASTRUCTURES @name @type Vancouver International Airport airport The first record contains information related to a railway line, whereas the second contains information related to an airport. Both entities have been mapped on the same template but each one is clearly identified by the field @type . The first entity also shows the result of the SMARTENTRY transformation which finds in the text all the forms in which the concept is expressed ( CPR , Canadian Pacific Railway ) and normalizes them to a single constant form ( CPR ). You can also combine more than one transformation option and/or add a constant value to the extracted value. Consider for instance a new requirement: extract Canadian Pacific Railway with the SMARTENTRY transformation and also include its syncon ID in the extraction record. The previously defined code would be modified as follows: DEFINE Railway = TRANSFORM(TOKEN) IN TEMPLATE(INFRASTRUCTURES) { \"railway\" IN type TRANSFORMATION (SMARTENTRY) + \" \" + TRANSFORMATION(SYNCON) IN name } Following the transformation SMARTENTRY , a space has been added using the plus sign to add it to the final output. The space was added in order to separate the first value from the newly added syncon ID (see the SYNCON transformation ). According to the new transformer definition, the values extracted in the field called @name are now modified as such: Template : INFRASTRUCTURES @name @type CPR 99667 railway To better identify the two elements that now make up this value, another constant value can be added to act as a qualifier . DEFINE Railway = TRANSFORM(TOKEN) IN TEMPLATE(INFRASTRUCTURES) { \"railway\" IN type TRANSFORMATION (SMARTENTRY) +\" - Syncon ID: \" + TRANSFORMATION(SYNCON) IN name } The new command line states that the output should contain the entry for the extracted value, a space, a dash, another space, the constant - Syncon ID: and finally the ID associated to the extracted syncon. According to the new transformer definition, the value extracted in the field @name will now be: Template : INFRASTRUCTURES @name @type CPR - Syncon ID: 99667 railway","title":"TRANSFORM TOKEN"},{"location":"hybrid-rules/","text":"Hybrid rules If you need to extract information and categorize based on the same conditions, hybrid rules avoid you to create a double set of very similar rules, saving time and avoiding errors. A hybrid rule, when triggered, gives point to taxonomy domains and at the same time fills extraction template fields. To define a hybrid rule, use the plus sign ( + ) to write a header that combines the header of a categorization rule with that of an extraction rule , then define the condition as for an extraction rule: SCOPE scope { DOMAIN(domain_name) + IDENTIFY(template_name) { extraction rule style condition, specifying the fields to fill } } For example, considering the text: This chair was designed using Artificial Intelligence Celebrated French designer Phillipe Starck has come up with an algorithm for a chair. AFP RELAXNEWS Phillipe Starck has teamed up with a longstanding collaborator, Italian furniture company Kartell, to create the first chair to be designed using artificial intelligence, which is aptly named \"A.I\". Plastic furniture specialist Kartell embarked on a new direction when it adopted a development process using AI in the company of a long-time collaborator, French designer Philippe Starck. The man behind the ultra-famous \"Ghost\" chair selected a series of data and an algorithm to create a chair in line with criteria for strength and solidity as well as the designer's aesthetic preferences. and the extraction template: TEMPLATE(BRANDS) { @BRAND, @PRODUCT } this hybrid rule: SCOPE SENTENCE { DOMAIN(05) + IDENTIFY(BRANDS) { @BRAND[ANCESTOR(376882)] <> LEMMA(\"design\") } } is activated by the brands cited in the text\u2014expressed by concepts that, in the Knowledge Graph, \"descend\" form the ancestor syncon with id 376882\u2014loosely followed by occurrences of the lemma design . As a consequence of activation, the rule both adds a default amount of points to the score of domain 05 and fills the @BRAND field of the BRANDS template, thus generating an extraction record. Rule labels, when needed, must be applied to the first portion of the header. For example: SCOPE scope { DOMAIN[[rule label]](domain_name) + IDENTIFY(template_name) { conditions } } Or: SCOPE scope { IDENTIFY[[rule label]](template_name) + DOMAIN(domain_name) { conditions } }","title":"Hybrid rules"},{"location":"hybrid-rules/#hybrid-rules","text":"If you need to extract information and categorize based on the same conditions, hybrid rules avoid you to create a double set of very similar rules, saving time and avoiding errors. A hybrid rule, when triggered, gives point to taxonomy domains and at the same time fills extraction template fields. To define a hybrid rule, use the plus sign ( + ) to write a header that combines the header of a categorization rule with that of an extraction rule , then define the condition as for an extraction rule: SCOPE scope { DOMAIN(domain_name) + IDENTIFY(template_name) { extraction rule style condition, specifying the fields to fill } } For example, considering the text: This chair was designed using Artificial Intelligence Celebrated French designer Phillipe Starck has come up with an algorithm for a chair. AFP RELAXNEWS Phillipe Starck has teamed up with a longstanding collaborator, Italian furniture company Kartell, to create the first chair to be designed using artificial intelligence, which is aptly named \"A.I\". Plastic furniture specialist Kartell embarked on a new direction when it adopted a development process using AI in the company of a long-time collaborator, French designer Philippe Starck. The man behind the ultra-famous \"Ghost\" chair selected a series of data and an algorithm to create a chair in line with criteria for strength and solidity as well as the designer's aesthetic preferences. and the extraction template: TEMPLATE(BRANDS) { @BRAND, @PRODUCT } this hybrid rule: SCOPE SENTENCE { DOMAIN(05) + IDENTIFY(BRANDS) { @BRAND[ANCESTOR(376882)] <> LEMMA(\"design\") } } is activated by the brands cited in the text\u2014expressed by concepts that, in the Knowledge Graph, \"descend\" form the ancestor syncon with id 376882\u2014loosely followed by occurrences of the lemma design . As a consequence of activation, the rule both adds a default amount of points to the score of domain 05 and fills the @BRAND field of the BRANDS template, thus generating an extraction record. Rule labels, when needed, must be applied to the first portion of the header. For example: SCOPE scope { DOMAIN[[rule label]](domain_name) + IDENTIFY(template_name) { conditions } } Or: SCOPE scope { IDENTIFY[[rule label]](template_name) + DOMAIN(domain_name) { conditions } }","title":"Hybrid rules"},{"location":"import/","text":"IMPORT The IMPORT statement includes rules contained in another source file. The syntax is: IMPORT file where file is the corresponding path of a rules file. The file path must be written between quotation marks. For example: IMPORT \"sports.cr\" When a build is performed, the IMPORT statement is replaced by the contents of the referenced rules file, so essentially, it is as if the file containing the IMPORT statement and the referenced file merged into one large file, which is the sum of the two files put together. The advantage of using IMPORT is that you can divide the rules of a project into multiple files which are easier to interpret and manage as opposed to just a single large file. If your project is complex, with many categories or template-field pairs and a lot of rules, IMPORT will provide a structure in which the rules can be kept in order. For example, in a categorization project, it is advisable to have a different file for every branch of a taxonomy, or in some cases, for every category. Similarly, in an extraction project there should be a file for every template or even for every field, depending on the number of rules. This makes rules easier to find and manage. If segments and sub-rules are used, they should be kept in separate files and they should be imported first (before the categorization and/or extraction rules). The proper order will ensure that the rules which reference the segments or sub-rules are processed after the definition of these two elements. For example: IMPORT \"subrules.cr\" IMPORT \"segments.cr\" IMPORT \"category1.cr\" IMPORT \"category2.cr\" The same file cannot be imported more than once and circular imports are not permitted (where file A imports file B and file B imports file A). The import file path can include sub-folders, such as this example: IMPORT \"leisure/sport/golf.cr\" In this way, rules can be divided among multiple files and these files can be organized into folders and sub-folders, if further structuring of the project is deemed necessary. The IMPORT starts to check the source files from the relative path in which the importing .cr file is placed. That means the import starts first from the subfolder and then from the rules folder. For example, in case where the rugby.cr file is inside a subfolder sport and a oval.cr source file is created in the same subfolder, the correct IMPORT syntax in rugby.cr is: IMPORT \"oval.cr\" and not: IMPORT \"sport\\oval.cr\" When the path includes sub-folder, please note that the slash character ( / ) is compatible with both Linux and Windows production environments, while backslash ( \\ ) can be used only in Windows.","title":"IMPORT"},{"location":"import/#import","text":"The IMPORT statement includes rules contained in another source file. The syntax is: IMPORT file where file is the corresponding path of a rules file. The file path must be written between quotation marks. For example: IMPORT \"sports.cr\" When a build is performed, the IMPORT statement is replaced by the contents of the referenced rules file, so essentially, it is as if the file containing the IMPORT statement and the referenced file merged into one large file, which is the sum of the two files put together. The advantage of using IMPORT is that you can divide the rules of a project into multiple files which are easier to interpret and manage as opposed to just a single large file. If your project is complex, with many categories or template-field pairs and a lot of rules, IMPORT will provide a structure in which the rules can be kept in order. For example, in a categorization project, it is advisable to have a different file for every branch of a taxonomy, or in some cases, for every category. Similarly, in an extraction project there should be a file for every template or even for every field, depending on the number of rules. This makes rules easier to find and manage. If segments and sub-rules are used, they should be kept in separate files and they should be imported first (before the categorization and/or extraction rules). The proper order will ensure that the rules which reference the segments or sub-rules are processed after the definition of these two elements. For example: IMPORT \"subrules.cr\" IMPORT \"segments.cr\" IMPORT \"category1.cr\" IMPORT \"category2.cr\" The same file cannot be imported more than once and circular imports are not permitted (where file A imports file B and file B imports file A). The import file path can include sub-folders, such as this example: IMPORT \"leisure/sport/golf.cr\" In this way, rules can be divided among multiple files and these files can be organized into folders and sub-folders, if further structuring of the project is deemed necessary. The IMPORT starts to check the source files from the relative path in which the importing .cr file is placed. That means the import starts first from the subfolder and then from the rules folder. For example, in case where the rugby.cr file is inside a subfolder sport and a oval.cr source file is created in the same subfolder, the correct IMPORT syntax in rugby.cr is: IMPORT \"oval.cr\" and not: IMPORT \"sport\\oval.cr\" When the path includes sub-folder, please note that the slash character ( / ) is compatible with both Linux and Windows production environments, while backslash ( \\ ) can be used only in Windows.","title":"IMPORT"},{"location":"names/","text":"Names The characters which can be used to name domains, templates, fields, segments, rule names and sub-rules names are: Lowercase or uppercase letters of the English alphabet ( a , b , c , ..., A , B , C , ...). Digits ( 0 , 1 , 2 , 3 , ...). Underscore ( _ ). Period ( . ). A name can not start with a digit, a period or an underscore. It is not recommended to end a name with a period. Examples of valid names: sports Crime01 FirstName last_name C01.01 Examples of invalid names: Invalid name Reason why it's non valid _DOSSIER_INFO Begins with an underscore 1FIELD Begins with a digit 00001.01 Begins with a digit 00001 Begins with a digit 10_subrule Begins with a digit donn\u00e9es_personnelles Contains an invalid character ( \u00e9 ) PERSONAL-DATA Contains an invalid character (dash, - )","title":"Names"},{"location":"names/#names","text":"The characters which can be used to name domains, templates, fields, segments, rule names and sub-rules names are: Lowercase or uppercase letters of the English alphabet ( a , b , c , ..., A , B , C , ...). Digits ( 0 , 1 , 2 , 3 , ...). Underscore ( _ ). Period ( . ). A name can not start with a digit, a period or an underscore. It is not recommended to end a name with a period. Examples of valid names: sports Crime01 FirstName last_name C01.01 Examples of invalid names: Invalid name Reason why it's non valid _DOSSIER_INFO Begins with an underscore 1FIELD Begins with a digit 00001.01 Begins with a digit 00001 Begins with a digit 10_subrule Begins with a digit donn\u00e9es_personnelles Contains an invalid character ( \u00e9 ) PERSONAL-DATA Contains an invalid character (dash, - )","title":"Names"},{"location":"operators/","text":"Operators overview Operators are language elements that allow the user to combine operands and sub-conditions to create complex conditions. The available choices are: Boolean operators Positional sequence operators Boolean-positional sequence operators ( NEXT , NEXT NOT ) Logical operators","title":"Operators overview"},{"location":"operators/#operators-overview","text":"Operators are language elements that allow the user to combine operands and sub-conditions to create complex conditions. The available choices are: Boolean operators Positional sequence operators Boolean-positional sequence operators ( NEXT , NEXT NOT ) Logical operators","title":"Operators overview"},{"location":"operators/boolean/","text":"Boolean operators Boolean operators are used to combine operands or sub-conditions to create simple or complex Boolean expressions. Operator Description AND Logical AND AND NOT Negation OR Inclusive OR XOR Exclusive OR","title":"Boolean operators"},{"location":"operators/boolean/#boolean-operators","text":"Boolean operators are used to combine operands or sub-conditions to create simple or complex Boolean expressions. Operator Description AND Logical AND AND NOT Negation OR Inclusive OR XOR Exclusive OR","title":"Boolean operators"},{"location":"operators/boolean/and/","text":"AND operator AND is the Boolean operator that combines attributes, two at a time, to create Boolean expressions that will be true, only if both the operands are true. Syntax is: operand1 AND operand2 ... AND must be written in uppercase. operand# may refer to a simple attribute, to a set combination of attributes or to a sequence of attributes (positional or logical). Consider the following example: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { LEMMA(\"abandon\") AND LEMMA(\"oil well\") } } The rule's condition is the combination of two LEMMA attributes. It will match the portion of input text delimited by rule's scope, if it contains at least a token having its lemma attribute set to abandon and at least another token with its lemma attribute set to oil well . Consider this text: Improperly abandoned oil wells, improperly abandoned facilities, and abandoned oil sumps are all potential sources of safety hazards. Prior to current abandonment procedures, oil wells were cut off below ground and capped. However, this procedure may not comply with the current abandonment standards. The first sentence contains: abandoned = lemma \"abandon\", three times oil wells = lemma \"oil well\", one time so it matches the rule's condition three times, the first time because of the first abandoned and the only oil wells , the second time because of the second abandoned and the only oil wells and the third time because of the third abandoned and the only oil wells . This means that the rule is triggered three times, the rule's action is performed three times and a NORMAL quantity of points (e.g., 60) is added to dom1 domain's cumulative score three times. Combined attributes can have more than one value. Compare the first sample rule with the following: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { LEMMA(\"abandon\", \"abandoned\", \"abandonment\", \"leave\", \"desert\", \"decommission\", \"decommissioning\", \"closure\") AND LEMMA (\"oil well\") } } When run against the same sample text, the rule is triggered four times, because the second sentence contains both abandonment AND oil wells . If the rule's scope is broader, as in: SCOPE SECTION (BODY) { DOMAIN(dom1:NORMAL) { LEMMA(\"abandon\", \"abandoned\", \"abandonment\", \"leave\", \"desert\", \"decommission\", \"decommissioning\", \"closure\") AND LEMMA (\"oil well\") } } the co-occurrence of attributes can happen across sentence boundaries. Since the sample text matches the first attribute five times and the second attribute twice, the rule now triggers ten times. In the following rule: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { KEYWORD(\"uncased\") >> LEMMA(\"well\") AND ANCESTOR(13769) // 13769: bore-hole, bore, borehole, drill hole } } AND is used to combine a strict positional sequence with the concepts that descend from an \"ancestor\" concept. If run against the following sample text: Temperature Distribution Around an Uncased Well with a History of Variations in Wellbore Temperature the condition is met because the Well disambiguation matches the ANCESTOR attribute. The extraction rule below shows the AND operator used to combine operands that are sets of combinations of attributes. SCOPE PHRASE (NP) { IDENTIFY(TEST) { @Well[LEMMA(\"well\") + TYPE(NOU)] AND @Depth[ANCESTOR(58535) + SYNCON(UNKNOWN)] // linear measure, linear unit, long measure } } The first operand will match the lemma well , only if it's a noun ( well is also an adverb, a verb and even an adjective), the second operand will match any type of linear measure, only if it is an unknown entity, that is an entity which is not inside the Knowledge Graph, but was assigned a virtual supernomen by the disambiguator. The condition must be met in the scope of a noun phrase. If the rule above is run against the following text: Repsol, in partnership with CGX, Tullow and YPF, is drilling a 6,500-metre well (21,000-feet) in the Guyana Basin that experts claim could hold 15 billion barrels of oil and 42 trillion cubic feet (1.2 trillion cubic metres) of natural gas. it is triggered by 6,500-metre (a virtual child of syncon 58808, meter , which in turn is a child of syncon 58535, linear measures ) and the noun well , both of which are found within the same noun phrase a 6,500-metre well . 21,000-feet is also a virtual child of syncon 58835 and thus a potentially valid value for the second attribute, but the noun phrase in which it is located does not contain the noun well .","title":"AND operator"},{"location":"operators/boolean/and/#and-operator","text":"AND is the Boolean operator that combines attributes, two at a time, to create Boolean expressions that will be true, only if both the operands are true. Syntax is: operand1 AND operand2 ... AND must be written in uppercase. operand# may refer to a simple attribute, to a set combination of attributes or to a sequence of attributes (positional or logical). Consider the following example: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { LEMMA(\"abandon\") AND LEMMA(\"oil well\") } } The rule's condition is the combination of two LEMMA attributes. It will match the portion of input text delimited by rule's scope, if it contains at least a token having its lemma attribute set to abandon and at least another token with its lemma attribute set to oil well . Consider this text: Improperly abandoned oil wells, improperly abandoned facilities, and abandoned oil sumps are all potential sources of safety hazards. Prior to current abandonment procedures, oil wells were cut off below ground and capped. However, this procedure may not comply with the current abandonment standards. The first sentence contains: abandoned = lemma \"abandon\", three times oil wells = lemma \"oil well\", one time so it matches the rule's condition three times, the first time because of the first abandoned and the only oil wells , the second time because of the second abandoned and the only oil wells and the third time because of the third abandoned and the only oil wells . This means that the rule is triggered three times, the rule's action is performed three times and a NORMAL quantity of points (e.g., 60) is added to dom1 domain's cumulative score three times. Combined attributes can have more than one value. Compare the first sample rule with the following: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { LEMMA(\"abandon\", \"abandoned\", \"abandonment\", \"leave\", \"desert\", \"decommission\", \"decommissioning\", \"closure\") AND LEMMA (\"oil well\") } } When run against the same sample text, the rule is triggered four times, because the second sentence contains both abandonment AND oil wells . If the rule's scope is broader, as in: SCOPE SECTION (BODY) { DOMAIN(dom1:NORMAL) { LEMMA(\"abandon\", \"abandoned\", \"abandonment\", \"leave\", \"desert\", \"decommission\", \"decommissioning\", \"closure\") AND LEMMA (\"oil well\") } } the co-occurrence of attributes can happen across sentence boundaries. Since the sample text matches the first attribute five times and the second attribute twice, the rule now triggers ten times. In the following rule: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { KEYWORD(\"uncased\") >> LEMMA(\"well\") AND ANCESTOR(13769) // 13769: bore-hole, bore, borehole, drill hole } } AND is used to combine a strict positional sequence with the concepts that descend from an \"ancestor\" concept. If run against the following sample text: Temperature Distribution Around an Uncased Well with a History of Variations in Wellbore Temperature the condition is met because the Well disambiguation matches the ANCESTOR attribute. The extraction rule below shows the AND operator used to combine operands that are sets of combinations of attributes. SCOPE PHRASE (NP) { IDENTIFY(TEST) { @Well[LEMMA(\"well\") + TYPE(NOU)] AND @Depth[ANCESTOR(58535) + SYNCON(UNKNOWN)] // linear measure, linear unit, long measure } } The first operand will match the lemma well , only if it's a noun ( well is also an adverb, a verb and even an adjective), the second operand will match any type of linear measure, only if it is an unknown entity, that is an entity which is not inside the Knowledge Graph, but was assigned a virtual supernomen by the disambiguator. The condition must be met in the scope of a noun phrase. If the rule above is run against the following text: Repsol, in partnership with CGX, Tullow and YPF, is drilling a 6,500-metre well (21,000-feet) in the Guyana Basin that experts claim could hold 15 billion barrels of oil and 42 trillion cubic feet (1.2 trillion cubic metres) of natural gas. it is triggered by 6,500-metre (a virtual child of syncon 58808, meter , which in turn is a child of syncon 58535, linear measures ) and the noun well , both of which are found within the same noun phrase a 6,500-metre well . 21,000-feet is also a virtual child of syncon 58835 and thus a potentially valid value for the second attribute, but the noun phrase in which it is located does not contain the noun well .","title":"AND operator"},{"location":"operators/boolean/and-not/","text":"AND NOT operator AND NOT is the Boolean operator that allows users to combine attributes, two at a time, to create Boolean expressions that will be true, if the first operand is true and the second operand is false. Syntax is: operand1 AND NOT operand2 ... AND NOT must be written in uppercase. operand# may refer to a simple attribute, to a set combination of attributes or to a sequence of attributes (positional or logical). Consider the following example: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { ANCESTOR(134210) // 134210: well AND NOT LEMMA(\"abandon\") } } The rule's condition is the combination of an ANCESTOR attribute and a LEMMA attribute. It will match the portion of input text delimited by the rule's scope, if it contains at least a token descending from syncon 134210 and it does not contain the lemma abandon . Consider this text: There are about 603 fields in the Niger Delta. Over 55 per cent of these are onshore, while the remaining is in the shallow waters (less than 500 meters). Of these fields, 193 are currently producing while 23 have either been closed down or abandoned. Across the Niger Delta, abandoned drill locations (including well stubs) numbering well over several tens of thousands. A large number of exploratory wells were drilled on land, swamp and offshore locations. The fifth sentence contains exploratory wells , a child of syncon 134210 and it does not contain occurrences of the lemma abandon so the rule can trigger and a NORMAL quantity of points (e.g., 60) is added to dom1 domain's cumulative score. The fourth sentence contains a match for the ANCESTOR attribute ( well ), but since this same sentence also contains the verb form abandoned , the rule's condition is not met. Combined attributes can have more than one value. Compare the first sample rule with the following: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { LEMMA(\"well\", \"field\", \"exploratory well\") AND NOT LEMMA(\"abandon\", \"close down\") } } Run against the same sample text, the rule is triggered twice, because the first sentence contains fields , the last sentence contains exploratory wells and both sentences do not contain occurrences of the lemma abandon or the lemma close down . The other sentences contain fields and well , matched by the first attribute, but since the same sentence also contains, respectively, closed down and abandoned , matched by the second attribute, the rule will not trigger. If the rule's scope is narrower, as in: SCOPE CLAUSE { DOMAIN(dom1:NORMAL) { LEMMA(\"well\", \"field\", \"exploratory well\") AND NOT LEMMA(\"abandon\", \"close down\") } } the rule will be triggered three times, because of the third sentence, where fields is matched by the first attribute and the clause in which the token is located does not contain occurrences of the lemmas close down or abandon . In the following extraction rules: SCOPE SENTENCE { IDENTIFY(TEST) { LEMMA(\"offshore\") <1:4> @OffShoreField_gas[TYPE(NPR)]|[TEXT] <1:4> LEMMA(\"discovery\") AND NOT ANCESTOR(64566) + TYPE(NOU)// 64566: petroleum, rock oil, fossil oil, oil, black gold } IDENTIFY(TEST) { LEMMA(\"offshore\") <1:4> @OffShoreField_oil[TYPE(NPR)]|[TEXT] <1:4> LEMMA(\"discovery\") AND NOT ANCESTOR(64471) + TYPE(NOU)// 64471: natural gas, gas } } the operator AND NOT is used to combine a positional sequence with the concepts that descend from an \"ancestor\" concept. If run against the following sample text: The offshore Tuui and Maari discoveries are predominantly oil. The basin remains under-explored compared to many comparable rift complex basins of its size and there remains considerable potential for further discoveries. the second rule will be triggered twice, extracting Tuui and Maari in the @OffShoreField_oil field; the first rule is not triggered, because its conditions ask for the lemma offshore , a proper noun and the lemma discovery in the same sentence at a certain maximum distance from each other (and the first sentence has them all), but does not \"tolerate\" the presence of oil , a concept which descends from ancestor syncon 64566.","title":"AND NOT operator"},{"location":"operators/boolean/and-not/#and-not-operator","text":"AND NOT is the Boolean operator that allows users to combine attributes, two at a time, to create Boolean expressions that will be true, if the first operand is true and the second operand is false. Syntax is: operand1 AND NOT operand2 ... AND NOT must be written in uppercase. operand# may refer to a simple attribute, to a set combination of attributes or to a sequence of attributes (positional or logical). Consider the following example: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { ANCESTOR(134210) // 134210: well AND NOT LEMMA(\"abandon\") } } The rule's condition is the combination of an ANCESTOR attribute and a LEMMA attribute. It will match the portion of input text delimited by the rule's scope, if it contains at least a token descending from syncon 134210 and it does not contain the lemma abandon . Consider this text: There are about 603 fields in the Niger Delta. Over 55 per cent of these are onshore, while the remaining is in the shallow waters (less than 500 meters). Of these fields, 193 are currently producing while 23 have either been closed down or abandoned. Across the Niger Delta, abandoned drill locations (including well stubs) numbering well over several tens of thousands. A large number of exploratory wells were drilled on land, swamp and offshore locations. The fifth sentence contains exploratory wells , a child of syncon 134210 and it does not contain occurrences of the lemma abandon so the rule can trigger and a NORMAL quantity of points (e.g., 60) is added to dom1 domain's cumulative score. The fourth sentence contains a match for the ANCESTOR attribute ( well ), but since this same sentence also contains the verb form abandoned , the rule's condition is not met. Combined attributes can have more than one value. Compare the first sample rule with the following: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { LEMMA(\"well\", \"field\", \"exploratory well\") AND NOT LEMMA(\"abandon\", \"close down\") } } Run against the same sample text, the rule is triggered twice, because the first sentence contains fields , the last sentence contains exploratory wells and both sentences do not contain occurrences of the lemma abandon or the lemma close down . The other sentences contain fields and well , matched by the first attribute, but since the same sentence also contains, respectively, closed down and abandoned , matched by the second attribute, the rule will not trigger. If the rule's scope is narrower, as in: SCOPE CLAUSE { DOMAIN(dom1:NORMAL) { LEMMA(\"well\", \"field\", \"exploratory well\") AND NOT LEMMA(\"abandon\", \"close down\") } } the rule will be triggered three times, because of the third sentence, where fields is matched by the first attribute and the clause in which the token is located does not contain occurrences of the lemmas close down or abandon . In the following extraction rules: SCOPE SENTENCE { IDENTIFY(TEST) { LEMMA(\"offshore\") <1:4> @OffShoreField_gas[TYPE(NPR)]|[TEXT] <1:4> LEMMA(\"discovery\") AND NOT ANCESTOR(64566) + TYPE(NOU)// 64566: petroleum, rock oil, fossil oil, oil, black gold } IDENTIFY(TEST) { LEMMA(\"offshore\") <1:4> @OffShoreField_oil[TYPE(NPR)]|[TEXT] <1:4> LEMMA(\"discovery\") AND NOT ANCESTOR(64471) + TYPE(NOU)// 64471: natural gas, gas } } the operator AND NOT is used to combine a positional sequence with the concepts that descend from an \"ancestor\" concept. If run against the following sample text: The offshore Tuui and Maari discoveries are predominantly oil. The basin remains under-explored compared to many comparable rift complex basins of its size and there remains considerable potential for further discoveries. the second rule will be triggered twice, extracting Tuui and Maari in the @OffShoreField_oil field; the first rule is not triggered, because its conditions ask for the lemma offshore , a proper noun and the lemma discovery in the same sentence at a certain maximum distance from each other (and the first sentence has them all), but does not \"tolerate\" the presence of oil , a concept which descends from ancestor syncon 64566.","title":"AND NOT operator"},{"location":"operators/boolean/or/","text":"OR operator OR is the Boolean operator that allows to combine attributes, two at a time, to create a Boolean expression that will be true, if at least one operand is true. Syntax is: operand1 OR operand2 ... OR must be written in uppercase. operand# may refer to a simple attribute, to a set combination of attributes or to a sequence of attributes (positional or logical). Consider the following example: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { KEYWORD(\"uncased well\") OR ANCESTOR(13769)// 13769: bore-hole, bore, borehole, drill hole } } The rule's condition is the combination of a KEYWORD attribute and an ANCESTOR attribute. It will match the portion of input text delimited by rule's scope, if it contains at least a token having its literal value set to uncased well or at least a token from a concept that descends from syncon 13769. Consider this text: A simple method is presented to estimate the distribution of temperature around an uncased well at any time. A method of determining temperature around the wellbore during the drilling period has been adopted from the literature and is extended to the period by superpositioning in the time domain the effect of temperature change. The first sentence contains: uncased well = keyword \"uncased well\", once The second sentence contains: wellbore = concept descending from syncon 13769, once The rule's condition is thus met twice and the rule is triggered twice. Now consider the same rule if run against a new sample text: When an uncased well is put into production, the pressure in the wellbore is lower than the pore pressure The rule is triggered only once, because there is only one sentence and it matches both operands (the first because of uncased well and the second because of wellbore ). The OR operator can be used to combine two attributes of the same type. Consider the following rule: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { KEYWORD(\"well bore\") OR KEYWORD(\"well bores\") } } against the sample text: Bering Exploration to utilize coiled tubing technology to re-enter abandoned well bore HOUSTON - Bering Exploration plans to utilize lateral technology to re-enter well bores that were previously abandoned. Bering currently has 34 abandoned well bores on its existing leases and has initially targeted three for re-entry utilizing coiled tubing laterals (CTL). The above rule is triggered three times by three different sentences, the first because of the first operand, the others because of the second operand. Now consider a modified sample rule where well bore and well bores are listed within the same attribute. SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { KEYWORD(\"well bore\",\"well bores\") } } If run against the same text, this rule would generate the same result as the previous rule. Using an attribute with n values (as in KEYWORD(\"well bore\", \"well bores\") ) is equivalent to combining n single-valued attributes of the same type with the OR attribute. Similarly, writing a single rule containing n attributes separated by the OR operator, as in: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { KEYWORD(\"uncased well\") OR ANCESTOR(13769)// 13769: bore-hole, bore, borehole, drill hole } } is equivalent to writing n separate rules with a single attribute each, as in. SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { KEYWORD(\"uncased well\") } DOMAIN(dom1:NORMAL) { ANCESTOR(13769)// 13769: bore-hole, bore, borehole, drill hole } } If used too often, the OR operator can cause a slowdown in the rules execution and, consequently, the engine performance, therefore it is advisable to use the OR operator only when necessary. Besides simple attributes, the OR operator can be used to combine sequences and combination of attributes as shown below. SCOPE SENTENCE { //abandon a well - lemma &VO ancestor DOMAIN(dom1:NORMAL) { LEMMA(\"abandon\", \"cap\") &VO ANCESTOR(18635, 39645) // oil well, oiler // oilfield, oil field OR LEMMA(\"abandoned\") + TYPE(ADJ) >> ANCESTOR(18635, 39645) // oil well, oiler // oilfield, oil field } } This rule combines a logical sequence with a strict positional sequence. Consider running the above rule against the following sample text: Most of the abandoned oil wells are dangerous because very few of them are really properly capped or retired. Most of these oil wells were not probably abandoned with proper evacuation procedures and very few of them ever get checked after abandonment. So pressure that builds up from gases in the earth and shifting earthquakes can cause oil and gas to come back to the surface, even on an oil well that had been capped. The text contains three pairs of values that match the rule, each one in a different sentence and therefore triggers the rule three times. The first pair is abandoned + oil wells in the first sentence, matched by the second operand, the second pair is oil wells + were (not) abandoned in the second sentence and the third is oil well + had been capped , both matched by the first operand, because oil wells and oil well are disambiguated as concepts descending from syncon 18635 and because a verb-object relation exists between were (not) abandoned and oil wells and between had been capped and oil well .","title":"OR operator"},{"location":"operators/boolean/or/#or-operator","text":"OR is the Boolean operator that allows to combine attributes, two at a time, to create a Boolean expression that will be true, if at least one operand is true. Syntax is: operand1 OR operand2 ... OR must be written in uppercase. operand# may refer to a simple attribute, to a set combination of attributes or to a sequence of attributes (positional or logical). Consider the following example: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { KEYWORD(\"uncased well\") OR ANCESTOR(13769)// 13769: bore-hole, bore, borehole, drill hole } } The rule's condition is the combination of a KEYWORD attribute and an ANCESTOR attribute. It will match the portion of input text delimited by rule's scope, if it contains at least a token having its literal value set to uncased well or at least a token from a concept that descends from syncon 13769. Consider this text: A simple method is presented to estimate the distribution of temperature around an uncased well at any time. A method of determining temperature around the wellbore during the drilling period has been adopted from the literature and is extended to the period by superpositioning in the time domain the effect of temperature change. The first sentence contains: uncased well = keyword \"uncased well\", once The second sentence contains: wellbore = concept descending from syncon 13769, once The rule's condition is thus met twice and the rule is triggered twice. Now consider the same rule if run against a new sample text: When an uncased well is put into production, the pressure in the wellbore is lower than the pore pressure The rule is triggered only once, because there is only one sentence and it matches both operands (the first because of uncased well and the second because of wellbore ). The OR operator can be used to combine two attributes of the same type. Consider the following rule: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { KEYWORD(\"well bore\") OR KEYWORD(\"well bores\") } } against the sample text: Bering Exploration to utilize coiled tubing technology to re-enter abandoned well bore HOUSTON - Bering Exploration plans to utilize lateral technology to re-enter well bores that were previously abandoned. Bering currently has 34 abandoned well bores on its existing leases and has initially targeted three for re-entry utilizing coiled tubing laterals (CTL). The above rule is triggered three times by three different sentences, the first because of the first operand, the others because of the second operand. Now consider a modified sample rule where well bore and well bores are listed within the same attribute. SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { KEYWORD(\"well bore\",\"well bores\") } } If run against the same text, this rule would generate the same result as the previous rule. Using an attribute with n values (as in KEYWORD(\"well bore\", \"well bores\") ) is equivalent to combining n single-valued attributes of the same type with the OR attribute. Similarly, writing a single rule containing n attributes separated by the OR operator, as in: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { KEYWORD(\"uncased well\") OR ANCESTOR(13769)// 13769: bore-hole, bore, borehole, drill hole } } is equivalent to writing n separate rules with a single attribute each, as in. SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { KEYWORD(\"uncased well\") } DOMAIN(dom1:NORMAL) { ANCESTOR(13769)// 13769: bore-hole, bore, borehole, drill hole } } If used too often, the OR operator can cause a slowdown in the rules execution and, consequently, the engine performance, therefore it is advisable to use the OR operator only when necessary. Besides simple attributes, the OR operator can be used to combine sequences and combination of attributes as shown below. SCOPE SENTENCE { //abandon a well - lemma &VO ancestor DOMAIN(dom1:NORMAL) { LEMMA(\"abandon\", \"cap\") &VO ANCESTOR(18635, 39645) // oil well, oiler // oilfield, oil field OR LEMMA(\"abandoned\") + TYPE(ADJ) >> ANCESTOR(18635, 39645) // oil well, oiler // oilfield, oil field } } This rule combines a logical sequence with a strict positional sequence. Consider running the above rule against the following sample text: Most of the abandoned oil wells are dangerous because very few of them are really properly capped or retired. Most of these oil wells were not probably abandoned with proper evacuation procedures and very few of them ever get checked after abandonment. So pressure that builds up from gases in the earth and shifting earthquakes can cause oil and gas to come back to the surface, even on an oil well that had been capped. The text contains three pairs of values that match the rule, each one in a different sentence and therefore triggers the rule three times. The first pair is abandoned + oil wells in the first sentence, matched by the second operand, the second pair is oil wells + were (not) abandoned in the second sentence and the third is oil well + had been capped , both matched by the first operand, because oil wells and oil well are disambiguated as concepts descending from syncon 18635 and because a verb-object relation exists between were (not) abandoned and oil wells and between had been capped and oil well .","title":"OR operator"},{"location":"operators/boolean/xor/","text":"XOR operator XOR is the Boolean operator that allows users to combine attributes, two at a time, to create Boolean expressions that will be true, if one of the operands is true, but will be false, if both are true. The syntax is: operand1 XOR operand2 ... XOR must be written in uppercase. operand# may refer to a simple attribute, to a set combination of attributes or to a sequence of attributes (positional or logical). Consider the following example: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { KEYWORD(\"uncased well\") XOR ANCESTOR(13769)// 13769: bore-hole, bore, borehole, drill hole } } The rule's condition is the combination of a KEYWORD attribute and an ANCESTOR attribute. It will match the portion of input text delimited by rule's scope, if it contains at least a token having its literal value set to uncased well or at least a token from a concept which descends from syncon 13769. Consider this text: A simple method is presented to estimate the distribution of temperature around an uncased well at any time. A method of determining temperature around the wellbore during the drilling period has been adopted from the literature and is extended to the period by superpositioning in the time domain the effect of temperature change. The first sentence contains: uncased well = keyword \"uncased well\", once The second sentence contains: wellbore = concept descending from syncon 13769, once The rule's condition is thus met twice and the rule is triggered twice. When an uncased well is put into production, the pressure in the wellbore is lower than the pore pressure Since uncased well and wellbore are in the same sentence, the rule is not triggered: both operands are true (in the rule's scope). See the topic about the OR operator to compare this behavior to the behavior that could be observed, if the same sample sentence were analyzed with the same rule, but with the OR operator. Note As shown with these examples, the exclusive OR is a peculiar operator defining very strict conditions in a rule. The user is advised to use it only when strictly necessary. The XOR operator can also be used to combine simple attributes with multiple values and combinations or sequences of attributes, both positional and logical. To see specific syntax, please refer to the examples given for the operators AND , AND NOT and OR .","title":"XOR operator"},{"location":"operators/boolean/xor/#xor-operator","text":"XOR is the Boolean operator that allows users to combine attributes, two at a time, to create Boolean expressions that will be true, if one of the operands is true, but will be false, if both are true. The syntax is: operand1 XOR operand2 ... XOR must be written in uppercase. operand# may refer to a simple attribute, to a set combination of attributes or to a sequence of attributes (positional or logical). Consider the following example: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { KEYWORD(\"uncased well\") XOR ANCESTOR(13769)// 13769: bore-hole, bore, borehole, drill hole } } The rule's condition is the combination of a KEYWORD attribute and an ANCESTOR attribute. It will match the portion of input text delimited by rule's scope, if it contains at least a token having its literal value set to uncased well or at least a token from a concept which descends from syncon 13769. Consider this text: A simple method is presented to estimate the distribution of temperature around an uncased well at any time. A method of determining temperature around the wellbore during the drilling period has been adopted from the literature and is extended to the period by superpositioning in the time domain the effect of temperature change. The first sentence contains: uncased well = keyword \"uncased well\", once The second sentence contains: wellbore = concept descending from syncon 13769, once The rule's condition is thus met twice and the rule is triggered twice. When an uncased well is put into production, the pressure in the wellbore is lower than the pore pressure Since uncased well and wellbore are in the same sentence, the rule is not triggered: both operands are true (in the rule's scope). See the topic about the OR operator to compare this behavior to the behavior that could be observed, if the same sample sentence were analyzed with the same rule, but with the OR operator. Note As shown with these examples, the exclusive OR is a peculiar operator defining very strict conditions in a rule. The user is advised to use it only when strictly necessary. The XOR operator can also be used to combine simple attributes with multiple values and combinations or sequences of attributes, both positional and logical. To see specific syntax, please refer to the examples given for the operators AND , AND NOT and OR .","title":"XOR operator"},{"location":"operators/compound-sequences/","text":"Compound sequences Positional and logical sequences may be combined to create compound sequences. The syntax is like this: operand1 positional operator operand2 logical operator operand3 ... Compound sequences are viewed by the text intelligence engine as separate sub-conditions joined by the positional and logical operators. Consider the following example: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { TYPE(NOU, NPR, NPH) &SV LEMMA(\"investigate\") <1:3> ANCESTOR(3113, 48760, 33606)// 3113: crime, offence, offense 48760: decease, last, death, tomb, mortality 33606: fortuitous event, fortuity, chance event, accident } } In the sample case, the first sub-condition is composed by the first two attributes linked by the logical operator ( &SV ), while the second is composed by the last two attributes linked by the positional operator ( <1:3> ). Therefore, the whole condition can be represented in the following way: 1. TYPE(NOU, NPR, NPH) &SV LEMMA(\"investigate\") 2. LEMMA(\"investigate\") <1:3> ANCESTOR(3113, 48760, 33606) The first sub-condition matches a common noun or a proper noun or a person's name being the subject of any inflection of verb investigate . The second sub-condition matches any inflection of lemma investigate followed by, within at most three words, any concept descending from syncon 3113 ( crime ), 48760 ( death ) or 33606 ( accident ). Both sub-conditions must be satisfied to trigger the rule. If the rule above is run against the following text: Authorities are investigating the death of a teenager working in the roof of a home in East Bunbury. The man, believed to be 18-years-old, was working alongside an electrician at a Petherick Street home when the incident occurred about 10.30am (AWST). An ambulance was called and fire crews had to pull the young man, who was unconscious, from inside the roof of the home. Attempts to resuscitate him failed and he was pronounced dead at Bunbury Regional Hospital. Police and officers from Worksafe, Energy Safety and Western Power are investigating. sub-condition 1 is satisfied by: authorities and investigating (first sentence) and sub-condition 2 is satisfied by: investigating and death (first sentence) so the rule is triggered. The use of of the negation operand ( ! ) is allowed, provided that the operand to which it refers is not part of a logical sequence. The syntax for using negation is as follows: !operand1 positional operator operand2 logical operator operand3 ... The following example is not valid: SCOPE scope_option { DOMAIN(domain_name:score_option)|IDENTIFY(template_name) { operand1 positional operator !operand2 logical operator operand3 ... } } Consider the first sample rule modified with a negation: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { TYPE(NOU, NPR, NPH) &SV LEMMA(\"investigate\") <1:3> !ANCESTOR(3113, 48760, 33606)// 3113: crime, offence, offense 48760: decease, last, death, tomb, mortality 33606: fortuitous event, fortuity, chance event, accident } } Now the second sub-condition matches any inflection of the lemma investigate only if not followed by, within at most three words, any concept descending from syncon 3113 ( crime ), 48760 ( death ) or 33606 ( accident ). The rule is no longer triggered by the first sentence because the verb investigating is followed by death within a distance of two words. On the other hand, the fifth sentence now triggers the rule two times thanks to Police and officers , which are both subjects of investigating .","title":"Compound sequences"},{"location":"operators/compound-sequences/#compound-sequences","text":"Positional and logical sequences may be combined to create compound sequences. The syntax is like this: operand1 positional operator operand2 logical operator operand3 ... Compound sequences are viewed by the text intelligence engine as separate sub-conditions joined by the positional and logical operators. Consider the following example: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { TYPE(NOU, NPR, NPH) &SV LEMMA(\"investigate\") <1:3> ANCESTOR(3113, 48760, 33606)// 3113: crime, offence, offense 48760: decease, last, death, tomb, mortality 33606: fortuitous event, fortuity, chance event, accident } } In the sample case, the first sub-condition is composed by the first two attributes linked by the logical operator ( &SV ), while the second is composed by the last two attributes linked by the positional operator ( <1:3> ). Therefore, the whole condition can be represented in the following way: 1. TYPE(NOU, NPR, NPH) &SV LEMMA(\"investigate\") 2. LEMMA(\"investigate\") <1:3> ANCESTOR(3113, 48760, 33606) The first sub-condition matches a common noun or a proper noun or a person's name being the subject of any inflection of verb investigate . The second sub-condition matches any inflection of lemma investigate followed by, within at most three words, any concept descending from syncon 3113 ( crime ), 48760 ( death ) or 33606 ( accident ). Both sub-conditions must be satisfied to trigger the rule. If the rule above is run against the following text: Authorities are investigating the death of a teenager working in the roof of a home in East Bunbury. The man, believed to be 18-years-old, was working alongside an electrician at a Petherick Street home when the incident occurred about 10.30am (AWST). An ambulance was called and fire crews had to pull the young man, who was unconscious, from inside the roof of the home. Attempts to resuscitate him failed and he was pronounced dead at Bunbury Regional Hospital. Police and officers from Worksafe, Energy Safety and Western Power are investigating. sub-condition 1 is satisfied by: authorities and investigating (first sentence) and sub-condition 2 is satisfied by: investigating and death (first sentence) so the rule is triggered. The use of of the negation operand ( ! ) is allowed, provided that the operand to which it refers is not part of a logical sequence. The syntax for using negation is as follows: !operand1 positional operator operand2 logical operator operand3 ... The following example is not valid: SCOPE scope_option { DOMAIN(domain_name:score_option)|IDENTIFY(template_name) { operand1 positional operator !operand2 logical operator operand3 ... } } Consider the first sample rule modified with a negation: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { TYPE(NOU, NPR, NPH) &SV LEMMA(\"investigate\") <1:3> !ANCESTOR(3113, 48760, 33606)// 3113: crime, offence, offense 48760: decease, last, death, tomb, mortality 33606: fortuitous event, fortuity, chance event, accident } } Now the second sub-condition matches any inflection of the lemma investigate only if not followed by, within at most three words, any concept descending from syncon 3113 ( crime ), 48760 ( death ) or 33606 ( accident ). The rule is no longer triggered by the first sentence because the verb investigating is followed by death within a distance of two words. On the other hand, the fifth sentence now triggers the rule two times thanks to Police and officers , which are both subjects of investigating .","title":"Compound sequences"},{"location":"operators/logical/","text":"Logical operators Logical operators allow the programmer to create combinations of operands based on the syntactic or logical role of the operands themselves in the sentence. The syntax is: operand1 logical_operator operand2 Possible values for logical_operator are: Operator Variant Description &VS &SV The verb and the subject &VO &OV The verb and the direct object &SO &OS The subject and the direct object &SS Two subjects (for the same verb) &OO Two objects (of the same verb) Operators are language keywords and must be written in uppercase. The reciprocal position in the text of matched tokens is irrelevant since the operators match both active and passive sentences (i.e., both grammatical and logical subjects are detected). The position (left or right) of each letter ( S , V , O ) inside the operator keyword indicates the position of the corresponding operand in the combination , not in the text . For example, with &SV the subject is matched by the left operands and the verb by the right, but matched tokens can appear in the text in any order; operands must be swapped when using &VS , but the resulting combination would be perfectly equivalent to the former. Therefore the variants can be considered advantageous because the operands can be written in the order preferred by the user. Consider the following categorization rule: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { TYPE(NOU, NPH) &OV ANCESTOR(71230,71231)// 71230: take in, seize, arrest, apprehend, cop, collar, nab, slough, nail, sneeze, pick up 71231: catch, capture, get, captive } } The rule's condition matches any common name or person's name that, in a clause, is the object of a verb ( &OV ) with a meaning descending from syncon 71230 ( to arrest ) or syncon 71231 ( to capture ). This alternative formulation: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { ANCESTOR(71230,71231)// 71230: take in, seize, arrest, apprehend, cop, collar, nab, slough, nail, sneeze, pick up 71231: catch, capture, get, captive &VO TYPE(NOU, NPH) } } is perfectly equivalent. If one of the rules above is run against the following text: Police arrested the man caught on camera while allegedly burglarizing a home in Rutherford, the Rutherford Police Department announced in a release. Stenborg was arrested at the Lodi address at about 1 p.m.. Stenborg's clothes matched those in the surveillance footage, police said. Police recovered the jewelry stolen from the Rutherford home, police said. it gets triggered two times, the first time by arrested and man in the first sentence and the second by Stemborg and arrested in the second sentence. The person's name Stenborg is recognized as the subject of the passive voice was arrested . The disambiguator is able to bring the passive voices back to the corresponding active form and thus recognize that the passive subject is indeed the active object. Info In the expert.ai programmers' community, logical operators are also known as logical sequences or logical relations .","title":"Logical operators"},{"location":"operators/logical/#logical-operators","text":"Logical operators allow the programmer to create combinations of operands based on the syntactic or logical role of the operands themselves in the sentence. The syntax is: operand1 logical_operator operand2 Possible values for logical_operator are: Operator Variant Description &VS &SV The verb and the subject &VO &OV The verb and the direct object &SO &OS The subject and the direct object &SS Two subjects (for the same verb) &OO Two objects (of the same verb) Operators are language keywords and must be written in uppercase. The reciprocal position in the text of matched tokens is irrelevant since the operators match both active and passive sentences (i.e., both grammatical and logical subjects are detected). The position (left or right) of each letter ( S , V , O ) inside the operator keyword indicates the position of the corresponding operand in the combination , not in the text . For example, with &SV the subject is matched by the left operands and the verb by the right, but matched tokens can appear in the text in any order; operands must be swapped when using &VS , but the resulting combination would be perfectly equivalent to the former. Therefore the variants can be considered advantageous because the operands can be written in the order preferred by the user. Consider the following categorization rule: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { TYPE(NOU, NPH) &OV ANCESTOR(71230,71231)// 71230: take in, seize, arrest, apprehend, cop, collar, nab, slough, nail, sneeze, pick up 71231: catch, capture, get, captive } } The rule's condition matches any common name or person's name that, in a clause, is the object of a verb ( &OV ) with a meaning descending from syncon 71230 ( to arrest ) or syncon 71231 ( to capture ). This alternative formulation: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { ANCESTOR(71230,71231)// 71230: take in, seize, arrest, apprehend, cop, collar, nab, slough, nail, sneeze, pick up 71231: catch, capture, get, captive &VO TYPE(NOU, NPH) } } is perfectly equivalent. If one of the rules above is run against the following text: Police arrested the man caught on camera while allegedly burglarizing a home in Rutherford, the Rutherford Police Department announced in a release. Stenborg was arrested at the Lodi address at about 1 p.m.. Stenborg's clothes matched those in the surveillance footage, police said. Police recovered the jewelry stolen from the Rutherford home, police said. it gets triggered two times, the first time by arrested and man in the first sentence and the second by Stemborg and arrested in the second sentence. The person's name Stenborg is recognized as the subject of the passive voice was arrested . The disambiguator is able to bring the passive voices back to the corresponding active form and thus recognize that the passive subject is indeed the active object. Info In the expert.ai programmers' community, logical operators are also known as logical sequences or logical relations .","title":"Logical operators"},{"location":"operators/next/","text":"NEXT operator NEXT is an operator that combines the Boolean operator AND with the flexible positional sequence operator ( <> ). The syntax is: operand1 NEXT operand2 NEXT is a language keyword and must be written in uppercase. The operator requires two input text tokens to match the combined operands in the order in which they are declared in the rule, regardless of the distance between them, as long as both are within the scope of the rule. Consider the following extraction rule: SCOPE SENTENCE*2 { IDENTIFY(TEST) { @NAME[TYPE(NPH)] NEXT @PHONE_NUMBER[TYPE(PHO)] } } The rule's condition matches a person's name ( TYPE(NPH) ) followed by a phone number ( TYPE(PHO) ) in the scope of at most two consecutive sentences. If the rule is run against this text: Personal Data ------------ Name: Tom Smith Phone number: +1 123 456 7890 Date of birth: 21/10/1967 the condition is met by Tom Smith and +1 123 456 7890 , which occur in consecutive sentences, so the rule is triggered.","title":"NEXT operator"},{"location":"operators/next/#next-operator","text":"NEXT is an operator that combines the Boolean operator AND with the flexible positional sequence operator ( <> ). The syntax is: operand1 NEXT operand2 NEXT is a language keyword and must be written in uppercase. The operator requires two input text tokens to match the combined operands in the order in which they are declared in the rule, regardless of the distance between them, as long as both are within the scope of the rule. Consider the following extraction rule: SCOPE SENTENCE*2 { IDENTIFY(TEST) { @NAME[TYPE(NPH)] NEXT @PHONE_NUMBER[TYPE(PHO)] } } The rule's condition matches a person's name ( TYPE(NPH) ) followed by a phone number ( TYPE(PHO) ) in the scope of at most two consecutive sentences. If the rule is run against this text: Personal Data ------------ Name: Tom Smith Phone number: +1 123 456 7890 Date of birth: 21/10/1967 the condition is met by Tom Smith and +1 123 456 7890 , which occur in consecutive sentences, so the rule is triggered.","title":"NEXT operator"},{"location":"operators/next-not/","text":"NEXT NOT operator NEXT NOT is an operator that combines the Boolean operator AND NOT with the flexible positional sequence operator ( <> ). It allows the user to combine operands and verify their presence/absence and position in a portion of text made of multiple sentences. In other words, in a scope larger than a sentence, the operator verifies whether the first declared operand is not followed by the second operand within the rule's scope. According to the scope scale, any number of sentences can be interposed between the operands. The syntax is: operand1 NEXT NOT operand2 ... NEXT NOT is a language keyword and must be written in uppercase. If two (or more) operands are defined in a rule, the NEXT NOT operator verifies whether the first operand is present and if it is not followed by the second operand. Consider the following categorization rule: SCOPE SENTENCE*2 { DOMAIN(dom1) { ANCESTOR(78207)//@SYN: #78207# [actual constructions] <:4> LEMMA(\"comply\",\"compliance\") <:4> LEMMA(\"fire safety\") NEXT NOT KEYWORD(\"however\") } } The condition matches a positional sequence of three attributes if not followed by the keyword however within the rule's scope. If the rule is run against this text: The mill complies with the fire safety standard. However, further upgrade work will be required to avert fire risk. The manufacturing plant is in compliance with the fire safety standard. A building code compliance certificate will be issued. the condition is met by: manufacturing plant , third sentence = descendant of syncon 78207. compliance , third sentence = lemma compliance . fire safety , third sentence = lemma fire safety . Absence of keyword however in the third and in the fourth sentence. The condition is not met in the first sentence because of the presence of keyword however in the second sentence.","title":"NEXT NOT operator"},{"location":"operators/next-not/#next-not-operator","text":"NEXT NOT is an operator that combines the Boolean operator AND NOT with the flexible positional sequence operator ( <> ). It allows the user to combine operands and verify their presence/absence and position in a portion of text made of multiple sentences. In other words, in a scope larger than a sentence, the operator verifies whether the first declared operand is not followed by the second operand within the rule's scope. According to the scope scale, any number of sentences can be interposed between the operands. The syntax is: operand1 NEXT NOT operand2 ... NEXT NOT is a language keyword and must be written in uppercase. If two (or more) operands are defined in a rule, the NEXT NOT operator verifies whether the first operand is present and if it is not followed by the second operand. Consider the following categorization rule: SCOPE SENTENCE*2 { DOMAIN(dom1) { ANCESTOR(78207)//@SYN: #78207# [actual constructions] <:4> LEMMA(\"comply\",\"compliance\") <:4> LEMMA(\"fire safety\") NEXT NOT KEYWORD(\"however\") } } The condition matches a positional sequence of three attributes if not followed by the keyword however within the rule's scope. If the rule is run against this text: The mill complies with the fire safety standard. However, further upgrade work will be required to avert fire risk. The manufacturing plant is in compliance with the fire safety standard. A building code compliance certificate will be issued. the condition is met by: manufacturing plant , third sentence = descendant of syncon 78207. compliance , third sentence = lemma compliance . fire safety , third sentence = lemma fire safety . Absence of keyword however in the third and in the fourth sentence. The condition is not met in the first sentence because of the presence of keyword however in the second sentence.","title":"NEXT NOT operator"},{"location":"operators/positional-sequences/","text":"Positional sequence operators Positional sequence operators allow users to create composite conditions that match two or more disambiguation tokens based on their reciprocal position and the type of tokens that are allowed between them. Operator Name Description >> Strict sequence The two tokens matched by the operands on the sides of the operator must be strictly consecutive, no other token is allowed between them > Loose sequence The two tokens matched by the operands on the sides of the operator must be positioned one after the other, but tokens with low semantic value\u2014adjectives, adverbs, conjunctions, articles\u2014are allowed between them <> Flexible sequence The two tokens matched by the operands on the sides of the operator must be positioned one after the other, but any number of tokens of any type can exist between them within the same sentence << Strict sequence with right reference Equivalent to >> , in the opposite direction, except in the presence of a negated operand < Loose sequence with right reference Equivalent to > , in the opposite direction, except in the presence of a negated operand Positional sequences can be combined with Boolean operators to create complex conditions.","title":"Positional sequence operators"},{"location":"operators/positional-sequences/#positional-sequence-operators","text":"Positional sequence operators allow users to create composite conditions that match two or more disambiguation tokens based on their reciprocal position and the type of tokens that are allowed between them. Operator Name Description >> Strict sequence The two tokens matched by the operands on the sides of the operator must be strictly consecutive, no other token is allowed between them > Loose sequence The two tokens matched by the operands on the sides of the operator must be positioned one after the other, but tokens with low semantic value\u2014adjectives, adverbs, conjunctions, articles\u2014are allowed between them <> Flexible sequence The two tokens matched by the operands on the sides of the operator must be positioned one after the other, but any number of tokens of any type can exist between them within the same sentence << Strict sequence with right reference Equivalent to >> , in the opposite direction, except in the presence of a negated operand < Loose sequence with right reference Equivalent to > , in the opposite direction, except in the presence of a negated operand Positional sequences can be combined with Boolean operators to create complex conditions.","title":"Positional sequence operators"},{"location":"operators/positional-sequences/flexible/","text":"Flexible sequence Unconstrained range The flexible sequence operator (less-than sign followed by greater-than sign, <> ) requires that the two tokens matched by the operands on its sides are positioned one after the other and in the same sentence, regardless how many tokens separate them. Syntax is: operand1 <> operand2 Consider the following example: SCOPE SENTENCE { IDENTIFY (TEST) { @company[ANCESTOR(37475) + TYPE(NPR) + ROLE(SUBJECT)]// 37475: company, enterprise, firm, house, > LEMMA(\"produce\", \"design\")+ TYPE (VER) <> @product[ANCESTOR(78687)]// 78687: artifact, artefact } } This extraction rule is meant to extract proper names of companies and the products they manufacture. The first operand matches any concept descending from syncon 37475 ( company ), but is limited to proper nouns ( +TYPE(NPR) ), thus excluding any common noun like limited liability company . The operand is further restricted so that only proper nouns playing the role of subject in a sentence or a clause ( +ROLE(SUBJECT) ) are matched. The second operand matches the inflections of the verbs produce and design and the matches third the concepts descending from syncon 78687 ( artifact ). A loose sequence operator is used to combine the first and the second operand while the flexible sequence operator is used between the second and the third. If the rule is run against the following sample text: The company Prada produces high-end ready-to-wear clothes for men and women. In addition Prada designs a range of children's clothes, fragrances, cosmetic products and accessories for men and women, including handbags, shoes, wallets and sunglasses. it is triggered several times. In the first sentence Prada is matched by the first operand, produces is matched by the second and clothes by the third. The rule is triggered because the tokens are found in the expected order. The second sentence activates the rule multiple times because of Prada , designs and all the \"artifacts\" ( children's clothes , fragrances , products , handbags , shoes etc.); the distance from the previous token doesn't matter since they are in the same sentence. Constrained range The flexible sequence operator accepts a parameter to specify the minimum required distance and the maximum allowed distance between the tokens corresponding to the combined operands. The parameter syntax is: <min:> or: <:max> or: <min:max> where min and max are two integer numbers indicating, respectively, the minimum required distance and the maximum allowed distance. When both min and max are specified, min can be negative. The distance is measured in words , punctuation marks and other symbols indicating the end of a sentence (for example period, exclamation and question marks) are counted as well. Consider the following example: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { TYPE(NPH) <1:3> ANCESTOR(71230)//# 71230: arrest, apprehend, seize, sneeze, slough, take in, nab, collar, pick up, cop, nail } } The condition of this categorization rule matches any person's name followed, in a range between one and three words inside the same sentence, by any expression of any concept that is a descendant of syncon 76936 ( to arrest ). If the rule is run against the following sample text: Cook County Sheriff Tammy Wheeler informed the press that Chris Collins was finally arrested today by Rhode Island State Police with domestic disorderly conduct and assault. Chris Collins is matched by the first operand and was (...) arrested is matched by the second. The distance between the two tokens is three words: 1. was 2. finally 3. arrested so the rule is triggered. By contrast, Tammy Wheeler and was (...) arrested , which are matched too by the two operands, do not trigger the rule because the distance between the matched tokens (nine words) exceeds the allowed maximum. Now, consider the following sample text: Cook County Sheriff Tammy Wheeler informed the press that today Rhode Island State Police finally arrested Chris Collins with domestic disorderly conduct and assault. In this case the rule is not triggered because arrested precedes Chris Collins . If the range is changed this way: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { TYPE(NPH) <-3:3> ANCESTOR(71230)//# 71230: arrest, apprehend, seize, sneeze, slough, take in, nab, collar, pick up, cop, nail } } the condition is met again because arrested has distance -1 from Chris Collins . Warning Do not combine negative attributes with a flexible sequence operator having a negative range. If you have to use negation, write two separate rules. Note Range <1:1> is equivalent to the strict sequence operator ( >> ). The <:max> syntax is equivalent to <1:max> , however the second form if preferred because it's more intelligible. It is also possible to have <0:max> . Zero is the conventional distance between two words that, together, are an inflection of a given compound lemma (for example state police ). Distance zero is useful when it is convenient to match compound lemmas together with possible variants that are not compound lemmas. Consider the following rule: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { KEYWORD(\"state\") <0:3> KEYWORD(\"police\", \"trooper\", \"troopers\") } } The condition states that the keyword state defined in the first attribute, has to precede a token matched by one or more of the keywords in the second attribute ( police , trooper , troopers ). The rule will be triggered only if the two elements are in the same sentence and are separated by a minimum of 0 to a maximum of 3 words. The range constraint min=0 allows the sequence to be valid even if the two tokens belong to the same lemma. If the rule above is run against the following text: New details are emerging as police continue to investigate a shooting spree that left several people dead and three state police troopers injured Friday morning in Central Pennsylvania. The incident started at a small church in Frankstown Township in Blair County, which is near Altoona. When it was over, three state troopers had to be hospitalized for injuries. \"I think we have three very fortunate state police members tonight\", said Lt. Col. George Bivens, of the Pennsylvania State Police. the rule is activated by: First sentence, state police First sentence, state (...) troopers Third sentence, state troopers Fourth sentence, state police In particular, in the first sentence, state police troopers triggers the rule two times, one because of state + police and the other because of state + troopers . The rule is triggered even if the disambiguator recognizes state police as a single lemma because of the minimum distance zero constraint.","title":"Flexible sequence"},{"location":"operators/positional-sequences/flexible/#flexible-sequence","text":"","title":"Flexible sequence"},{"location":"operators/positional-sequences/flexible/#unconstrained-range","text":"The flexible sequence operator (less-than sign followed by greater-than sign, <> ) requires that the two tokens matched by the operands on its sides are positioned one after the other and in the same sentence, regardless how many tokens separate them. Syntax is: operand1 <> operand2 Consider the following example: SCOPE SENTENCE { IDENTIFY (TEST) { @company[ANCESTOR(37475) + TYPE(NPR) + ROLE(SUBJECT)]// 37475: company, enterprise, firm, house, > LEMMA(\"produce\", \"design\")+ TYPE (VER) <> @product[ANCESTOR(78687)]// 78687: artifact, artefact } } This extraction rule is meant to extract proper names of companies and the products they manufacture. The first operand matches any concept descending from syncon 37475 ( company ), but is limited to proper nouns ( +TYPE(NPR) ), thus excluding any common noun like limited liability company . The operand is further restricted so that only proper nouns playing the role of subject in a sentence or a clause ( +ROLE(SUBJECT) ) are matched. The second operand matches the inflections of the verbs produce and design and the matches third the concepts descending from syncon 78687 ( artifact ). A loose sequence operator is used to combine the first and the second operand while the flexible sequence operator is used between the second and the third. If the rule is run against the following sample text: The company Prada produces high-end ready-to-wear clothes for men and women. In addition Prada designs a range of children's clothes, fragrances, cosmetic products and accessories for men and women, including handbags, shoes, wallets and sunglasses. it is triggered several times. In the first sentence Prada is matched by the first operand, produces is matched by the second and clothes by the third. The rule is triggered because the tokens are found in the expected order. The second sentence activates the rule multiple times because of Prada , designs and all the \"artifacts\" ( children's clothes , fragrances , products , handbags , shoes etc.); the distance from the previous token doesn't matter since they are in the same sentence.","title":"Unconstrained range"},{"location":"operators/positional-sequences/flexible/#constrained-range","text":"The flexible sequence operator accepts a parameter to specify the minimum required distance and the maximum allowed distance between the tokens corresponding to the combined operands. The parameter syntax is: <min:> or: <:max> or: <min:max> where min and max are two integer numbers indicating, respectively, the minimum required distance and the maximum allowed distance. When both min and max are specified, min can be negative. The distance is measured in words , punctuation marks and other symbols indicating the end of a sentence (for example period, exclamation and question marks) are counted as well. Consider the following example: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { TYPE(NPH) <1:3> ANCESTOR(71230)//# 71230: arrest, apprehend, seize, sneeze, slough, take in, nab, collar, pick up, cop, nail } } The condition of this categorization rule matches any person's name followed, in a range between one and three words inside the same sentence, by any expression of any concept that is a descendant of syncon 76936 ( to arrest ). If the rule is run against the following sample text: Cook County Sheriff Tammy Wheeler informed the press that Chris Collins was finally arrested today by Rhode Island State Police with domestic disorderly conduct and assault. Chris Collins is matched by the first operand and was (...) arrested is matched by the second. The distance between the two tokens is three words: 1. was 2. finally 3. arrested so the rule is triggered. By contrast, Tammy Wheeler and was (...) arrested , which are matched too by the two operands, do not trigger the rule because the distance between the matched tokens (nine words) exceeds the allowed maximum. Now, consider the following sample text: Cook County Sheriff Tammy Wheeler informed the press that today Rhode Island State Police finally arrested Chris Collins with domestic disorderly conduct and assault. In this case the rule is not triggered because arrested precedes Chris Collins . If the range is changed this way: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { TYPE(NPH) <-3:3> ANCESTOR(71230)//# 71230: arrest, apprehend, seize, sneeze, slough, take in, nab, collar, pick up, cop, nail } } the condition is met again because arrested has distance -1 from Chris Collins . Warning Do not combine negative attributes with a flexible sequence operator having a negative range. If you have to use negation, write two separate rules. Note Range <1:1> is equivalent to the strict sequence operator ( >> ). The <:max> syntax is equivalent to <1:max> , however the second form if preferred because it's more intelligible. It is also possible to have <0:max> . Zero is the conventional distance between two words that, together, are an inflection of a given compound lemma (for example state police ). Distance zero is useful when it is convenient to match compound lemmas together with possible variants that are not compound lemmas. Consider the following rule: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { KEYWORD(\"state\") <0:3> KEYWORD(\"police\", \"trooper\", \"troopers\") } } The condition states that the keyword state defined in the first attribute, has to precede a token matched by one or more of the keywords in the second attribute ( police , trooper , troopers ). The rule will be triggered only if the two elements are in the same sentence and are separated by a minimum of 0 to a maximum of 3 words. The range constraint min=0 allows the sequence to be valid even if the two tokens belong to the same lemma. If the rule above is run against the following text: New details are emerging as police continue to investigate a shooting spree that left several people dead and three state police troopers injured Friday morning in Central Pennsylvania. The incident started at a small church in Frankstown Township in Blair County, which is near Altoona. When it was over, three state troopers had to be hospitalized for injuries. \"I think we have three very fortunate state police members tonight\", said Lt. Col. George Bivens, of the Pennsylvania State Police. the rule is activated by: First sentence, state police First sentence, state (...) troopers Third sentence, state troopers Fourth sentence, state police In particular, in the first sentence, state police troopers triggers the rule two times, one because of state + police and the other because of state + troopers . The rule is triggered even if the disambiguator recognizes state police as a single lemma because of the minimum distance zero constraint.","title":"Constrained range"},{"location":"operators/positional-sequences/inhibitors/","text":"Inhibitors of the distribution in sequences The operands in a flexible sequence (operator <> ) and in a loose sequence (operator > ) can optionally accept inhibitors which limit their matches to the first or the last occurrence. In other words, if an operator matches more than one token, it is possible to discard all but the first occurrence with the minus inhibitor or all but the last occurrence with the plus inhibitor. The syntax is: +|-operand1 <>|> +|-operand2 ... Consider the following categorization rule: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { ANCESTOR(69346)// 69346: look into, investigate, see about <> ANCESTOR(3113)// 3113: crime, offence, offense } } The rule's condition matches any concept descending from syncon 69346 ( investigate ) flexibly followed, in the same sentence, by any concept descending from syncon 3113 ( crime ). If the rule is run against the following text: A Tech officer investigated an attempted sexual assault and kidnapping, which occurred at Wall Residence Hall. A female student alleged a sexual assault by an acquaintance in her apartment. it is triggered two times, both in the first sentence, due to: investigated followed by sexual assault . investigated followed by kidnapping . One token is matched by the first attribute and two are matched by the second. Now consider the previous rule with an inhibitor added to the second operand: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { ANCESTOR(69346)// 69346: look into, investigate, see about <> -ANCESTOR(3113)// 3113: crime, offence, offense } } The second attribute will now match only sexual assault because it is the first (in positional order) of the tokens matched by the attribute: if the plus sign was used instead, the second attribute would match only kidnapping because it is the last token. When a single rule can have multiple candidates for two or more operands, it is possible to combine plus and minus signs in order to combine the occurrences as needed. For example, given the following flexible sequence: attribute_A <> attribute_B and the following tokens' stream: A1 A2 B1 B2 different uses of plus and minus will produce the results illustrated in the table below. Condition with inhibitors Result +attribute_A <> +attribute_B A2 B2 +attribute_A <> -attribute_B A2 B1 -attribute_A <> -attribute_B A1 B1 -attribute_A <> +attribute_B A1 B2","title":"Inhibitors of the distribution in sequences"},{"location":"operators/positional-sequences/inhibitors/#inhibitors-of-the-distribution-in-sequences","text":"The operands in a flexible sequence (operator <> ) and in a loose sequence (operator > ) can optionally accept inhibitors which limit their matches to the first or the last occurrence. In other words, if an operator matches more than one token, it is possible to discard all but the first occurrence with the minus inhibitor or all but the last occurrence with the plus inhibitor. The syntax is: +|-operand1 <>|> +|-operand2 ... Consider the following categorization rule: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { ANCESTOR(69346)// 69346: look into, investigate, see about <> ANCESTOR(3113)// 3113: crime, offence, offense } } The rule's condition matches any concept descending from syncon 69346 ( investigate ) flexibly followed, in the same sentence, by any concept descending from syncon 3113 ( crime ). If the rule is run against the following text: A Tech officer investigated an attempted sexual assault and kidnapping, which occurred at Wall Residence Hall. A female student alleged a sexual assault by an acquaintance in her apartment. it is triggered two times, both in the first sentence, due to: investigated followed by sexual assault . investigated followed by kidnapping . One token is matched by the first attribute and two are matched by the second. Now consider the previous rule with an inhibitor added to the second operand: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { ANCESTOR(69346)// 69346: look into, investigate, see about <> -ANCESTOR(3113)// 3113: crime, offence, offense } } The second attribute will now match only sexual assault because it is the first (in positional order) of the tokens matched by the attribute: if the plus sign was used instead, the second attribute would match only kidnapping because it is the last token. When a single rule can have multiple candidates for two or more operands, it is possible to combine plus and minus signs in order to combine the occurrences as needed. For example, given the following flexible sequence: attribute_A <> attribute_B and the following tokens' stream: A1 A2 B1 B2 different uses of plus and minus will produce the results illustrated in the table below. Condition with inhibitors Result +attribute_A <> +attribute_B A2 B2 +attribute_A <> -attribute_B A2 B1 -attribute_A <> -attribute_B A1 B1 -attribute_A <> +attribute_B A1 B2","title":"Inhibitors of the distribution in sequences"},{"location":"operators/positional-sequences/loose/","text":"Loose sequence The loose sequence operator (greater-than sign, > ) requires that the two tokens matched by the operands on its sides are positioned one after the other and in the same sentence, without any token between them or separated only by tokens with low semantic value, such as articles, adjectives, adverbs, prepositions and conjunctions. Nouns and verbs are therefore not allowed. The syntax is: operand1 > operand2 Consider the following example: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { LEMMA(\"disease\", \"disorder\") > ANCESTOR(26151)// 26151: nervous system } } The rule's condition matches any loose sequence of two tokens in which the first token has its lemma attribute set to disease or disorder and the next token is a concept descending from syncon 26151 ( nervous system ). Consider the following sample text: Major new epidemiological analyses are focusing attention on disorders of the nervous system as important causes of death and disability around the world. One in every 9 individuals dies of a nervous system disorder. The rule is triggered just once because of disorders of the nervous system found in the first sentence. The first operand of the condition matches disorders , the second matches nervous system and the tokens in between are acceptable because their semantic value is considered low ( of = preposition, the = article). The first operand of the condition also matches disorder in the second sentence and the second operand matches nervous system in the same sentence, but the overall condition is not met because the tokens are not positioned in the \"right\" order. Besides simple attributes, the loose sequence operator can be used with set combinations of attributes as shown in the example below. SCOPE SENTENCE { IDENTIFY (TEST) { @company[ANCESTOR(37475) + TYPE(NPR) + ROLE(SUBJECT)]// 37475: company, enterprise, firm, house, > LEMMA(\"produce\", \"design\") + TYPE (VER) > @product[ANCESTOR(78687)]// 78687: artifact, artefact } } This extraction rule is meant to extract proper names of companies and the products they manufacture. The first operand matches any concept descending from syncon 37475 ( company ), but is limited to proper nouns ( +TYPE(NPR) ), thus excluding any common noun like limited liability company . The operand is further restricted so that only proper nouns playing the role of subject in a sentence or a clause ( +ROLE(SUBJECT) ) are matched. The second operand matches the inflections of the verbs produce and design and the third matches the concepts which descend from syncon 78687 ( artifact ). If the rule is run against the following sample text: Prada produces high-end ready-to-wear clothes for men and women. In addition Prada designs a range of children's clothes, fragrances, cosmetic products and accessories for men and women, including handbags, shoes, wallets and sunglasses. in the first sentence, Prada is matched by the first operand, produces by the second and clothes by the third. The rule is triggered because: Tokens are found in the expected order. There are no tokens between Prada and produces (and zero tokens is OK). There are only adjectives ( high-end , ready-to-wear ) between produces and clothes . Prada designs a range of children's clothes in the second sentence does not trigger the rule even if all three operands find a match in the expected order because of the noun range (nouns have high semantic value). Right reference The loose sequence operator with right reference (less-than sign, < ) is perfectly equivalent to the loose sequence operator unless it's combined with negative operands. To understand the effect of the right reference on sequence interpretation read the paragraphs under Right reference operators in the topic about negations in sequences .","title":"Loose sequence"},{"location":"operators/positional-sequences/loose/#loose-sequence","text":"The loose sequence operator (greater-than sign, > ) requires that the two tokens matched by the operands on its sides are positioned one after the other and in the same sentence, without any token between them or separated only by tokens with low semantic value, such as articles, adjectives, adverbs, prepositions and conjunctions. Nouns and verbs are therefore not allowed. The syntax is: operand1 > operand2 Consider the following example: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { LEMMA(\"disease\", \"disorder\") > ANCESTOR(26151)// 26151: nervous system } } The rule's condition matches any loose sequence of two tokens in which the first token has its lemma attribute set to disease or disorder and the next token is a concept descending from syncon 26151 ( nervous system ). Consider the following sample text: Major new epidemiological analyses are focusing attention on disorders of the nervous system as important causes of death and disability around the world. One in every 9 individuals dies of a nervous system disorder. The rule is triggered just once because of disorders of the nervous system found in the first sentence. The first operand of the condition matches disorders , the second matches nervous system and the tokens in between are acceptable because their semantic value is considered low ( of = preposition, the = article). The first operand of the condition also matches disorder in the second sentence and the second operand matches nervous system in the same sentence, but the overall condition is not met because the tokens are not positioned in the \"right\" order. Besides simple attributes, the loose sequence operator can be used with set combinations of attributes as shown in the example below. SCOPE SENTENCE { IDENTIFY (TEST) { @company[ANCESTOR(37475) + TYPE(NPR) + ROLE(SUBJECT)]// 37475: company, enterprise, firm, house, > LEMMA(\"produce\", \"design\") + TYPE (VER) > @product[ANCESTOR(78687)]// 78687: artifact, artefact } } This extraction rule is meant to extract proper names of companies and the products they manufacture. The first operand matches any concept descending from syncon 37475 ( company ), but is limited to proper nouns ( +TYPE(NPR) ), thus excluding any common noun like limited liability company . The operand is further restricted so that only proper nouns playing the role of subject in a sentence or a clause ( +ROLE(SUBJECT) ) are matched. The second operand matches the inflections of the verbs produce and design and the third matches the concepts which descend from syncon 78687 ( artifact ). If the rule is run against the following sample text: Prada produces high-end ready-to-wear clothes for men and women. In addition Prada designs a range of children's clothes, fragrances, cosmetic products and accessories for men and women, including handbags, shoes, wallets and sunglasses. in the first sentence, Prada is matched by the first operand, produces by the second and clothes by the third. The rule is triggered because: Tokens are found in the expected order. There are no tokens between Prada and produces (and zero tokens is OK). There are only adjectives ( high-end , ready-to-wear ) between produces and clothes . Prada designs a range of children's clothes in the second sentence does not trigger the rule even if all three operands find a match in the expected order because of the noun range (nouns have high semantic value).","title":"Loose sequence"},{"location":"operators/positional-sequences/loose/#right-reference","text":"The loose sequence operator with right reference (less-than sign, < ) is perfectly equivalent to the loose sequence operator unless it's combined with negative operands. To understand the effect of the right reference on sequence interpretation read the paragraphs under Right reference operators in the topic about negations in sequences .","title":"Right reference"},{"location":"operators/positional-sequences/negation/","text":"Negations in sequences Overview The negation operator (exclamation mark, ! ) can be placed before an operand in a positional sequence to negate the operand, that is to indicate that something must not be present in that position of the sequence. The syntax is: !operand It is possible to negate more than one operand in a positional sequence, but at least one operand must be \"positive\", i.e. not negated. Consider the following rule: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { LEMMA(\"arrest\") >> !LEMMA(\"record\") } } The rule's condition is met by the lemma arrest not strictly followed by the lemma record . If the rule is run against the following text: An officer who resigned from Weslaco Police Department amid drunken-driving allegations earlier this month was arrested after refusing a Breathalyzer test, according to arrest records obtained by The Monitor. lemma arrest is found two times. but the second time it is followed by lemma record so the rule is triggered only the first time. The previous example shows how to negate the last operand in a sequence. It is also possible to negate the first operand and to negate several operands, as described below. Left reference principle By default, positive operands \"point to\" all the negative operands between them and the next positive operand, if any, in the sequence. Also, at at the same time, each positive operand \"points to\" the next positive operand. In other words, a positive operand is the left reference of any negative operand following it in the sequence and also of the next positive operand, if any. Negative operands, on their hand, are the left reference of the first positive operand following them. So, if there are negative operands between positive operands, it is as if they are transparent when it comes to the relationship of a positive operand with the next. For example, in the following sequence: Positive operand 1 Sequence operator 1 Negative operand Sequence operator 2 Positive operand 2 operator 1 determines the combination between the first positive operand and the negative operand and operator 2 determines the combination between the first positive operand and the second positive operand. In the absence of preceding positive operands, negative operands \"point to\" the next positive operand. For example, in the following sequence: Negative operand 1 Sequence operator 1 Negative operand 2 Sequence operator 2 Positive operand operator 1 determines the combination between the first negative operand and the positive operand and operator 2 determines the combination between the second negative operand and the positive operand. Consider the following rule. SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { !ANCESTOR(47277)// 47277: military personnel, military man, servicemember, serviceman, man, service person, service man, serviceperson, military-man <> !LEMMA(\"avoid\", \"try\", \"attempt\") > LEMMA(\"arrest\") } } The first negative operand is combined to the only positive operand with a flexible sequence operator; the second negative operand is combined with a loose sequence operator, as illustrated below. !ANCESTOR(47277) flexibly followed by: \u2572 LEMMA(\"arrest\") \u2571 !LEMMA(\"avoid\", \"try\", \"attempt\") loosely followed by: It is sufficient that one negative operand is matched not to satisfy the rule. If the rule above is run against the following text: Last night more than forty Israeli soldiers invaded the city of Nablus and raided two homes looking for two young men. One of them was arrested during the raid and the other one avoided arrest as he was working at the time of the raid. At 2.30 am, Israeli soldiers broke into Mead Nijad's interrupting the family's sleep. As the soldiers entered the house, they ordered everyone to have their hands up; they asked for Emad, blindfolded, handcuffed and arrested him. the rule's condition is matched just once. In fact, the positive operand matches three tokens ( arrested , arrest in first sentence and arrest in the third sentence), but avoided (matched by !LEMMA(\"avoid\", \"try\", \"attempt\") ) immediately precedes the first occurrence of arrest and soldiers (matched by !ANCESTOR(47277) precedes the second occurrence from a distance. Right reference operators Normal sequence operators point in the forward direction. Right reference operators (single less-than sign, < , and double less-than sign, << ) point in the opposite direction. Right reference operators can be used interchangeably with normal operators, but when all of the rule's operands are positive there's no reason for doing so since the sequences built with them are perfectly equivalent to sequences built with normal operators and they are less clear. For example, conditions: LEMMA(\"confirm\") >> LEMMA(\"arrest\") and: LEMMA(\"confirm\") << LEMMA(\"arrest\") are perfectly equivalent, but the second contrasts with the general left-to-right reading direction which is common in the Rules language. Right reference operators, however, are useful in changing the normal relationship between negative operands and positive operands; in fact, they should be used only when negative operands are involved. Consider the following categorization rule, meant to give points to the dom1 domain whenever the text concerns the arrest of people. SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { TYPE(NPH) <> ANCESTOR(71230, 71231)// 71230: take in, seize, arrest, apprehend, cop, collar, nab, slough, nail, sneeze, pick up 71231: catch, capture, get, captive } } The rule's condition matches a person's name ( TYPE (NPH) ) followed, at any distance, but in the same sentence, by any concept that descends from syncon 71230 ( to arrest ) or syncon 71231 ( to capture ). If the rule is run against the following text: John McAfee, the multimillionaire software developer, has not been captured, despite a cryptic post on his own blog saying that he is in police custody. it is triggered by the John McAfee and captured sequence, but this is not what the users expects, because the text states that the man has not been captured . A more precise rule is triggered only if the verb is not negated. The following change in rule's condition: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { TYPE(NPH) <> !KEYWORD (\"has not been\", \"have not been\", \"was not\", \"wasn't\", \"were not\", \"weren't\") >> ANCESTOR(71230, 71231)// 71230: take in, seize, arrest, apprehend, cop, collar, nab, slough, nail, sneeze, pick up 71231: catch, capture, get, captive } } seems to achieve the desired effect. The second operand should match has not been before captured but, being a negated operand, it should prevent the whole condition from being satisfied. Indeed, the rule is not triggered (apparently a good result), but it is not due to the user's logic. If the modified rule is run against this text: McAfee was in the midst of recounting an incident earlier this month in which 42 armed Belizean officers allegedly stormed his compound, arrested him and detained him for 14 hours with no food or water, and then let him go without charges. it isn't triggered either and, again, this is not what the user was going for. Now we have one negative operand and two positive operands: how are they combined? The rule above is interpreted by the text intelligence engine as illustrated below. strictly followed by ANCESTOR(71230, 71231) \u2571 TYPE(NPH) \u2572 NOT flexibly followed by KEYWORD (\"has not been\", \"have not... This can be read in this way: a person's name strictly followed ( >> ) by any concept descending from syncon 71230 ( to arrest ) or syncon 71231 ( to capture ) and , at the same time, not flexibly followed ( <> ) by one of the keyword in the negated operand. This is because of the left reference principle: the first positive operand is both the left reference of the negative operator and of the second positive operand, therefore the negative operand between the positive operands is transparent when it comes to the relationship between the two positive operands: TYPE(NPH) >> ANCESTOR(71230, 71231) This \"positive\" condition doesn't hold true because arrested is too far from McAfee , so the whole condition is not met and the rule is not triggered. The negative operand would be evaluated like this: TYPE(NPH) <> !KEYWORD (\"has not been\", \"have not been\", \"was not\", ... and actually it holds true, but it's \"too late\". Sequence operators with a right reference come to the rescue! If the rule above is changed this way: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { TYPE(NPH) <> !KEYWORD (\"has not been\", \"have not been\", \"was not\", \"wasn't\", \"were not\", \"weren't\") << ANCESTOR(71230, 71231)// 71230: take in, seize, arrest, apprehend, cop, collar, nab, slough, nail, sneeze, pick up 71231: catch, capture, get, captive } } it is interpreted by the text intelligence engine as follows: TYPE(NPH) \u2572 flexibly followed by ANCESTOR(71230, 71231) \u2571 NOT strictly preceded by KEYWORD (\"has not been\", \"have not... The right reference operator takes the negative operand away from the first positive operand's \"attraction\" and ties it to the second positive operand which becomes the negative operand right reference . The first operator determines the combination between the first positive operand and the next positive operand, not the combination between the first positive and the next negative. Results will now be expected for the sample texts.","title":"Negations in sequences"},{"location":"operators/positional-sequences/negation/#negations-in-sequences","text":"","title":"Negations in sequences"},{"location":"operators/positional-sequences/negation/#overview","text":"The negation operator (exclamation mark, ! ) can be placed before an operand in a positional sequence to negate the operand, that is to indicate that something must not be present in that position of the sequence. The syntax is: !operand It is possible to negate more than one operand in a positional sequence, but at least one operand must be \"positive\", i.e. not negated. Consider the following rule: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { LEMMA(\"arrest\") >> !LEMMA(\"record\") } } The rule's condition is met by the lemma arrest not strictly followed by the lemma record . If the rule is run against the following text: An officer who resigned from Weslaco Police Department amid drunken-driving allegations earlier this month was arrested after refusing a Breathalyzer test, according to arrest records obtained by The Monitor. lemma arrest is found two times. but the second time it is followed by lemma record so the rule is triggered only the first time. The previous example shows how to negate the last operand in a sequence. It is also possible to negate the first operand and to negate several operands, as described below.","title":"Overview"},{"location":"operators/positional-sequences/negation/#left-reference-principle","text":"By default, positive operands \"point to\" all the negative operands between them and the next positive operand, if any, in the sequence. Also, at at the same time, each positive operand \"points to\" the next positive operand. In other words, a positive operand is the left reference of any negative operand following it in the sequence and also of the next positive operand, if any. Negative operands, on their hand, are the left reference of the first positive operand following them. So, if there are negative operands between positive operands, it is as if they are transparent when it comes to the relationship of a positive operand with the next. For example, in the following sequence: Positive operand 1 Sequence operator 1 Negative operand Sequence operator 2 Positive operand 2 operator 1 determines the combination between the first positive operand and the negative operand and operator 2 determines the combination between the first positive operand and the second positive operand. In the absence of preceding positive operands, negative operands \"point to\" the next positive operand. For example, in the following sequence: Negative operand 1 Sequence operator 1 Negative operand 2 Sequence operator 2 Positive operand operator 1 determines the combination between the first negative operand and the positive operand and operator 2 determines the combination between the second negative operand and the positive operand. Consider the following rule. SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { !ANCESTOR(47277)// 47277: military personnel, military man, servicemember, serviceman, man, service person, service man, serviceperson, military-man <> !LEMMA(\"avoid\", \"try\", \"attempt\") > LEMMA(\"arrest\") } } The first negative operand is combined to the only positive operand with a flexible sequence operator; the second negative operand is combined with a loose sequence operator, as illustrated below. !ANCESTOR(47277) flexibly followed by: \u2572 LEMMA(\"arrest\") \u2571 !LEMMA(\"avoid\", \"try\", \"attempt\") loosely followed by: It is sufficient that one negative operand is matched not to satisfy the rule. If the rule above is run against the following text: Last night more than forty Israeli soldiers invaded the city of Nablus and raided two homes looking for two young men. One of them was arrested during the raid and the other one avoided arrest as he was working at the time of the raid. At 2.30 am, Israeli soldiers broke into Mead Nijad's interrupting the family's sleep. As the soldiers entered the house, they ordered everyone to have their hands up; they asked for Emad, blindfolded, handcuffed and arrested him. the rule's condition is matched just once. In fact, the positive operand matches three tokens ( arrested , arrest in first sentence and arrest in the third sentence), but avoided (matched by !LEMMA(\"avoid\", \"try\", \"attempt\") ) immediately precedes the first occurrence of arrest and soldiers (matched by !ANCESTOR(47277) precedes the second occurrence from a distance.","title":"Left reference principle"},{"location":"operators/positional-sequences/negation/#right-reference-operators","text":"Normal sequence operators point in the forward direction. Right reference operators (single less-than sign, < , and double less-than sign, << ) point in the opposite direction. Right reference operators can be used interchangeably with normal operators, but when all of the rule's operands are positive there's no reason for doing so since the sequences built with them are perfectly equivalent to sequences built with normal operators and they are less clear. For example, conditions: LEMMA(\"confirm\") >> LEMMA(\"arrest\") and: LEMMA(\"confirm\") << LEMMA(\"arrest\") are perfectly equivalent, but the second contrasts with the general left-to-right reading direction which is common in the Rules language. Right reference operators, however, are useful in changing the normal relationship between negative operands and positive operands; in fact, they should be used only when negative operands are involved. Consider the following categorization rule, meant to give points to the dom1 domain whenever the text concerns the arrest of people. SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { TYPE(NPH) <> ANCESTOR(71230, 71231)// 71230: take in, seize, arrest, apprehend, cop, collar, nab, slough, nail, sneeze, pick up 71231: catch, capture, get, captive } } The rule's condition matches a person's name ( TYPE (NPH) ) followed, at any distance, but in the same sentence, by any concept that descends from syncon 71230 ( to arrest ) or syncon 71231 ( to capture ). If the rule is run against the following text: John McAfee, the multimillionaire software developer, has not been captured, despite a cryptic post on his own blog saying that he is in police custody. it is triggered by the John McAfee and captured sequence, but this is not what the users expects, because the text states that the man has not been captured . A more precise rule is triggered only if the verb is not negated. The following change in rule's condition: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { TYPE(NPH) <> !KEYWORD (\"has not been\", \"have not been\", \"was not\", \"wasn't\", \"were not\", \"weren't\") >> ANCESTOR(71230, 71231)// 71230: take in, seize, arrest, apprehend, cop, collar, nab, slough, nail, sneeze, pick up 71231: catch, capture, get, captive } } seems to achieve the desired effect. The second operand should match has not been before captured but, being a negated operand, it should prevent the whole condition from being satisfied. Indeed, the rule is not triggered (apparently a good result), but it is not due to the user's logic. If the modified rule is run against this text: McAfee was in the midst of recounting an incident earlier this month in which 42 armed Belizean officers allegedly stormed his compound, arrested him and detained him for 14 hours with no food or water, and then let him go without charges. it isn't triggered either and, again, this is not what the user was going for. Now we have one negative operand and two positive operands: how are they combined? The rule above is interpreted by the text intelligence engine as illustrated below. strictly followed by ANCESTOR(71230, 71231) \u2571 TYPE(NPH) \u2572 NOT flexibly followed by KEYWORD (\"has not been\", \"have not... This can be read in this way: a person's name strictly followed ( >> ) by any concept descending from syncon 71230 ( to arrest ) or syncon 71231 ( to capture ) and , at the same time, not flexibly followed ( <> ) by one of the keyword in the negated operand. This is because of the left reference principle: the first positive operand is both the left reference of the negative operator and of the second positive operand, therefore the negative operand between the positive operands is transparent when it comes to the relationship between the two positive operands: TYPE(NPH) >> ANCESTOR(71230, 71231) This \"positive\" condition doesn't hold true because arrested is too far from McAfee , so the whole condition is not met and the rule is not triggered. The negative operand would be evaluated like this: TYPE(NPH) <> !KEYWORD (\"has not been\", \"have not been\", \"was not\", ... and actually it holds true, but it's \"too late\". Sequence operators with a right reference come to the rescue! If the rule above is changed this way: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { TYPE(NPH) <> !KEYWORD (\"has not been\", \"have not been\", \"was not\", \"wasn't\", \"were not\", \"weren't\") << ANCESTOR(71230, 71231)// 71230: take in, seize, arrest, apprehend, cop, collar, nab, slough, nail, sneeze, pick up 71231: catch, capture, get, captive } } it is interpreted by the text intelligence engine as follows: TYPE(NPH) \u2572 flexibly followed by ANCESTOR(71230, 71231) \u2571 NOT strictly preceded by KEYWORD (\"has not been\", \"have not... The right reference operator takes the negative operand away from the first positive operand's \"attraction\" and ties it to the second positive operand which becomes the negative operand right reference . The first operator determines the combination between the first positive operand and the next positive operand, not the combination between the first positive and the next negative. Results will now be expected for the sample texts.","title":"Right reference operators"},{"location":"operators/positional-sequences/strict/","text":"Strict sequence The strict sequence operator (double greater-than sign, >> ) requires that the two tokens matched by the operands on its sides are positioned one immediately after the other and in the same sentence, only white space is allowed between them. The syntax is: operand1 >> operand2 Consider the following example: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { ANCESTOR(26151)// 26151: nervous system >> LEMMA(\"disease\", \"disorder\") } } The rule's condition matches any strict sequence of two tokens in which the first token is a concept descending from syncon 26151 ( nervous system ) and the next token has its lemma attribute set to disease or disorder . Consider the following sample text: Major new epidemiological analyses are focusing attention on disorders of the nervous system as important causes of death and disability around the world. One in every 9 individuals dies of a nervous system disorder. The rule is triggered just once because of the nervous system + disorder sequence found in the second sentence. disorders of the nervous system text in the first sentence does not trigger the rule: the first operand of the condition matches nervous system and the second matches disorders , but the overall condition is not met because tokens are not positioned in the \"right\" order. Besides simple attributes, the strict sequence operator can be used with set combinations of attributes as shown in the example below. SCOPE SENTENCE { IDENTIFY (MilitaryPersonnel) { @rank[ANCESTOR(46377, 47277)+ TYPE(NOU)]//46377:officer 47277: military personnel >> @name[ANCESTOR(78452)+ SYNCON (UNKNOWN)]// 78452: person } } This extraction rule is meant to extract proper names of military personnel along with their rank. The first operand attribute matches any concept descending from syncon 46377 ( officer ) or syncon 42277 ( military personnel ), but is limited to common nouns ( + TYPE(NOU) ), thus excluding any proper nouns like Dwight Eisenhower . The second operand matches any unknown proper name ( + SYNCON (UNKNOWN) ) recognized as the name of a person ( ANCESTOR(78452) ). If the rule is run against the following sample text: A year spent learning to lead troops in the land of the long white cloud has enriched Oakey lad Lieutenant James Martin with a wealth of life experience. it will be triggered by Lieutenant James Martin and it will fill the @rank field with Lieutenant and the @name field with James Martin . Right reference The strict sequence operator with right reference (double less-than sign, << ) is perfectly equivalent to the strict sequence operator unless it's combined with negative operands. To understand the effect of the right reference on sequence interpretation read the paragraphs under Right reference operators in the topic about negations in sequences .","title":"Strict sequence"},{"location":"operators/positional-sequences/strict/#strict-sequence","text":"The strict sequence operator (double greater-than sign, >> ) requires that the two tokens matched by the operands on its sides are positioned one immediately after the other and in the same sentence, only white space is allowed between them. The syntax is: operand1 >> operand2 Consider the following example: SCOPE SENTENCE { DOMAIN(dom1:NORMAL) { ANCESTOR(26151)// 26151: nervous system >> LEMMA(\"disease\", \"disorder\") } } The rule's condition matches any strict sequence of two tokens in which the first token is a concept descending from syncon 26151 ( nervous system ) and the next token has its lemma attribute set to disease or disorder . Consider the following sample text: Major new epidemiological analyses are focusing attention on disorders of the nervous system as important causes of death and disability around the world. One in every 9 individuals dies of a nervous system disorder. The rule is triggered just once because of the nervous system + disorder sequence found in the second sentence. disorders of the nervous system text in the first sentence does not trigger the rule: the first operand of the condition matches nervous system and the second matches disorders , but the overall condition is not met because tokens are not positioned in the \"right\" order. Besides simple attributes, the strict sequence operator can be used with set combinations of attributes as shown in the example below. SCOPE SENTENCE { IDENTIFY (MilitaryPersonnel) { @rank[ANCESTOR(46377, 47277)+ TYPE(NOU)]//46377:officer 47277: military personnel >> @name[ANCESTOR(78452)+ SYNCON (UNKNOWN)]// 78452: person } } This extraction rule is meant to extract proper names of military personnel along with their rank. The first operand attribute matches any concept descending from syncon 46377 ( officer ) or syncon 42277 ( military personnel ), but is limited to common nouns ( + TYPE(NOU) ), thus excluding any proper nouns like Dwight Eisenhower . The second operand matches any unknown proper name ( + SYNCON (UNKNOWN) ) recognized as the name of a person ( ANCESTOR(78452) ). If the rule is run against the following sample text: A year spent learning to lead troops in the land of the long white cloud has enriched Oakey lad Lieutenant James Martin with a wealth of life experience. it will be triggered by Lieutenant James Martin and it will fill the @rank field with Lieutenant and the @name field with James Martin .","title":"Strict sequence"},{"location":"operators/positional-sequences/strict/#right-reference","text":"The strict sequence operator with right reference (double less-than sign, << ) is perfectly equivalent to the strict sequence operator unless it's combined with negative operands. To understand the effect of the right reference on sequence interpretation read the paragraphs under Right reference operators in the topic about negations in sequences .","title":"Right reference"},{"location":"operators/prev/","text":"PREV operator PREV is an operator that combines the Boolean operator AND with the flexible positional sequence operator ( <> ). The syntax is: operand1 PREV operand2 PREV is a language keyword and must be written in uppercase. The operator requires two input text tokens to match the combined operands in the order in which they are declared in the rule, regardless of the distance between them, as long as both are within the scope of the rule. Consider the following extraction rule: SCOPE SENTENCE*2 { IDENTIFY(TEST) { @PHONE_NUMBER[TYPE(PHO)] PREV @NAME[TYPE(NPH)] } } The rule's condition matches a phone number ( TYPE(PHO) ) preceded by a person's name ( TYPE(NPH) ) in the scope of at most two antecedent sentences. If the rule is run against this text: Personal Data ------------ Name: Tom Smith Phone number: +1 123 456 7890 Date of birth: 21/10/1967 the condition is met by +1 123 456 7890 and Tom Smith , which occur in antecedent sentences, so the rule is triggered.","title":"PREV operator"},{"location":"operators/prev/#prev-operator","text":"PREV is an operator that combines the Boolean operator AND with the flexible positional sequence operator ( <> ). The syntax is: operand1 PREV operand2 PREV is a language keyword and must be written in uppercase. The operator requires two input text tokens to match the combined operands in the order in which they are declared in the rule, regardless of the distance between them, as long as both are within the scope of the rule. Consider the following extraction rule: SCOPE SENTENCE*2 { IDENTIFY(TEST) { @PHONE_NUMBER[TYPE(PHO)] PREV @NAME[TYPE(NPH)] } } The rule's condition matches a phone number ( TYPE(PHO) ) preceded by a person's name ( TYPE(NPH) ) in the scope of at most two antecedent sentences. If the rule is run against this text: Personal Data ------------ Name: Tom Smith Phone number: +1 123 456 7890 Date of birth: 21/10/1967 the condition is met by +1 123 456 7890 and Tom Smith , which occur in antecedent sentences, so the rule is triggered.","title":"PREV operator"},{"location":"operators/prev-not/","text":"PREV NOT operator PREV NOT is an operator that combines the Boolean operator AND NOT with the flexible positional sequence operator ( <> ). It allows the user to combine operands and verify their presence/absence and position in a portion of text made of multiple sentences. In other words, in a scope larger than a sentence, the operator verifies whether the first declared operand is not preceded by the second operand within the rule's scope. According to the scope scale, any number of sentences can be interposed between the operands. The syntax is: operand1 PREV NOT operand2 ... PREV NOT is a language keyword and must be written in uppercase. If two (or more) operands are defined in a rule, the PREV NOT operator will verify whether the first operand is present and not preceded by the second operand. Consider the following categorization rule: SCOPE SENTENCE*2 { DOMAIN(dom1) { ANCESTOR(78207)//@SYN: #78207# [actual constructions] <:4> LEMMA(\"comply\",\"compliance\") <:4> LEMMA(\"fire safety\") PREV NOT KEYWORD(\"code\") } } The condition matches a positional sequence of three attributes if not preceded by the keyword code within the rule's scope. If the rule is run against this text: The mill complies with the fire safety standard. However, further upgrade work will be required to avert fire risk. The manufacturing plant is in compliance with the fire safety standard. A building code compliance certificate will be issued. the condition is met by: manufacturing plant , third sentence = descendant of syncon 78207. compliance , third sentence = lemma compliance . fire safety , third sentence = lemma fire safety . Absence of keyword code in the second and in the third sentence.","title":"PREV NOT operator"},{"location":"operators/prev-not/#prev-not-operator","text":"PREV NOT is an operator that combines the Boolean operator AND NOT with the flexible positional sequence operator ( <> ). It allows the user to combine operands and verify their presence/absence and position in a portion of text made of multiple sentences. In other words, in a scope larger than a sentence, the operator verifies whether the first declared operand is not preceded by the second operand within the rule's scope. According to the scope scale, any number of sentences can be interposed between the operands. The syntax is: operand1 PREV NOT operand2 ... PREV NOT is a language keyword and must be written in uppercase. If two (or more) operands are defined in a rule, the PREV NOT operator will verify whether the first operand is present and not preceded by the second operand. Consider the following categorization rule: SCOPE SENTENCE*2 { DOMAIN(dom1) { ANCESTOR(78207)//@SYN: #78207# [actual constructions] <:4> LEMMA(\"comply\",\"compliance\") <:4> LEMMA(\"fire safety\") PREV NOT KEYWORD(\"code\") } } The condition matches a positional sequence of three attributes if not preceded by the keyword code within the rule's scope. If the rule is run against this text: The mill complies with the fire safety standard. However, further upgrade work will be required to avert fire risk. The manufacturing plant is in compliance with the fire safety standard. A building code compliance certificate will be issued. the condition is met by: manufacturing plant , third sentence = descendant of syncon 78207. compliance , third sentence = lemma compliance . fire safety , third sentence = lemma fire safety . Absence of keyword code in the second and in the third sentence.","title":"PREV NOT operator"},{"location":"options/","text":"OPTIONS The OPTIONS statement configures optional features of the text intelligence engine that are used to best match the characteristics of a given project. The statement can be placed in any source files, however, it is recommended to place it in the config.cr file. The syntax is: OPTIONS { option [option ...] } where option can be one of the followings: FIXED_SCORE STATIC_SCORE CHILD_TO_FATHER All three options above affect the categorization scoring mechanism . (Advanced use) EXPAND_SYNCON_CACHE=false : disables the cache used to store the expansion of ancestor syncons. The evaluation of every ANCESTOR attribute requires the creation of an in-memory list of descendant syncons. By default, this list is created and cached at compile time, then used at run-time. When this option is set, the list is created at run-time and only when really needed. (Advanced use) EXPAND_SYNCON_LIMIT=value : the value in this option is a positive integer number (default: 2000). It influences the creation of the list of descendant syncons when ANCESTOR attributes are evaluated (see the option above). If the number of descendants exceeds this limit, the cache will not be created at compile time.","title":"OPTIONS"},{"location":"options/#options","text":"The OPTIONS statement configures optional features of the text intelligence engine that are used to best match the characteristics of a given project. The statement can be placed in any source files, however, it is recommended to place it in the config.cr file. The syntax is: OPTIONS { option [option ...] } where option can be one of the followings: FIXED_SCORE STATIC_SCORE CHILD_TO_FATHER All three options above affect the categorization scoring mechanism . (Advanced use) EXPAND_SYNCON_CACHE=false : disables the cache used to store the expansion of ancestor syncons. The evaluation of every ANCESTOR attribute requires the creation of an in-memory list of descendant syncons. By default, this list is created and cached at compile time, then used at run-time. When this option is set, the list is created at run-time and only when really needed. (Advanced use) EXPAND_SYNCON_LIMIT=value : the value in this option is a positive integer number (default: 2000). It influences the creation of the list of descendant syncons when ANCESTOR attributes are evaluated (see the option above). If the number of descendants exceeds this limit, the cache will not be created at compile time.","title":"OPTIONS"},{"location":"scope/","text":"SCOPE overview When defining rules, SCOPE is the statement that allows the user to define the portion of text in which a single rule or a group of rules has to be instantiated and verified. A rule will generate a hit, only if it recognizes all the elements it contains within the defined interval. The syntax to define a rule's scope is the following: SCOPE scope_option { rules } Defining a scope is mandatory for each rule, but it is up to the user to decide the extent. For greater flexibility, users can choose from a variety of scope options. The options can be divided into two groups: the first relates to the standard textual divisions generated by the semantic disambiguator's text analysis; the second to the textual divisions that can be optionally defined for a specific project and/or text type. Standard options: PARAGRAPH SENTENCE CLAUSE PHRASE Custom options: SECTION SEGMENT The different types of standard and custom SCOPE options can be used by themselves or in combination, depending on the rules' goals. Within the same project, rules can share the same scope definition; however, this is not mandatory as a different SCOPE can be defined for each rule in the project. The value of selecting a wider or narrower scope option can be better appreciated when the rule(s) to be managed contain(s) expressions made of two or more attributes connected by boolean operators. If a rule looks for a single element, the results obtained applying the rule on paragraphs or on single sentences will be the same. However there is a significant difference between a rule that looks for two lemmas using the AND operator within a single sentence as opposed to within two paragraphs. In fact, if the SCOPE is SENTENCE , a rule could trigger only within each single sentence; it will never recognize elements which are contained in two different sentences, not even if they are adjacent. On the other hand, if the SCOPE is PARAGRAPH , a rule could trigger on all the text contained within a paragraph, thus going beyond a single sentence scope. SCOPE options can sometimes be used in combination with particular features that further restrict rules' actions to specific conditions; these conditions neither depend on the text's structure, nor on the position of the textual elements, but on the content and meaning of the text. These features are called: DOMAIN constraints and SENTENCE RELEVANT constraints. The first verifies whether a document has been associated to any domain during the disambiguation process, the second acts upon the most relevant sentences of a document. In other words, these two syntaxes take into consideration the context in which words occur. For a detailed description, please refer to the dedicated pages.","title":"SCOPE overview"},{"location":"scope/#scope-overview","text":"When defining rules, SCOPE is the statement that allows the user to define the portion of text in which a single rule or a group of rules has to be instantiated and verified. A rule will generate a hit, only if it recognizes all the elements it contains within the defined interval. The syntax to define a rule's scope is the following: SCOPE scope_option { rules } Defining a scope is mandatory for each rule, but it is up to the user to decide the extent. For greater flexibility, users can choose from a variety of scope options. The options can be divided into two groups: the first relates to the standard textual divisions generated by the semantic disambiguator's text analysis; the second to the textual divisions that can be optionally defined for a specific project and/or text type. Standard options: PARAGRAPH SENTENCE CLAUSE PHRASE Custom options: SECTION SEGMENT The different types of standard and custom SCOPE options can be used by themselves or in combination, depending on the rules' goals. Within the same project, rules can share the same scope definition; however, this is not mandatory as a different SCOPE can be defined for each rule in the project. The value of selecting a wider or narrower scope option can be better appreciated when the rule(s) to be managed contain(s) expressions made of two or more attributes connected by boolean operators. If a rule looks for a single element, the results obtained applying the rule on paragraphs or on single sentences will be the same. However there is a significant difference between a rule that looks for two lemmas using the AND operator within a single sentence as opposed to within two paragraphs. In fact, if the SCOPE is SENTENCE , a rule could trigger only within each single sentence; it will never recognize elements which are contained in two different sentences, not even if they are adjacent. On the other hand, if the SCOPE is PARAGRAPH , a rule could trigger on all the text contained within a paragraph, thus going beyond a single sentence scope. SCOPE options can sometimes be used in combination with particular features that further restrict rules' actions to specific conditions; these conditions neither depend on the text's structure, nor on the position of the textual elements, but on the content and meaning of the text. These features are called: DOMAIN constraints and SENTENCE RELEVANT constraints. The first verifies whether a document has been associated to any domain during the disambiguation process, the second acts upon the most relevant sentences of a document. In other words, these two syntaxes take into consideration the context in which words occur. For a detailed description, please refer to the dedicated pages.","title":"SCOPE overview"},{"location":"scope/custom-options/","text":"SCOPE custom options Introduction The custom scope options are those portions of a text that relate to the textual subdivisions that can be optionally defined by the user for a specific project and/or text type, they are used to delimit the area of action of a rule or a group of rules. There are two custom scope options and they are listed below: SECTION SEGMENT These can be used alone or they can be combined, either with each other, or with the standard scope options. SECTION SECTION is one of the two custom textual subdivisions that can be defined. The SECTION scope option can be used only if the input texts have section annotations. The names of such SECTION tags can be used to define the scope of both categorization and extraction rules, provided that these names have been declared beforehand. By selecting this scope option, a rule is required to apply to a text block previously tagged as a section. The syntax for the scope option SECTION is the following: SCOPE SECTION (section_name) { rules } Every input document can contain a single instance for each SECTION tag. Every project has a default standard section named BODY . All documents without section annotations are automatically treated as if they are entirely contained in the standard section BODY . In fact, it is possible to write rules to act upon this section with no need for input text pre-processing or section name declaration. For example: SCOPE SECTION (BODY) { //rule or list of rules } would act upon the text block previously defined as the section named BODY . When using the SECTION scope option, it is possible to select a single section name or a list of them. For example, if we consider a newspaper article, where the TITLE , LEAD and BODY parts of the text are annotated as sections, the following rule: SCOPE SECTION (TITLE, LEAD) { ///rule or list of rules } will act upon any text block contained in the TITLE and LEAD sections, thus not considering the text contained in the BODY section. Such a SCOPE definition would, for example, give priority to concepts mentioned in the title (presumably the main topic of the article) and ignore any correlated but secondary topics mentioned in the body of the article. SEGMENT SEGMENT is one of the two custom textual subdivisions that can be defined by means of specific semantic rules designed for segments creation. The names of segments can be used to define the scope of both categorization and extraction rules, provided that these names have been declared beforehand in the configuration file. By selecting this scope option, a rule is required to act upon a text block previously recognized as a segment. The syntax for the scope option SEGMENT is the following: SCOPE SEGMENT (segment_name) { rules } In comparison to sections, for which only a single instance per document is allowed, segments can be instantiated several times in a single input document. Every project has two predefined segments named SEGMENT1 and SEGMENT2 . It is possible to write rules to be instantiated on these segments with no need to declare the segment in the Section/Segment definition panel. For example: SCOPE SEGMENT (SEGMENT1) { //rule or list of rules } would act upon the text block previously defined as the segment named SEGMENT1 . When using the SEGMENT scope option, it is possible to select a single segment name or a list of them. For example, if we consider a letter or an e-mail, where it is possible to recognize text subdivisions regarding the sender and receiver, and these are contained in as many segments, the following lines: SCOPE SEGMENT (SENDER, RECEIVER) { //rule or list of rules } will act only upon specific text blocks identified as the SENDER and RECEIVER segments, thus ignoring the information contained in the body of the letter. Such a SCOPE definition could allow a user to find just the personal information of the sender and/or the receiver of the correspondence and exclude information regarding any third party mentioned in the body of the message.","title":"SCOPE custom options"},{"location":"scope/custom-options/#scope-custom-options","text":"","title":"SCOPE custom options"},{"location":"scope/custom-options/#introduction","text":"The custom scope options are those portions of a text that relate to the textual subdivisions that can be optionally defined by the user for a specific project and/or text type, they are used to delimit the area of action of a rule or a group of rules. There are two custom scope options and they are listed below: SECTION SEGMENT These can be used alone or they can be combined, either with each other, or with the standard scope options.","title":"Introduction"},{"location":"scope/custom-options/#section","text":"SECTION is one of the two custom textual subdivisions that can be defined. The SECTION scope option can be used only if the input texts have section annotations. The names of such SECTION tags can be used to define the scope of both categorization and extraction rules, provided that these names have been declared beforehand. By selecting this scope option, a rule is required to apply to a text block previously tagged as a section. The syntax for the scope option SECTION is the following: SCOPE SECTION (section_name) { rules } Every input document can contain a single instance for each SECTION tag. Every project has a default standard section named BODY . All documents without section annotations are automatically treated as if they are entirely contained in the standard section BODY . In fact, it is possible to write rules to act upon this section with no need for input text pre-processing or section name declaration. For example: SCOPE SECTION (BODY) { //rule or list of rules } would act upon the text block previously defined as the section named BODY . When using the SECTION scope option, it is possible to select a single section name or a list of them. For example, if we consider a newspaper article, where the TITLE , LEAD and BODY parts of the text are annotated as sections, the following rule: SCOPE SECTION (TITLE, LEAD) { ///rule or list of rules } will act upon any text block contained in the TITLE and LEAD sections, thus not considering the text contained in the BODY section. Such a SCOPE definition would, for example, give priority to concepts mentioned in the title (presumably the main topic of the article) and ignore any correlated but secondary topics mentioned in the body of the article.","title":"SECTION"},{"location":"scope/custom-options/#segment","text":"SEGMENT is one of the two custom textual subdivisions that can be defined by means of specific semantic rules designed for segments creation. The names of segments can be used to define the scope of both categorization and extraction rules, provided that these names have been declared beforehand in the configuration file. By selecting this scope option, a rule is required to act upon a text block previously recognized as a segment. The syntax for the scope option SEGMENT is the following: SCOPE SEGMENT (segment_name) { rules } In comparison to sections, for which only a single instance per document is allowed, segments can be instantiated several times in a single input document. Every project has two predefined segments named SEGMENT1 and SEGMENT2 . It is possible to write rules to be instantiated on these segments with no need to declare the segment in the Section/Segment definition panel. For example: SCOPE SEGMENT (SEGMENT1) { //rule or list of rules } would act upon the text block previously defined as the segment named SEGMENT1 . When using the SEGMENT scope option, it is possible to select a single segment name or a list of them. For example, if we consider a letter or an e-mail, where it is possible to recognize text subdivisions regarding the sender and receiver, and these are contained in as many segments, the following lines: SCOPE SEGMENT (SENDER, RECEIVER) { //rule or list of rules } will act only upon specific text blocks identified as the SENDER and RECEIVER segments, thus ignoring the information contained in the body of the letter. Such a SCOPE definition could allow a user to find just the personal information of the sender and/or the receiver of the correspondence and exclude information regarding any third party mentioned in the body of the message.","title":"SEGMENT"},{"location":"scope/domain-constraints/","text":"SCOPE with domain constraints Introduction When a text is analyzed, in addition to providing a four-level analysis of the document, the semantic disambiguator also automatically identifies the Knowledge Graph domains of the text based on the words used and their meaning. These domains correspond to a closed list of standard domains and are available each time a text is analyzed. The standard domains can be used to define the scope of both categorization and extraction rules, in order to activate or inhibit a rule if a specific domain has been associated to the entire document. In other words, there are specific options that take into consideration the context in which words occur when deciding whether a rule should be activated or inhibited. The syntax is: SCOPE scope_option domain_constraint (knowledge_graph_domain[:threshold]) { rules } where scope_option is one of the standard or custom scope options, knowledge_graph_domain is the name of a Knowledge Graph domain of choice and domain_constraint can be one the followings: IF DOMAIN IF NOT DOMAIN IF RELEVANT DOMAIN IF NOT IN SEGMENT threshold corresponds to either an integer or a decimal percentage. It refers to the percentage that was assigned by the disambiguator when the document was analyzed. If threshold is specified, the rule will be activated only if the document has been associated to the knowledge_graph_domain with a score that is equal to\u2014or greater than\u2014the threshold . IF DOMAIN IF DOMAIN is the constraint that allows enabling a rule only if a specific domain has been associated to the entire input document. Consider for example the following categorization rule: SCOPE SENTENCE IF DOMAIN (football) { DOMAIN(dom1:NORMAL) { SYNCON(305045) // Champions League } } This will be activated and match the concept of Champions League , only if the domain football has been associated to the document during the text disambiguation process. If the rule is modified as follows: SCOPE SENTENCE IF DOMAIN (game:2.5%) { DOMAIN(dom1:NORMAL) { SYNCON(305045) // Champions League } } the rule will be restricted only to those cases in which the document has been associated to the domain game by at least 2.5%. IF NOT DOMAIN IF NOT DOMAIN is the constraint that inhibits a rule, if a specific domain has been associated to the input document. Consider for example the following extraction rule: SCOPE SENTENCE IF NOT DOMAIN (military) { IDENTIFY(Template1) { @Field1[LEMMA(\"scout\")] } } It will be activated and extract the lemma scout , only if the domain military has not been associated to the document during the text disambiguation process. If the rule is modified as follows: SCOPE SENTENCE IF NOT DOMAIN(military:5%) { IDENTIFY(Template1) { @Field1[LEMMA(\"scout\")] } } it will be restricted only to those cases in which a document has been associated to the domain military by at least 5%. IF RELEVANT DOMAIN IF RELEVANT DOMAIN is the constraint enabling a rule only if a specific domain has been associated to the input document and this domain belongs to the relevant information identified for that document. Consider for example the following categorization rule: SCOPE SENTENCE IF RELEVANT DOMAIN (football) { DOMAIN(dom1:NORMAL) { SYNCON(317175) // FC Barcelona } } This will be activated and will match the concept of FC Barcelona only if the domain football has been associated to the input document during the text disambiguation process and this domain is part of the relevant information for that document. The difference between this constraint and the simple IF DOMAIN constraint is the set of domains which is considered: IF DOMAIN considers all domains associated to a document (even those with an extremely low score), whereas IF RELEVANT DOMAIN considers only those domains that have been evaluated as the most representative for a document. If the rule above is modified as follows: SCOPE SENTENCE IF RELEVANT DOMAIN (football:10%) { DOMAIN(dom1:NORMAL) { SYNCON(317175) // FC Barcelona } } it is restricted only to those cases in which a the document has been associated to the domain football with a 10% score at least.","title":"SCOPE with domain constraints"},{"location":"scope/domain-constraints/#scope-with-domain-constraints","text":"","title":"SCOPE with domain constraints"},{"location":"scope/domain-constraints/#introduction","text":"When a text is analyzed, in addition to providing a four-level analysis of the document, the semantic disambiguator also automatically identifies the Knowledge Graph domains of the text based on the words used and their meaning. These domains correspond to a closed list of standard domains and are available each time a text is analyzed. The standard domains can be used to define the scope of both categorization and extraction rules, in order to activate or inhibit a rule if a specific domain has been associated to the entire document. In other words, there are specific options that take into consideration the context in which words occur when deciding whether a rule should be activated or inhibited. The syntax is: SCOPE scope_option domain_constraint (knowledge_graph_domain[:threshold]) { rules } where scope_option is one of the standard or custom scope options, knowledge_graph_domain is the name of a Knowledge Graph domain of choice and domain_constraint can be one the followings: IF DOMAIN IF NOT DOMAIN IF RELEVANT DOMAIN IF NOT IN SEGMENT threshold corresponds to either an integer or a decimal percentage. It refers to the percentage that was assigned by the disambiguator when the document was analyzed. If threshold is specified, the rule will be activated only if the document has been associated to the knowledge_graph_domain with a score that is equal to\u2014or greater than\u2014the threshold .","title":"Introduction"},{"location":"scope/domain-constraints/#if-domain","text":"IF DOMAIN is the constraint that allows enabling a rule only if a specific domain has been associated to the entire input document. Consider for example the following categorization rule: SCOPE SENTENCE IF DOMAIN (football) { DOMAIN(dom1:NORMAL) { SYNCON(305045) // Champions League } } This will be activated and match the concept of Champions League , only if the domain football has been associated to the document during the text disambiguation process. If the rule is modified as follows: SCOPE SENTENCE IF DOMAIN (game:2.5%) { DOMAIN(dom1:NORMAL) { SYNCON(305045) // Champions League } } the rule will be restricted only to those cases in which the document has been associated to the domain game by at least 2.5%.","title":"IF DOMAIN"},{"location":"scope/domain-constraints/#if-not-domain","text":"IF NOT DOMAIN is the constraint that inhibits a rule, if a specific domain has been associated to the input document. Consider for example the following extraction rule: SCOPE SENTENCE IF NOT DOMAIN (military) { IDENTIFY(Template1) { @Field1[LEMMA(\"scout\")] } } It will be activated and extract the lemma scout , only if the domain military has not been associated to the document during the text disambiguation process. If the rule is modified as follows: SCOPE SENTENCE IF NOT DOMAIN(military:5%) { IDENTIFY(Template1) { @Field1[LEMMA(\"scout\")] } } it will be restricted only to those cases in which a document has been associated to the domain military by at least 5%.","title":"IF NOT DOMAIN"},{"location":"scope/domain-constraints/#if-relevant-domain","text":"IF RELEVANT DOMAIN is the constraint enabling a rule only if a specific domain has been associated to the input document and this domain belongs to the relevant information identified for that document. Consider for example the following categorization rule: SCOPE SENTENCE IF RELEVANT DOMAIN (football) { DOMAIN(dom1:NORMAL) { SYNCON(317175) // FC Barcelona } } This will be activated and will match the concept of FC Barcelona only if the domain football has been associated to the input document during the text disambiguation process and this domain is part of the relevant information for that document. The difference between this constraint and the simple IF DOMAIN constraint is the set of domains which is considered: IF DOMAIN considers all domains associated to a document (even those with an extremely low score), whereas IF RELEVANT DOMAIN considers only those domains that have been evaluated as the most representative for a document. If the rule above is modified as follows: SCOPE SENTENCE IF RELEVANT DOMAIN (football:10%) { DOMAIN(dom1:NORMAL) { SYNCON(317175) // FC Barcelona } } it is restricted only to those cases in which a the document has been associated to the domain football with a 10% score at least.","title":"IF RELEVANT DOMAIN"},{"location":"scope/options-combination/","text":"Combinations of standard and custom scope options Standard and custom options can be combined when setting a rule scope. In particular, it is possible to select a standard option type to be included within a custom scope option. By selecting this option, a rule is required to act upon a paragraph, sentence, clause or phrase that is contained in a given section or segment. The syntax for combining standard and custom scope options is the following: SCOPE standard_option (option_type) IN custom_option (name) { //rule or list of rules// } where: standard_option corresponds to one of the available options; custom_option corresponds to one of the available options; option_type corresponds to one of the types available for the standard options (if any); name corresponds to the name of one the sections or segments defined for a specific project. For example, if we consider a newspaper article, where the TITLE, LEAD and BODY of the text are annotated as SECTIONS , the following lines: SCOPE CLAUSE (INDEPENDENT) IN SECTION (LEAD) { //rule or list of rules// } will act upon a rule on a text block recognized as an independent clause within the section containing the lead paragraph. It is also possible to use a combination of PHRASE and CLAUSE scope options together with one of the custom scope options. The syntax for defining such a scope is the following: SCOPE PHRASE IN CLAUSE (clause_type) IN custom_option (name) { //rule or list of rules// } For example, the following definition: SCOPE PHRASE IN CLAUSE (INDEPENDENT) IN SECTION (LEAD) { //rule or list of rules// } will act upon a rule on a text block recognized as a noun phrase, contained in an independent clause, which is in turn included in the section containing the lead paragraph. The use of such complex combinations aims to identify a very precise and limited area for the rule to act upon; in fact, the hits generated by rules with this kind of scope are more likely to be characterized by high precision rather than high recall. IF NOT IN SEGMENT constraint IF NOT IN SEGMENT is the constraint that allows the user to prevent a rule if the conditon expressed occurs within a specific segment. The syntax for defining such a scope is the following: SCOPE standard_option IF NOT IN SEGMENT (segment_name) { //rule or list of rules// } For example, the following definition: SCOPE SENTENCE IF NOT IN SEGMENT (INSTRUCTION) { //rule or list of rules// } will act upon a rule on a text block composed of one sentence, but only if the sentence does not occur within the specified segment.","title":"Combinations of standard and custom scope options"},{"location":"scope/options-combination/#combinations-of-standard-and-custom-scope-options","text":"Standard and custom options can be combined when setting a rule scope. In particular, it is possible to select a standard option type to be included within a custom scope option. By selecting this option, a rule is required to act upon a paragraph, sentence, clause or phrase that is contained in a given section or segment. The syntax for combining standard and custom scope options is the following: SCOPE standard_option (option_type) IN custom_option (name) { //rule or list of rules// } where: standard_option corresponds to one of the available options; custom_option corresponds to one of the available options; option_type corresponds to one of the types available for the standard options (if any); name corresponds to the name of one the sections or segments defined for a specific project. For example, if we consider a newspaper article, where the TITLE, LEAD and BODY of the text are annotated as SECTIONS , the following lines: SCOPE CLAUSE (INDEPENDENT) IN SECTION (LEAD) { //rule or list of rules// } will act upon a rule on a text block recognized as an independent clause within the section containing the lead paragraph. It is also possible to use a combination of PHRASE and CLAUSE scope options together with one of the custom scope options. The syntax for defining such a scope is the following: SCOPE PHRASE IN CLAUSE (clause_type) IN custom_option (name) { //rule or list of rules// } For example, the following definition: SCOPE PHRASE IN CLAUSE (INDEPENDENT) IN SECTION (LEAD) { //rule or list of rules// } will act upon a rule on a text block recognized as a noun phrase, contained in an independent clause, which is in turn included in the section containing the lead paragraph. The use of such complex combinations aims to identify a very precise and limited area for the rule to act upon; in fact, the hits generated by rules with this kind of scope are more likely to be characterized by high precision rather than high recall.","title":"Combinations of standard and custom scope options"},{"location":"scope/options-combination/#if-not-in-segment-constraint","text":"IF NOT IN SEGMENT is the constraint that allows the user to prevent a rule if the conditon expressed occurs within a specific segment. The syntax for defining such a scope is the following: SCOPE standard_option IF NOT IN SEGMENT (segment_name) { //rule or list of rules// } For example, the following definition: SCOPE SENTENCE IF NOT IN SEGMENT (INSTRUCTION) { //rule or list of rules// } will act upon a rule on a text block composed of one sentence, but only if the sentence does not occur within the specified segment.","title":"IF NOT IN SEGMENT constraint"},{"location":"scope/options-intersection/","text":"Intersection of scope options SECTION and SEGMENT scope options can be used in advanced combinations that imply the intersection of a section and one or more segments, or the intersection of two or more segments. The syntax for enabling the first kind of combination is the following: SCOPE SECTION (section_name:segment_name) { //rule or list of rules// } When the SECTION scope option is used by itself, the input documents must contain SECTION annotations. For example, if we consider a newspaper article, where the TITLE, LEAD and BODY of the text are annotated as SECTION , and a segment named BYLINE is created to intercept information about the article's author, the following rule: SCOPE SECTION (BODY:BYLINE) { //rule or list of rules// } will act upon a text block contained in a particular \"region\" of the document resulting from the intersection of the section BODY and the segment BYLINE , thus allowing very specific matches on selected portions of a text. The syntax for enabling the second kind of combination is the following: SCOPE SEGMENT (segment_name:segment_name) { //rule or list of rules// } Scope intersection is also possible among two or more segments. For example, if we consider a letter or e-mail, where it is possible to recognize text subdivision containing the SENDER_NAME and the RECEIVER_NAME , and other parts containing ADDRESSES ; if these are contained in as many SEGMENTS , the following code: SCOPE SEGMENT (SENDER:ADDRESSES, RECEIVER:ADDRESSES) { //rule or list of rules// } will act upon the rule on text blocks contained in particular \"regions\" of the document resulting from the intersection of the segment SENDER with the segment ADDRESSES , and the segment RECEIVER with the segment ADDRESSES , thus recognizing, by proximity, the addresses respectively belonging to the sender and the receiver of the letter.","title":"Intersection of scope options"},{"location":"scope/options-intersection/#intersection-of-scope-options","text":"SECTION and SEGMENT scope options can be used in advanced combinations that imply the intersection of a section and one or more segments, or the intersection of two or more segments. The syntax for enabling the first kind of combination is the following: SCOPE SECTION (section_name:segment_name) { //rule or list of rules// } When the SECTION scope option is used by itself, the input documents must contain SECTION annotations. For example, if we consider a newspaper article, where the TITLE, LEAD and BODY of the text are annotated as SECTION , and a segment named BYLINE is created to intercept information about the article's author, the following rule: SCOPE SECTION (BODY:BYLINE) { //rule or list of rules// } will act upon a text block contained in a particular \"region\" of the document resulting from the intersection of the section BODY and the segment BYLINE , thus allowing very specific matches on selected portions of a text. The syntax for enabling the second kind of combination is the following: SCOPE SEGMENT (segment_name:segment_name) { //rule or list of rules// } Scope intersection is also possible among two or more segments. For example, if we consider a letter or e-mail, where it is possible to recognize text subdivision containing the SENDER_NAME and the RECEIVER_NAME , and other parts containing ADDRESSES ; if these are contained in as many SEGMENTS , the following code: SCOPE SEGMENT (SENDER:ADDRESSES, RECEIVER:ADDRESSES) { //rule or list of rules// } will act upon the rule on text blocks contained in particular \"regions\" of the document resulting from the intersection of the segment SENDER with the segment ADDRESSES , and the segment RECEIVER with the segment ADDRESSES , thus recognizing, by proximity, the addresses respectively belonging to the sender and the receiver of the letter.","title":"Intersection of scope options"},{"location":"scope/sentence-relevant/","text":"SCOPE with SENTENCE RELEVANT syntax Introduction During the disambiguation process, the disambiguator is able to identify the most relevant elements in a document: keywords, lemmas, syncons, domains, and sentences. This information can be used in a variety of ways to enhance the accuracy of categorization and extraction rules. In particular, the three most relevant sentences identified in each document can become a valid option or constraint for the SCOPE definition. In fact, the SENTENCE RELEVANT syntax can be used either alone as a proper SCOPE option, or in combination with all types of standard and custom scope options. SENTENCE RELEVANT scope option The syntax to use a proper SCOPE option is the following: SCOPE SENTENCE RELEVANT { rules } Any rule with such a scope definition will be activated only if the element(s) specified in the rule itself are found within at least one of the three relevant sentences of a document. The relevant sentences identified in a document are also assigned a percentage, which indicates their relevance compared to the rest of the sentences in the document. This value can also be used to furtherly restrict the SCOPE definition. SCOPE SENTENCE RELEVANT:threshold { rules } where threshold corresponds to either an integer or a decimal indicating the percent threshold to be considered. For example, using the following scope definition: SCOPE SENTENCE RELEVANT:10% { //rule or list of rules } the rule is restricted only to those cases when the element(s) specified in the rule itself are found within one of the three relevant sentences of a document having a relevance value of at least 10%. SENTENCE RELEVANT scope constraint The SENTENCE RELEVANT syntax can also be used in combination with other scope options to furtherly restrict the scope itself. Two cases are possible: The scope option defines a portion of text which is smaller than a sentence; therefore the relevant sentence contains the selected scope, which can be either PHRASE or CLAUSE . The scope option defines a portion of text which is larger than a sentence; therefore the relevant sentence is contained in the selected scope, which can be PARAGRAPH , SEGMENT or SECTION . Each of the two cases uses the SENTENCE RELEVANT syntax in a different way. When the scope is smaller, the syntax is implemented as follows: SCOPE scope_option IN SENTENCE RELEVANT { rules } When the scope is larger, the syntax is implemented as follows: SCOPE scope_option WITH SENTENCE RELEVANT { rules } where scope_option refers to the options mentioned above for each case. The relevant sentences identified in a document are also assigned percentage, which indicates their relevance compared to the rest of the sentences in the document. This value can be used to furtherly restrict the SCOPE definition. The syntax is the following: SCOPE scope_option IN SENTENCE RELEVANT:threshold { rules } SCOPE scope_option WITH SENTENCE RELEVANT:threshold { rules } where threshold corresponds to either an integer or a decimal indicating the percent threshold to be considered. A few examples for the different cases are listed below: Smaller scope option: SCOPE PHRASE IN SENTENCE RELEVANT SCOPE CLAUSE IN SENTENCE RELEVANT:17% SCOPE PHRASE (NP) IN SENTENCE RELEVANT:10% SCOPE CLAUSE (INDEPENDENT) IN SENTENCE RELEVANT SCOPE PHRASE IN CLAUSE(SUBORDINATE) IN SENTENCE RELEVANT:22% SCOPE PHRASE (NP, NP/PP) IN CLAUSE (INDEPENDENT) IN SENTENCE RELEVANT:10% Larger scope option: SCOPE PARAGRAPH WITH SENTENCE RELEVANT:2% SCOPE SECTION(TITLE, BODY) WITH SENTENCE RELEVANT SCOPE SEGMENT(SENDER) WITH SENTENCE RELEVANT:10%","title":"SCOPE with SENTENCE RELEVANT syntax"},{"location":"scope/sentence-relevant/#scope-with-sentence-relevant-syntax","text":"","title":"SCOPE with SENTENCE RELEVANT syntax"},{"location":"scope/sentence-relevant/#introduction","text":"During the disambiguation process, the disambiguator is able to identify the most relevant elements in a document: keywords, lemmas, syncons, domains, and sentences. This information can be used in a variety of ways to enhance the accuracy of categorization and extraction rules. In particular, the three most relevant sentences identified in each document can become a valid option or constraint for the SCOPE definition. In fact, the SENTENCE RELEVANT syntax can be used either alone as a proper SCOPE option, or in combination with all types of standard and custom scope options.","title":"Introduction"},{"location":"scope/sentence-relevant/#sentence-relevant-scope-option","text":"The syntax to use a proper SCOPE option is the following: SCOPE SENTENCE RELEVANT { rules } Any rule with such a scope definition will be activated only if the element(s) specified in the rule itself are found within at least one of the three relevant sentences of a document. The relevant sentences identified in a document are also assigned a percentage, which indicates their relevance compared to the rest of the sentences in the document. This value can also be used to furtherly restrict the SCOPE definition. SCOPE SENTENCE RELEVANT:threshold { rules } where threshold corresponds to either an integer or a decimal indicating the percent threshold to be considered. For example, using the following scope definition: SCOPE SENTENCE RELEVANT:10% { //rule or list of rules } the rule is restricted only to those cases when the element(s) specified in the rule itself are found within one of the three relevant sentences of a document having a relevance value of at least 10%.","title":"SENTENCE RELEVANT scope option"},{"location":"scope/sentence-relevant/#sentence-relevant-scope-constraint","text":"The SENTENCE RELEVANT syntax can also be used in combination with other scope options to furtherly restrict the scope itself. Two cases are possible: The scope option defines a portion of text which is smaller than a sentence; therefore the relevant sentence contains the selected scope, which can be either PHRASE or CLAUSE . The scope option defines a portion of text which is larger than a sentence; therefore the relevant sentence is contained in the selected scope, which can be PARAGRAPH , SEGMENT or SECTION . Each of the two cases uses the SENTENCE RELEVANT syntax in a different way. When the scope is smaller, the syntax is implemented as follows: SCOPE scope_option IN SENTENCE RELEVANT { rules } When the scope is larger, the syntax is implemented as follows: SCOPE scope_option WITH SENTENCE RELEVANT { rules } where scope_option refers to the options mentioned above for each case. The relevant sentences identified in a document are also assigned percentage, which indicates their relevance compared to the rest of the sentences in the document. This value can be used to furtherly restrict the SCOPE definition. The syntax is the following: SCOPE scope_option IN SENTENCE RELEVANT:threshold { rules } SCOPE scope_option WITH SENTENCE RELEVANT:threshold { rules } where threshold corresponds to either an integer or a decimal indicating the percent threshold to be considered. A few examples for the different cases are listed below: Smaller scope option: SCOPE PHRASE IN SENTENCE RELEVANT SCOPE CLAUSE IN SENTENCE RELEVANT:17% SCOPE PHRASE (NP) IN SENTENCE RELEVANT:10% SCOPE CLAUSE (INDEPENDENT) IN SENTENCE RELEVANT SCOPE PHRASE IN CLAUSE(SUBORDINATE) IN SENTENCE RELEVANT:22% SCOPE PHRASE (NP, NP/PP) IN CLAUSE (INDEPENDENT) IN SENTENCE RELEVANT:10% Larger scope option: SCOPE PARAGRAPH WITH SENTENCE RELEVANT:2% SCOPE SECTION(TITLE, BODY) WITH SENTENCE RELEVANT SCOPE SEGMENT(SENDER) WITH SENTENCE RELEVANT:10%","title":"SENTENCE RELEVANT scope constraint"},{"location":"scope/standard-options/","text":"SCOPE standard options Introduction The standard scope options are those portions of a text that relate to the textual subdivisions generated during the semantic disambiguation process, and they are used to delimit the area of action of a rule or group of rules. There are four standard options listed below in order of extent (from the widest to the narrowest): PARAGRAPH SENTENCE CLAUSE PHRASE PARAGRAPH scope PARAGRAPH is the broadest among the standard textual subdivisions. It is a unit of discourse consisting of one or more sentences dealing with a particular concept. Its start is typically indicated by the beginning of a new line. By selecting this option, a rule will be applied to every paragraph that has been recognized in the entire input document. The syntax for the scope option PARAGRAPH is the following: SCOPE PARAGRAPH { rules } It is also possible to operate on multiple paragraphs using the following syntax: SCOPE PARAGRAPH*n. { rules } Where the asterisk ( * ) is a multiplier and n is a number indicating how many adjoining paragraphs the rule will act upon. For example: SCOPE PARAGRAPH*2 { //rule or list of rules } the rule will act upon text blocks, each made of two paragraphs: the first and the second paragraph of the document, the second and the third, the third and the fourth, and so on. SENTENCE scope SENTENCE is one of the standard textual subdivisions. It consists of one or several words, linked to each other by a syntactic relation, which are able to convey meaning. The beginning of a sentence usually follows a punctuation mark such as a period, question mark or exclamation mark. By selecting this option, a rule will act upon each sentence that has been recognized in the whole input document. The syntax for the scope option SENTENCE is the following: SCOPE SENTENCE { rules } It is also possible to operate on multiple sentences by using the following syntax: SCOPE SENTENCE*n. { rules } Where the asterisk ( * ) is a multiplier and n is a number indicating how many adjoining sentences the rule will act upon. For example: SCOPE SENTENCE*3 { //rule or list of rules// } the rule would act upon text blocks made of three sentences: the first, the second and the third sentence of the text; the second, the third and the fourth and so on. CLAUSE scope CLAUSE is one of the standard textual subdivisions. It consists of one or several words within a sentence representing the smallest grammatical unit that can express a complete proposition. By selecting this option, the rule will be applied to every clause that has been recognized in the entire input document. The syntax for the scope option CLAUSE is the following: SCOPE CLAUSE { rules } The disambiguator is able to recognize the clauses in a sentence (if they exist), identify the types of clauses (independent or dependent) and also the types of dependent clauses. The predefined names of such clause types can be optionally used as constraints to define a clause scope. The syntax is: SCOPE CLAUSE (clause_type) { rules } The clause types are listed below: as GENERIC identifies a portion of text which is not a proper clause, the SCOPE CLAUSE syntax with clause_type GENERIC does not trigger any rule. Type Abbreviation Description INDEPENDENT IND Corresponds to the only clause of a simple sentence or to the main clause of a complex sentence containing several clauses SUBORDINATE SUB Corresponds to any kind of dependent clause that adds information to an independent clause, but which cannot stand by itself as a sentence. For example all the clause types listed here excepted the INDEPENDENT RELATIVE REL Corresponds to a kind of subordinate clause that begins with a relative pronoun and contains an element, whose interpretation is provided by an antecedent on which the subordinate clause is grammatically dependent NON-FINITE NF Corresponds to a kind of subordinate clause containing a verb which does not show tense (non-finite) PREPOSITIONAL PRP Corresponds to a kind of subordinate clause introduced by a preposition CAUSAL CSL Corresponds to a kind of subordinate clause that states the reason or cause of the fact stated in the independent clause TEMPORAL TMP Corresponds to a kind of subordinate clause that indicates an act or state that occurs prior to, at the same time as, or subsequent to the act or state of the main clause COMPARATIVE CMP Corresponds to a kind of subordinate clause that follows the comparative form of an adjective or adverb SUBJECT / OBJECT S/O Corresponds to a kind of subordinate noun clause which acts as the subject or the object of the preposition NON-FINITE SUBJECT / OBJECT NSO Corresponds to a kind of subordinate noun clause containing a verb which does not show tense (non-finite) CONDITIONAL SUBJECT / OBJECT CSO Corresponds to a kind of subordinate noun clause expressing factual implications or hypothetical situations and their consequences (introduced by if) PURPOSE PRS Corresponds to a kind of adverbial clause expressing purpose (commonly referred to in linguistics as final clause) CONSECUTIVE CNS Corresponds to a kind of subordinate clause which expresses the result of the action stated in the main clause or a preceding sentence CONCESSIVE CNC Corresponds to a kind of subordinate clause which expresses an opposite idea compared to the main clause ADVERSATIVE ADV Corresponds to a kind of subordinate clause which expresses an event or situation that is opposite to that of the main clause (introduced by but ...) GENERIC 1 SUB Denotes portions of text that are not real clauses PARENTHETIC INC Corresponds to a kind of clause, often explanatory or qualifying, inserted into a passage with which it is not grammatically connected, and marked off by brackets, dashes, etc. It is possible to select a single clause type or a list of several types to be used as constraint. For example: SCOPE CLAUSE (INDEPENDENT) { //rule or list of rules } is a rule which acts upon any text block recognized to be an independent clause within a sentence, whereas SCOPE CLAUSE (TEMPORAL, COMPARATIVE) { //rule or list of rules } acts upon any text block recognized to be either a temporal or a comparative clause. See also the combination of PHRASE and CLAUSE scope options . PHRASE scope PHRASE is the narrowest among the standard textual subdivisions. It consists of one or several words that function as a constituent of the sentence and act as single units in its syntax. Common examples of \"phrases\" are noun phrases, verb phrases, etc. By selecting this option, a rule will act upon every phrase that has been recognized in the whole input document. The syntax for the scope option PHRASE is the following: SCOPE PHRASE { rules } The disambiguator is able to recognize the phrases in a sentence and identify which types they are. The predefined names of such PHRASE types can be optionally used as constraints to define a PHRASE scope. The syntax is: SCOPE PHRASE (phrase_type) { rules } The phrase types are: Phrase Type Phrase Type (Italian only) Description AP GA Adjective Phrase CP CN Conjunction Phrase DP GV Adverb Phrase NA NA Not Applicable (usually indicates punctuation) NP GN Noun Phrase PN PN Nominal Predicate PP GP Preposition Phrase RP GR Relative Phrase VP PV Verb Phrase It is possible to select a single PHRASE type or a list of them to be used as constraint. For example: SCOPE PHRASE (NP) { //rule or list of rules } is a rule which acts upon any text block recognized to be a noun phrase, whereas: SCOPE PHRASE (PP, NP) { //rule or list of rules } will act upon any text block recognized to be either a prepositional phrase or a noun phrase. Finally, PHRASE types can be used in sequences of two or more elements, each separated by a slash sign ( / ). For example: SCOPE PHRASE (NP/VP) { //rule or list of rules } will act upon any text block recognized to be a noun phrase followed by a verb phrase. More complex combinations such as: SCOPE PHRASE (AP, PP, NP/VP) { //rule or list of rules } are also valid. Such a scope definition will act upon any text block recognized to be an adjective phrase or a prepositional phrase or a noun phrase followed by a verb phrase. Combination of PHRASE and CLAUSE scope options PHRASE and CLAUSE options can be combined when setting a rule's scope. In particular, it is possible to select a phrase included within a specific clause scope. In other words, selecting this option, a rule is required to act upon a phrase that has been recognized within a given clause. The syntax for combining the PHRASE and CLAUSE scope options is the following: SCOPE PHRASE IN CLAUSE (clause_type) { rules } where clause_type corresponds to one of the types available for the CLAUSE option. It is also possible to select one or more of the available phrase types to furtherly restrict the scope definition. SCOPE PHRASE (phrase_type) IN CLAUSE (clause_type) { rules } For example: SCOPE PHRASE (NP) IN CLAUSE (INDEPENDENT) { //rule or list of rules } will act upon a rule on any text block recognized to be a noun phrase within an independent clause. The use of such combinations aims to identify a very precise and limited area for the rule to act upon; in fact, the hits generated by the rules with this kind of scope are more likely to be characterized by high precision rather than high recall. As GENERIC identifies a portion of text which is not properly a clause, the scope clause syntax with clause_type GENERIC does not trigger any rule. \u21a9","title":"SCOPE standard options"},{"location":"scope/standard-options/#scope-standard-options","text":"","title":"SCOPE standard options"},{"location":"scope/standard-options/#introduction","text":"The standard scope options are those portions of a text that relate to the textual subdivisions generated during the semantic disambiguation process, and they are used to delimit the area of action of a rule or group of rules. There are four standard options listed below in order of extent (from the widest to the narrowest): PARAGRAPH SENTENCE CLAUSE PHRASE","title":"Introduction"},{"location":"scope/standard-options/#paragraph-scope","text":"PARAGRAPH is the broadest among the standard textual subdivisions. It is a unit of discourse consisting of one or more sentences dealing with a particular concept. Its start is typically indicated by the beginning of a new line. By selecting this option, a rule will be applied to every paragraph that has been recognized in the entire input document. The syntax for the scope option PARAGRAPH is the following: SCOPE PARAGRAPH { rules } It is also possible to operate on multiple paragraphs using the following syntax: SCOPE PARAGRAPH*n. { rules } Where the asterisk ( * ) is a multiplier and n is a number indicating how many adjoining paragraphs the rule will act upon. For example: SCOPE PARAGRAPH*2 { //rule or list of rules } the rule will act upon text blocks, each made of two paragraphs: the first and the second paragraph of the document, the second and the third, the third and the fourth, and so on.","title":"PARAGRAPH scope"},{"location":"scope/standard-options/#sentence-scope","text":"SENTENCE is one of the standard textual subdivisions. It consists of one or several words, linked to each other by a syntactic relation, which are able to convey meaning. The beginning of a sentence usually follows a punctuation mark such as a period, question mark or exclamation mark. By selecting this option, a rule will act upon each sentence that has been recognized in the whole input document. The syntax for the scope option SENTENCE is the following: SCOPE SENTENCE { rules } It is also possible to operate on multiple sentences by using the following syntax: SCOPE SENTENCE*n. { rules } Where the asterisk ( * ) is a multiplier and n is a number indicating how many adjoining sentences the rule will act upon. For example: SCOPE SENTENCE*3 { //rule or list of rules// } the rule would act upon text blocks made of three sentences: the first, the second and the third sentence of the text; the second, the third and the fourth and so on.","title":"SENTENCE scope"},{"location":"scope/standard-options/#clause-scope","text":"CLAUSE is one of the standard textual subdivisions. It consists of one or several words within a sentence representing the smallest grammatical unit that can express a complete proposition. By selecting this option, the rule will be applied to every clause that has been recognized in the entire input document. The syntax for the scope option CLAUSE is the following: SCOPE CLAUSE { rules } The disambiguator is able to recognize the clauses in a sentence (if they exist), identify the types of clauses (independent or dependent) and also the types of dependent clauses. The predefined names of such clause types can be optionally used as constraints to define a clause scope. The syntax is: SCOPE CLAUSE (clause_type) { rules } The clause types are listed below: as GENERIC identifies a portion of text which is not a proper clause, the SCOPE CLAUSE syntax with clause_type GENERIC does not trigger any rule. Type Abbreviation Description INDEPENDENT IND Corresponds to the only clause of a simple sentence or to the main clause of a complex sentence containing several clauses SUBORDINATE SUB Corresponds to any kind of dependent clause that adds information to an independent clause, but which cannot stand by itself as a sentence. For example all the clause types listed here excepted the INDEPENDENT RELATIVE REL Corresponds to a kind of subordinate clause that begins with a relative pronoun and contains an element, whose interpretation is provided by an antecedent on which the subordinate clause is grammatically dependent NON-FINITE NF Corresponds to a kind of subordinate clause containing a verb which does not show tense (non-finite) PREPOSITIONAL PRP Corresponds to a kind of subordinate clause introduced by a preposition CAUSAL CSL Corresponds to a kind of subordinate clause that states the reason or cause of the fact stated in the independent clause TEMPORAL TMP Corresponds to a kind of subordinate clause that indicates an act or state that occurs prior to, at the same time as, or subsequent to the act or state of the main clause COMPARATIVE CMP Corresponds to a kind of subordinate clause that follows the comparative form of an adjective or adverb SUBJECT / OBJECT S/O Corresponds to a kind of subordinate noun clause which acts as the subject or the object of the preposition NON-FINITE SUBJECT / OBJECT NSO Corresponds to a kind of subordinate noun clause containing a verb which does not show tense (non-finite) CONDITIONAL SUBJECT / OBJECT CSO Corresponds to a kind of subordinate noun clause expressing factual implications or hypothetical situations and their consequences (introduced by if) PURPOSE PRS Corresponds to a kind of adverbial clause expressing purpose (commonly referred to in linguistics as final clause) CONSECUTIVE CNS Corresponds to a kind of subordinate clause which expresses the result of the action stated in the main clause or a preceding sentence CONCESSIVE CNC Corresponds to a kind of subordinate clause which expresses an opposite idea compared to the main clause ADVERSATIVE ADV Corresponds to a kind of subordinate clause which expresses an event or situation that is opposite to that of the main clause (introduced by but ...) GENERIC 1 SUB Denotes portions of text that are not real clauses PARENTHETIC INC Corresponds to a kind of clause, often explanatory or qualifying, inserted into a passage with which it is not grammatically connected, and marked off by brackets, dashes, etc. It is possible to select a single clause type or a list of several types to be used as constraint. For example: SCOPE CLAUSE (INDEPENDENT) { //rule or list of rules } is a rule which acts upon any text block recognized to be an independent clause within a sentence, whereas SCOPE CLAUSE (TEMPORAL, COMPARATIVE) { //rule or list of rules } acts upon any text block recognized to be either a temporal or a comparative clause. See also the combination of PHRASE and CLAUSE scope options .","title":"CLAUSE scope"},{"location":"scope/standard-options/#phrase-scope","text":"PHRASE is the narrowest among the standard textual subdivisions. It consists of one or several words that function as a constituent of the sentence and act as single units in its syntax. Common examples of \"phrases\" are noun phrases, verb phrases, etc. By selecting this option, a rule will act upon every phrase that has been recognized in the whole input document. The syntax for the scope option PHRASE is the following: SCOPE PHRASE { rules } The disambiguator is able to recognize the phrases in a sentence and identify which types they are. The predefined names of such PHRASE types can be optionally used as constraints to define a PHRASE scope. The syntax is: SCOPE PHRASE (phrase_type) { rules } The phrase types are: Phrase Type Phrase Type (Italian only) Description AP GA Adjective Phrase CP CN Conjunction Phrase DP GV Adverb Phrase NA NA Not Applicable (usually indicates punctuation) NP GN Noun Phrase PN PN Nominal Predicate PP GP Preposition Phrase RP GR Relative Phrase VP PV Verb Phrase It is possible to select a single PHRASE type or a list of them to be used as constraint. For example: SCOPE PHRASE (NP) { //rule or list of rules } is a rule which acts upon any text block recognized to be a noun phrase, whereas: SCOPE PHRASE (PP, NP) { //rule or list of rules } will act upon any text block recognized to be either a prepositional phrase or a noun phrase. Finally, PHRASE types can be used in sequences of two or more elements, each separated by a slash sign ( / ). For example: SCOPE PHRASE (NP/VP) { //rule or list of rules } will act upon any text block recognized to be a noun phrase followed by a verb phrase. More complex combinations such as: SCOPE PHRASE (AP, PP, NP/VP) { //rule or list of rules } are also valid. Such a scope definition will act upon any text block recognized to be an adjective phrase or a prepositional phrase or a noun phrase followed by a verb phrase.","title":"PHRASE scope"},{"location":"scope/standard-options/#combination-of-phrase-and-clause-scope-options","text":"PHRASE and CLAUSE options can be combined when setting a rule's scope. In particular, it is possible to select a phrase included within a specific clause scope. In other words, selecting this option, a rule is required to act upon a phrase that has been recognized within a given clause. The syntax for combining the PHRASE and CLAUSE scope options is the following: SCOPE PHRASE IN CLAUSE (clause_type) { rules } where clause_type corresponds to one of the types available for the CLAUSE option. It is also possible to select one or more of the available phrase types to furtherly restrict the scope definition. SCOPE PHRASE (phrase_type) IN CLAUSE (clause_type) { rules } For example: SCOPE PHRASE (NP) IN CLAUSE (INDEPENDENT) { //rule or list of rules } will act upon a rule on any text block recognized to be a noun phrase within an independent clause. The use of such combinations aims to identify a very precise and limited area for the rule to act upon; in fact, the hits generated by the rules with this kind of scope are more likely to be characterized by high precision rather than high recall. As GENERIC identifies a portion of text which is not properly a clause, the scope clause syntax with clause_type GENERIC does not trigger any rule. \u21a9","title":"Combination of PHRASE and CLAUSE scope options"},{"location":"scripting/","text":"Scripting overview Text intelligence engines created with Studio execute the script defined in the main.jr file. By default, when you create a project with Studio, the main.jr file only defines the initialize and the shutdown functions and contains the commented out prototypes of other functions. In this state, the script doesn't affect the engine's results, which are thus solely determined by rules, but if you uncomment one or more functions and put specific code inside them, you could effectively control and extend the analysis pipeline. All of the predefined or commented out functions in main.jr are event handlers , namely portions of code that are automatically executed before or after a specific processing event. The initialize function is executed when the engine starts, the shutdown function is executed immediately before the engine is stopped, while the other functions are called at specific moments of the document analysis pipeline . The phases of the pipeline and the events that are fired after those phases are shown in the following figure. Events are listed inside the dashed area. The handling functions corresponding to events are listed in the following table. Event Event handling function Prepare onPrepare Tagger onTagger Categorizer onCategorizer Finalize onFinalize The following articles in this section describe what you can do within each of these event handling functions, while specific articles are devoted to: The REX object which allows for regular expression-based find & replace operations. The DIS object which gives access to the results of the disambiguation phase. The script can be debugged with the Studio built-in debugger .","title":"Scripting overview"},{"location":"scripting/#scripting-overview","text":"Text intelligence engines created with Studio execute the script defined in the main.jr file. By default, when you create a project with Studio, the main.jr file only defines the initialize and the shutdown functions and contains the commented out prototypes of other functions. In this state, the script doesn't affect the engine's results, which are thus solely determined by rules, but if you uncomment one or more functions and put specific code inside them, you could effectively control and extend the analysis pipeline. All of the predefined or commented out functions in main.jr are event handlers , namely portions of code that are automatically executed before or after a specific processing event. The initialize function is executed when the engine starts, the shutdown function is executed immediately before the engine is stopped, while the other functions are called at specific moments of the document analysis pipeline . The phases of the pipeline and the events that are fired after those phases are shown in the following figure. Events are listed inside the dashed area. The handling functions corresponding to events are listed in the following table. Event Event handling function Prepare onPrepare Tagger onTagger Categorizer onCategorizer Finalize onFinalize The following articles in this section describe what you can do within each of these event handling functions, while specific articles are devoted to: The REX object which allows for regular expression-based find & replace operations. The DIS object which gives access to the results of the disambiguation phase. The script can be debugged with the Studio built-in debugger .","title":"Scripting overview"},{"location":"scripting/dis-object/","text":"Introduction to the DIS object The DIS objects and its methods The predefined DIS object gives access to the disambiguation results and also allows you to tag or untag text subdivisions. It can be used in the onTagger() , onCategorizer() and onFinalize() functions, which are executed after disambiguation. The functionalities of the DIS object are exposed through its methods, which can be grouped into these categories: Count text subdivisions Get the text of the whole document or that of text subdivisions Get objects corresponding to text subdivisions to explore their properties Get the index of the text subdivision of a certain kind which contains a character at a given position with respect to the document text Tag and untag text subdivisions (Reserved for future use) Access the results of document understanding Text subdivisions: tokens and atoms With the exception of the methods that have to do with document understanding , all the other methods of the DIS object are based on the creation of text subdivisions\u2014with different granularity\u2014operated by disambiguation. At the token level, subdivisions also include sub-tokens called atoms. Disambiguation lists atoms immediately after the token they are part of. For example, given this input text: Michael Jordan was one of the best basketball players of all time. disambiguation identifies these 15 units as either tokens or atoms: Index Text Sub-token (atom)? 0 Michael Jordan No 1 Michael Yes 2 Jordan Yes 3 was No 4 one No 5 of No 6 the No 7 best No 8 basketball players No 9 basketball Yes 10 players Yes 11 of No 12 all No 13 time No 14 . No","title":"Introduction to the DIS object"},{"location":"scripting/dis-object/#introduction-to-the-dis-object","text":"","title":"Introduction to the DIS object"},{"location":"scripting/dis-object/#the-dis-objects-and-its-methods","text":"The predefined DIS object gives access to the disambiguation results and also allows you to tag or untag text subdivisions. It can be used in the onTagger() , onCategorizer() and onFinalize() functions, which are executed after disambiguation. The functionalities of the DIS object are exposed through its methods, which can be grouped into these categories: Count text subdivisions Get the text of the whole document or that of text subdivisions Get objects corresponding to text subdivisions to explore their properties Get the index of the text subdivision of a certain kind which contains a character at a given position with respect to the document text Tag and untag text subdivisions (Reserved for future use) Access the results of document understanding","title":"The DIS objects and its methods"},{"location":"scripting/dis-object/#text-subdivisions-tokens-and-atoms","text":"With the exception of the methods that have to do with document understanding , all the other methods of the DIS object are based on the creation of text subdivisions\u2014with different granularity\u2014operated by disambiguation. At the token level, subdivisions also include sub-tokens called atoms. Disambiguation lists atoms immediately after the token they are part of. For example, given this input text: Michael Jordan was one of the best basketball players of all time. disambiguation identifies these 15 units as either tokens or atoms: Index Text Sub-token (atom)? 0 Michael Jordan No 1 Michael Yes 2 Jordan Yes 3 was No 4 one No 5 of No 6 the No 7 best No 8 basketball players No 9 basketball Yes 10 players Yes 11 of No 12 all No 13 time No 14 . No","title":"Text subdivisions: tokens and atoms"},{"location":"scripting/dis-object/count/","text":"Count text subdivisions The DIS module provides a number of methods to count the text subdivisions of a certain type that the disambiguation identified. These methods are listed in the following table. Subdivision type Method Section getSectionsCount Paragraph getParagraphsCount Sentence getSentencesCount Phrase getPhraseCount Token getTokensCount For example, considering the following input text: If you are a whiskey lover, you will know that it is a spirit produced from fermented grain and aged in the wood. And a spirit is an alcoholic beverage in which the alcohol content has been increased by distillation. this statement: var sentenceCount = DIS . getSentencesCount (); sets the sentenceCount variable to 2, since the disambiguation found two sentences in the text. This other statement: var tokenCount = DIS . getTokensCount (); sets the tokenCount variable to 44 that is the number of detected tokens. Remember that also atoms are counted as tokens . The syntax is similar: all the methods have no arguments and return an integer number.","title":"Count text subdivisions"},{"location":"scripting/dis-object/count/#count-text-subdivisions","text":"The DIS module provides a number of methods to count the text subdivisions of a certain type that the disambiguation identified. These methods are listed in the following table. Subdivision type Method Section getSectionsCount Paragraph getParagraphsCount Sentence getSentencesCount Phrase getPhraseCount Token getTokensCount For example, considering the following input text: If you are a whiskey lover, you will know that it is a spirit produced from fermented grain and aged in the wood. And a spirit is an alcoholic beverage in which the alcohol content has been increased by distillation. this statement: var sentenceCount = DIS . getSentencesCount (); sets the sentenceCount variable to 2, since the disambiguation found two sentences in the text. This other statement: var tokenCount = DIS . getTokensCount (); sets the tokenCount variable to 44 that is the number of detected tokens. Remember that also atoms are counted as tokens . The syntax is similar: all the methods have no arguments and return an integer number.","title":"Count text subdivisions"},{"location":"scripting/dis-object/du/","text":"Document understanding results The DIS object exposes methods meant to tag/untag and give access to the results of document understanding functionalities that are going to be added soon to Studio and to text intelligence engines. Document understanding is a separate process from disambiguation, it determines the inner structure of documents, for example it identifies the tables contained in the pages of a PDF document. For now, those methods perform no action and they are reserved for future use. They are listed below. Method Purpose tagCell Tag a tables's cell untagCell Untag a tables's cell getTablesCount Get the count of tables that have been identified getTable Return a table object getTableCell Return a cell object given a table and row-column coordinates getCell Return a cell from the sequence of the cells of all the tables getCellText Return the text of a cell","title":"Document understanding results"},{"location":"scripting/dis-object/du/#document-understanding-results","text":"The DIS object exposes methods meant to tag/untag and give access to the results of document understanding functionalities that are going to be added soon to Studio and to text intelligence engines. Document understanding is a separate process from disambiguation, it determines the inner structure of documents, for example it identifies the tables contained in the pages of a PDF document. For now, those methods perform no action and they are reserved for future use. They are listed below. Method Purpose tagCell Tag a tables's cell untagCell Untag a tables's cell getTablesCount Get the count of tables that have been identified getTable Return a table object getTableCell Return a cell object given a table and row-column coordinates getCell Return a cell from the sequence of the cells of all the tables getCellText Return the text of a cell","title":"Document understanding results"},{"location":"scripting/dis-object/index-from-position/","text":"Get index from position The tokenIndexFromPos method returns the index, in the sequence of tokens and atoms identified by the disambiguation, of the token whose text contains a character occupying a given position. For example considering the text: Michael Jordan was one of the best basketball players of all time. character at position 41 is the second b of basketball players : Michael Jordan was one of the best basket b all players of all time. \u2191 01234567890123456789012345678901234567890 1 234567890123456789012345 0 1 2 3 4 5 6 so this statement: var tokenIndex = DIS . tokenIndexFromPos ( 41 ); sets the tokenIndex variable to 8, which is the index of the basketball players token in the sequence of tokens and atoms identified by disambiguation. Note When a token is composed of atoms, the same character is part of both the token and one of its atoms, but the tokenIndexFromPos method always returns the index of the token. The syntax is: DIS.tokenIndexFromPos(position) where position is the zero-based position of a character in the document text. Token index can be used to get the text of the token, get the token object or tag/untag the token.","title":"Get index from position"},{"location":"scripting/dis-object/index-from-position/#get-index-from-position","text":"The tokenIndexFromPos method returns the index, in the sequence of tokens and atoms identified by the disambiguation, of the token whose text contains a character occupying a given position. For example considering the text: Michael Jordan was one of the best basketball players of all time. character at position 41 is the second b of basketball players : Michael Jordan was one of the best basket b all players of all time. \u2191 01234567890123456789012345678901234567890 1 234567890123456789012345 0 1 2 3 4 5 6 so this statement: var tokenIndex = DIS . tokenIndexFromPos ( 41 ); sets the tokenIndex variable to 8, which is the index of the basketball players token in the sequence of tokens and atoms identified by disambiguation. Note When a token is composed of atoms, the same character is part of both the token and one of its atoms, but the tokenIndexFromPos method always returns the index of the token. The syntax is: DIS.tokenIndexFromPos(position) where position is the zero-based position of a character in the document text. Token index can be used to get the text of the token, get the token object or tag/untag the token.","title":"Get index from position"},{"location":"scripting/dis-object/tag-untag/","text":"Tag and untag The DIS object provides a number of methods to tag or untag the tokens of the text subdivisions that the disambiguation identified. The methods are listed in the following table. Subdivision Tag method Untag method Paragraph tagParagraph untagParagraph Sentence tagSentence untagSentence Phrase tagPhrase untagPhrase Token tagToken untagToken Every method works at the token level and adds or removes a tag to/from all the tokens of a specified text subdivision. So for example this statement: DIS . tagSentence ( 5 , \"myTAG\" ); tags all the tokens of the 6th sentence of the text\u2014that with zero-based index 5\u2014with the myTAG label, while: DIS . untagSentence ( 5 , \"myTAG\" ); removes the myTAG label from all the tokens of the same sentence. All methods have a similar syntax: DIS. method (subdivision_index, tag_label) where subdivision_index is the index of the text subdivision to tag and tag_label is a string\u2014constant or variable\u2014representing the tag label.","title":"Tag and untag"},{"location":"scripting/dis-object/tag-untag/#tag-and-untag","text":"The DIS object provides a number of methods to tag or untag the tokens of the text subdivisions that the disambiguation identified. The methods are listed in the following table. Subdivision Tag method Untag method Paragraph tagParagraph untagParagraph Sentence tagSentence untagSentence Phrase tagPhrase untagPhrase Token tagToken untagToken Every method works at the token level and adds or removes a tag to/from all the tokens of a specified text subdivision. So for example this statement: DIS . tagSentence ( 5 , \"myTAG\" ); tags all the tokens of the 6th sentence of the text\u2014that with zero-based index 5\u2014with the myTAG label, while: DIS . untagSentence ( 5 , \"myTAG\" ); removes the myTAG label from all the tokens of the same sentence. All methods have a similar syntax: DIS. method (subdivision_index, tag_label) where subdivision_index is the index of the text subdivision to tag and tag_label is a string\u2014constant or variable\u2014representing the tag label.","title":"Tag and untag"},{"location":"scripting/dis-object/text/","text":"Get text The DIS object provides a number of methods to get the text of the subdivisions that the disambiguation identified. These methods are listed in the following table. Subdivision type Method Entire document getText Section getSectionText Paragraph getParagraphText Sentence getSentenceText Phrase getPhraseText Token getTokenText For example, considering the following text: Michael Jordan was one of the best basketball players of all time. this statement: var tokenText = DIS . getTokenText ( 6 ); sets the tokenText variable to the text ( the ) of the seventh subdivision in the sequence of tokens and atoms identified by disambiguation. This other statement: var phraseText = DIS . getPhraseText ( 3 ); sets the phraseText variable to the text ( of all the time ) of the fourth phrase. The getText method has no arguments, while all the other methods share this syntax: DIS. method (index) where index is the integer number representing the zero-based index of a specific text subdivision in the sequence of all the subsequent subdivisions of the same type that the disambiguation identified.","title":"Get text"},{"location":"scripting/dis-object/text/#get-text","text":"The DIS object provides a number of methods to get the text of the subdivisions that the disambiguation identified. These methods are listed in the following table. Subdivision type Method Entire document getText Section getSectionText Paragraph getParagraphText Sentence getSentenceText Phrase getPhraseText Token getTokenText For example, considering the following text: Michael Jordan was one of the best basketball players of all time. this statement: var tokenText = DIS . getTokenText ( 6 ); sets the tokenText variable to the text ( the ) of the seventh subdivision in the sequence of tokens and atoms identified by disambiguation. This other statement: var phraseText = DIS . getPhraseText ( 3 ); sets the phraseText variable to the text ( of all the time ) of the fourth phrase. The getText method has no arguments, while all the other methods share this syntax: DIS. method (index) where index is the integer number representing the zero-based index of a specific text subdivision in the sequence of all the subsequent subdivisions of the same type that the disambiguation identified.","title":"Get text"},{"location":"scripting/dis-object/text-subdivisions-objects/","text":"Text subdivisions objects Overview Some methods of the DIS object return objects corresponding to text subdivisions. These objects have properties that correspond to the attributes that the disambiguation has found. The methods are listed below. Subdivision type Method Section getSection Paragraph getParagraph Sentence getSentence Phrase getPhrase Token getToken For example, this statement: sentence = DIS . getSentence ( 3 ); sets the sentence variable to an object representing the 4th sentence of the document's text. Syntax All methods have a similar syntax: DIS. method (index) where index is the integer number representing the zero-based index of a specific text subdivision in the sequence of all the subsequent subdivisions of the same type that the disambiguation identified. Sections The objects returned by the getSection method have these properties: Property Description name Section name position Position of the first character of the sentence in the text length Sentence length sentenceBegin Index of the first sentence of the section sentenceEnd Index of the last sentence of the section phraseBegin Index of the first phrase of the section phraseEnd Index of the last phrase of the section tokenBegin Index of the first token of the section tokenEnd Index of the last token of the section Paragraphs The objects returned by the getParagraph method have these properties: Property Description position Position of the first character of the paragraph in the text length Paragraph length sentenceBegin Index of the first sentence of the paragraph sentenceEnd Index of the last sentence of the paragraph phraseBegin Index of the first phrase of the paragraph phraseEnd Index of the last phrase of the paragraph tokenBegin Index of the first token of the paragraph tokenEnd Index of the last token of the paragraph Sentences The objects returned by the getSentence method have these properties: Property Description position Position of the first character of the sentence in the text length Sentence length phraseBegin Index of the first phrase of the sentence phraseEnd Index of the last phrase of the sentence tokenBegin Index of the first token of the sentence tokenEnd Index of the last token of the sentence Phrases The objects returned by the getPhrase method have these properties: Property Description position Position of the first character of the phrase in the text length Phrase length gk Phrase type pr Reserved for future use pk Clause type lt Index of the main token of the phrase tokenBegin Index of the first token of the phrase tokenEnd Index of the last token of the phrase Tokens The objects returned by the getToken method have these properties: Property Description position Position of the first character of the token in the text length Token length index Index of the token grammarType Word class typeClass In case of proper nouns, the entity type lemma Token's lemma lemmaId Id of the token's lemma inside the reference Knowledge Graph synId Id of the token's syncon inside the reference Knowledge Graph externalSynIds Array of Ids of the same concept in external Knowledge Graph sections 1 dadId Reserved for future use phrase Index of the phrase containing the token sentence Index of the sentence containing the token paragraph Index of the paragraph containing the token section Index of the section containing the token headingId Reserved for future use cellId Reserved for future use isToken true if the object corresponds to a token subdivision, false if it corresponds to an atom (sub-token) isAtom true if the object corresponds to an atom of a token, false if it corresponds to a token The base Knowledge Graph can be linked to so-called external sections\u2014for example Wikipedia, GeoNames or custom sections\u2014in which the same concept is identified with different numbers. \u21a9","title":"Text subdivisions objects"},{"location":"scripting/dis-object/text-subdivisions-objects/#text-subdivisions-objects","text":"","title":"Text subdivisions objects"},{"location":"scripting/dis-object/text-subdivisions-objects/#overview","text":"Some methods of the DIS object return objects corresponding to text subdivisions. These objects have properties that correspond to the attributes that the disambiguation has found. The methods are listed below. Subdivision type Method Section getSection Paragraph getParagraph Sentence getSentence Phrase getPhrase Token getToken For example, this statement: sentence = DIS . getSentence ( 3 ); sets the sentence variable to an object representing the 4th sentence of the document's text.","title":"Overview"},{"location":"scripting/dis-object/text-subdivisions-objects/#syntax","text":"All methods have a similar syntax: DIS. method (index) where index is the integer number representing the zero-based index of a specific text subdivision in the sequence of all the subsequent subdivisions of the same type that the disambiguation identified.","title":"Syntax"},{"location":"scripting/dis-object/text-subdivisions-objects/#sections","text":"The objects returned by the getSection method have these properties: Property Description name Section name position Position of the first character of the sentence in the text length Sentence length sentenceBegin Index of the first sentence of the section sentenceEnd Index of the last sentence of the section phraseBegin Index of the first phrase of the section phraseEnd Index of the last phrase of the section tokenBegin Index of the first token of the section tokenEnd Index of the last token of the section","title":"Sections"},{"location":"scripting/dis-object/text-subdivisions-objects/#paragraphs","text":"The objects returned by the getParagraph method have these properties: Property Description position Position of the first character of the paragraph in the text length Paragraph length sentenceBegin Index of the first sentence of the paragraph sentenceEnd Index of the last sentence of the paragraph phraseBegin Index of the first phrase of the paragraph phraseEnd Index of the last phrase of the paragraph tokenBegin Index of the first token of the paragraph tokenEnd Index of the last token of the paragraph","title":"Paragraphs"},{"location":"scripting/dis-object/text-subdivisions-objects/#sentences","text":"The objects returned by the getSentence method have these properties: Property Description position Position of the first character of the sentence in the text length Sentence length phraseBegin Index of the first phrase of the sentence phraseEnd Index of the last phrase of the sentence tokenBegin Index of the first token of the sentence tokenEnd Index of the last token of the sentence","title":"Sentences"},{"location":"scripting/dis-object/text-subdivisions-objects/#phrases","text":"The objects returned by the getPhrase method have these properties: Property Description position Position of the first character of the phrase in the text length Phrase length gk Phrase type pr Reserved for future use pk Clause type lt Index of the main token of the phrase tokenBegin Index of the first token of the phrase tokenEnd Index of the last token of the phrase","title":"Phrases"},{"location":"scripting/dis-object/text-subdivisions-objects/#tokens","text":"The objects returned by the getToken method have these properties: Property Description position Position of the first character of the token in the text length Token length index Index of the token grammarType Word class typeClass In case of proper nouns, the entity type lemma Token's lemma lemmaId Id of the token's lemma inside the reference Knowledge Graph synId Id of the token's syncon inside the reference Knowledge Graph externalSynIds Array of Ids of the same concept in external Knowledge Graph sections 1 dadId Reserved for future use phrase Index of the phrase containing the token sentence Index of the sentence containing the token paragraph Index of the paragraph containing the token section Index of the section containing the token headingId Reserved for future use cellId Reserved for future use isToken true if the object corresponds to a token subdivision, false if it corresponds to an atom (sub-token) isAtom true if the object corresponds to an atom of a token, false if it corresponds to a token The base Knowledge Graph can be linked to so-called external sections\u2014for example Wikipedia, GeoNames or custom sections\u2014in which the same concept is identified with different numbers. \u21a9","title":"Tokens"},{"location":"scripting/initialize-shutdown/","text":"initialize and shutdown functions initialize The initialize function is called by a text intelligence engine at startup. In Studio, this happens after every build operation. This is the default definition: function initialize ( cmdline ) { return true ; } The cmdline argument is reserved for future use. This function is the right place to put the initialization of objects that are needed in other event handling functions. The engine, in fact, will continue its execution, only if the initialize function returns true . If your code catches initialization errors and returns false , the engine will stop, thus avoiding unwanted effects you'd surely have if you let the engine go on and have it execute other functions with global objects that have not corretcly been initialized. shutdown The shutdown function is called by a text intelligence engine immediately before it stops. In Studio, this happens after every build operation but the first: the engine that was previously built is stopped and a new one is started. This is the default definition: function shutdown () { } This can be the place in which to deallocate global objects, especially in Studio, otherwise occupied memory is released only when you close Studio. For example, the best practice is to put here REX.close() , which closes every previously instantiated regular expression.","title":"initialize and shutdown functions"},{"location":"scripting/initialize-shutdown/#initialize-and-shutdown-functions","text":"","title":"initialize and shutdown functions"},{"location":"scripting/initialize-shutdown/#initialize","text":"The initialize function is called by a text intelligence engine at startup. In Studio, this happens after every build operation. This is the default definition: function initialize ( cmdline ) { return true ; } The cmdline argument is reserved for future use. This function is the right place to put the initialization of objects that are needed in other event handling functions. The engine, in fact, will continue its execution, only if the initialize function returns true . If your code catches initialization errors and returns false , the engine will stop, thus avoiding unwanted effects you'd surely have if you let the engine go on and have it execute other functions with global objects that have not corretcly been initialized.","title":"initialize"},{"location":"scripting/initialize-shutdown/#shutdown","text":"The shutdown function is called by a text intelligence engine immediately before it stops. In Studio, this happens after every build operation but the first: the engine that was previously built is stopped and a new one is started. This is the default definition: function shutdown () { } This can be the place in which to deallocate global objects, especially in Studio, otherwise occupied memory is released only when you close Studio. For example, the best practice is to put here REX.close() , which closes every previously instantiated regular expression.","title":"shutdown"},{"location":"scripting/modules/","text":"Modules overview As stated in the scripting overview , the main.jr file is where the text intelligence engine expects to find a script that allows extending the default document workflow. When executing the document processing pipeline, the engine runs any event handler defined in the file. The event handlers are script functions that can use other functions and variables defined within the same file. It is also possible to use code written in other files with the .jr extension as long as their code is appropriately structured. These additional files are called modules . A module is roughly comparable to a class of objects. By declaring a variable with this syntax in the main.jr file: var variable = require( {module path} ); the variable becomes an instance of the module with the given path. For example: var sphere1 = require ( 'modules/sphere' ); The path is that of the .jr file with respect to the project folder and without the extension. So, in the case of the example above, the module file would be found in a modules sub-folder and its name would be sphere.jr . Info The use of require() is not limited to the main.jr file, it can also be used in a module to reference other modules. Functions, variables and constants defined within a module have a local scope, they are not visible outside the module. However, defining new properties of the exports predefined object creates public properties and methods for the instances of the module created with require() . To illustrate modules and how to use the exports object to expose their functionalities, let's use the example of a geometric object, the sphere. What we could want is a module that's able to \"create\" a sphere object by specifying its radius and, after that, exposes the surface and the volume of the resulting sphere through two of the object's properties. The module file could be called sphere.jr , be located in the modules sub-folder of the project and have these contents: var localSurface ; var localVolume ; function surfaceFunction ( radius ) { return 4 * Math . PI * Math . pow ( radius , 2 ); } function volumeFunction ( radius ) { return 4 / 3 * Math . PI * Math . pow ( radius , 3 ); } function createFunction ( radius ) { localSurface = surfaceFunction ( radius ); localVolume = volumeFunction ( radius ); exports . surface = localSurface ; exports . volume = localVolume ; } exports . create = createFunction ; The createFunction function \"creates\" a sphere with the given radius by computing its surface and its volume. The results of these computations are first stored in two local variables that are then exposed as module (or \"object\") properties using the exports object. Finally also the createFunction function itself is exposed as a module method. export.surface , exports.volume and exports.create are properties of the exports object defined on the fly. The first two make the internal variables localSurface and localVolume externally visible as public properties surface and volume , while the third makes the internal function createFunction visible as the create() method. Neither the surfaceFunction and the volumeFunction functions nor the localSurface and the localVolume variables are externally visible. The module can be used like this in any function of main.jr : var sphere1 = require ( 'modules/sphere' ); sphere1 . create ( 7.5 ); CONSOLE . log ( 'The surface of a sphere with a radius of 7.5 is ' + sphere1 . surface + ' and the volume is ' + sphere1 . volume ); Info Also constants can become public properties of a module using this syntax: exports.property = {constant name or value}","title":"Modules overview"},{"location":"scripting/modules/#modules-overview","text":"As stated in the scripting overview , the main.jr file is where the text intelligence engine expects to find a script that allows extending the default document workflow. When executing the document processing pipeline, the engine runs any event handler defined in the file. The event handlers are script functions that can use other functions and variables defined within the same file. It is also possible to use code written in other files with the .jr extension as long as their code is appropriately structured. These additional files are called modules . A module is roughly comparable to a class of objects. By declaring a variable with this syntax in the main.jr file: var variable = require( {module path} ); the variable becomes an instance of the module with the given path. For example: var sphere1 = require ( 'modules/sphere' ); The path is that of the .jr file with respect to the project folder and without the extension. So, in the case of the example above, the module file would be found in a modules sub-folder and its name would be sphere.jr . Info The use of require() is not limited to the main.jr file, it can also be used in a module to reference other modules. Functions, variables and constants defined within a module have a local scope, they are not visible outside the module. However, defining new properties of the exports predefined object creates public properties and methods for the instances of the module created with require() . To illustrate modules and how to use the exports object to expose their functionalities, let's use the example of a geometric object, the sphere. What we could want is a module that's able to \"create\" a sphere object by specifying its radius and, after that, exposes the surface and the volume of the resulting sphere through two of the object's properties. The module file could be called sphere.jr , be located in the modules sub-folder of the project and have these contents: var localSurface ; var localVolume ; function surfaceFunction ( radius ) { return 4 * Math . PI * Math . pow ( radius , 2 ); } function volumeFunction ( radius ) { return 4 / 3 * Math . PI * Math . pow ( radius , 3 ); } function createFunction ( radius ) { localSurface = surfaceFunction ( radius ); localVolume = volumeFunction ( radius ); exports . surface = localSurface ; exports . volume = localVolume ; } exports . create = createFunction ; The createFunction function \"creates\" a sphere with the given radius by computing its surface and its volume. The results of these computations are first stored in two local variables that are then exposed as module (or \"object\") properties using the exports object. Finally also the createFunction function itself is exposed as a module method. export.surface , exports.volume and exports.create are properties of the exports object defined on the fly. The first two make the internal variables localSurface and localVolume externally visible as public properties surface and volume , while the third makes the internal function createFunction visible as the create() method. Neither the surfaceFunction and the volumeFunction functions nor the localSurface and the localVolume variables are externally visible. The module can be used like this in any function of main.jr : var sphere1 = require ( 'modules/sphere' ); sphere1 . create ( 7.5 ); CONSOLE . log ( 'The surface of a sphere with a radius of 7.5 is ' + sphere1 . surface + ' and the volume is ' + sphere1 . volume ); Info Also constants can become public properties of a module using this syntax: exports.property = {constant name or value}","title":"Modules overview"},{"location":"scripting/modules/jsonplug/","text":"JsonPlug JsonPlug is a post-processor that lets the users take advantage of the JsonPath query language to change the result object in a quick way for simple tasks which do not require custom or complex logics. The JsonPath module must be installed and called with a jsonpath variable. An exception will be thrown if this module is not installed. This post-processor allows you to: delete (selected categories, field or template: if a field is alone the whole template will be deleted). delete template (delete the father template of a selected field, regardless of other contained fields). add field (works with a selected template: if the user points to a field, it will be added to the parent template). add template (to the extraction node). clone (clone a field into a target template: if value to add is empty, the name will be cloned, otherwise a single string will be accepted as new name for the field, while value and tokens will be mantained; targets can be both field or template, the template will be chosen). clone new (clone fields into a new template, which will be placed in the extraction node). add category (add a category to the categorization node). modify (modify category id, category description, template name, field name, field value with a static string). modify regex (modify category id, category description, template name, field name, field value) with a regex; it supports backrefs. The order of the attributes to pass to the function is: Result (result object coming from the linguistic analysis; static value) Action (action to perform on the results object, see below) Boolean flag ( true = apply JsonPathAction if Jsonpath matches; false = apply JsonPathAction if JsonPath does not match anything, count = count how many JsonPath have matched) Jsonpath (source Jsonpath, used to validate the conditions and/or to select source fields/templates) Jsonpathaction (target Jsonpath, can be set up as the same as source Jsonpath for several actions) Recursive ( true = match as many items as possible with Jsonpathaction, false = stop after the first match) Values (always an array except for the delete action; the structure of this argument changes according to the Action , see below) jsonplug.jsonplug(result, \"delete\", true, \"$..extraction[?(@.template == 'Treatment_Details')]..fields[?(@.field == 'Sessions_Past_or_Future')]\", \"#this#\", true, \"\") If Jspathaction is not #this# , . or ^ , then Jsonpath is used as a boolean flag. jspath_condition_flag is a boolean, it can be: false true count ( > | < | <= | >= ) \\d e.g. count>0 , spaces are trimmed #this# or . in JsonPathAction will select the same value as Jsonpath . ^ in JsonPathAction will go up one level in respect to Jsonpath . ^ + (piece of jsonpath) will go up one level in respect to Jsonpath and navigate a sibling node. Recursive is either false or true , it is always applied to jspathaction , which can have the same value of Jsonpath . Values is always an array except for delete , in which it can be an empty string. Each action uses it in a different way: For delete , it can be an empty string (it's totally ignored, so it does not matter provided that it's passed). For add fields , it accepts only two keys: The first key is the field_name . The second key is the field_value . For add template , it accepts from three to infinite keys: The first key is the template_name , second is field_name , thirs is field_value . Each other pair (second+third, fourth+fifth... couples can be added ad infinitum .) is: field_name , field_value (no highlights supported at the moment). For clone , it must contain two keys: The first key can be empty, keeping the same field_name , or it can contain the new field_name , which will inherit the value of the source field. The second key can be either: no tokens (it doesn't clone tokens from the source). \"\" or clone from source (it clones the instances from the field(s) matched by the Jsonpath ). clone from sibling (it clones the instances from the field(s) matched by the Jsonpathaction ). For clone new , it must contain three keys: The first key is the new template_name . The second key can be empty, thus keeping the same field name, or it can contain the new field_name . the third key can be either: \"\" or \"no tokens\" (it doesn't clone tokens). clone from source (it clones the instances from the field(s) matched by the Jsonpath ). clone from sibling (it clones the instances from the field(s) matched by the Jsonpathaction ). For add category : The first key is the new category_ID . The second key is the category_description . The third key is the new category_score and compound . The fourth key is the new category_frequency . For modify , only one key is used, the latter being the new value. For modify regex , two keys are used: The first key is the regex to apply on the value. The second key is the replace string, it supports backrefs with $ for the matched groups.","title":"jsonplug"},{"location":"scripting/modules/jsonplug/#jsonplug","text":"JsonPlug is a post-processor that lets the users take advantage of the JsonPath query language to change the result object in a quick way for simple tasks which do not require custom or complex logics. The JsonPath module must be installed and called with a jsonpath variable. An exception will be thrown if this module is not installed. This post-processor allows you to: delete (selected categories, field or template: if a field is alone the whole template will be deleted). delete template (delete the father template of a selected field, regardless of other contained fields). add field (works with a selected template: if the user points to a field, it will be added to the parent template). add template (to the extraction node). clone (clone a field into a target template: if value to add is empty, the name will be cloned, otherwise a single string will be accepted as new name for the field, while value and tokens will be mantained; targets can be both field or template, the template will be chosen). clone new (clone fields into a new template, which will be placed in the extraction node). add category (add a category to the categorization node). modify (modify category id, category description, template name, field name, field value with a static string). modify regex (modify category id, category description, template name, field name, field value) with a regex; it supports backrefs. The order of the attributes to pass to the function is: Result (result object coming from the linguistic analysis; static value) Action (action to perform on the results object, see below) Boolean flag ( true = apply JsonPathAction if Jsonpath matches; false = apply JsonPathAction if JsonPath does not match anything, count = count how many JsonPath have matched) Jsonpath (source Jsonpath, used to validate the conditions and/or to select source fields/templates) Jsonpathaction (target Jsonpath, can be set up as the same as source Jsonpath for several actions) Recursive ( true = match as many items as possible with Jsonpathaction, false = stop after the first match) Values (always an array except for the delete action; the structure of this argument changes according to the Action , see below) jsonplug.jsonplug(result, \"delete\", true, \"$..extraction[?(@.template == 'Treatment_Details')]..fields[?(@.field == 'Sessions_Past_or_Future')]\", \"#this#\", true, \"\") If Jspathaction is not #this# , . or ^ , then Jsonpath is used as a boolean flag. jspath_condition_flag is a boolean, it can be: false true count ( > | < | <= | >= ) \\d e.g. count>0 , spaces are trimmed #this# or . in JsonPathAction will select the same value as Jsonpath . ^ in JsonPathAction will go up one level in respect to Jsonpath . ^ + (piece of jsonpath) will go up one level in respect to Jsonpath and navigate a sibling node. Recursive is either false or true , it is always applied to jspathaction , which can have the same value of Jsonpath . Values is always an array except for delete , in which it can be an empty string. Each action uses it in a different way: For delete , it can be an empty string (it's totally ignored, so it does not matter provided that it's passed). For add fields , it accepts only two keys: The first key is the field_name . The second key is the field_value . For add template , it accepts from three to infinite keys: The first key is the template_name , second is field_name , thirs is field_value . Each other pair (second+third, fourth+fifth... couples can be added ad infinitum .) is: field_name , field_value (no highlights supported at the moment). For clone , it must contain two keys: The first key can be empty, keeping the same field_name , or it can contain the new field_name , which will inherit the value of the source field. The second key can be either: no tokens (it doesn't clone tokens from the source). \"\" or clone from source (it clones the instances from the field(s) matched by the Jsonpath ). clone from sibling (it clones the instances from the field(s) matched by the Jsonpathaction ). For clone new , it must contain three keys: The first key is the new template_name . The second key can be empty, thus keeping the same field name, or it can contain the new field_name . the third key can be either: \"\" or \"no tokens\" (it doesn't clone tokens). clone from source (it clones the instances from the field(s) matched by the Jsonpath ). clone from sibling (it clones the instances from the field(s) matched by the Jsonpathaction ). For add category : The first key is the new category_ID . The second key is the category_description . The third key is the new category_score and compound . The fourth key is the new category_frequency . For modify , only one key is used, the latter being the new value. For modify regex , two keys are used: The first key is the regex to apply on the value. The second key is the replace string, it supports backrefs with $ for the matched groups.","title":"JsonPlug"},{"location":"scripting/oncategorizer/","text":"onCategorizer introduction Manipulating categorization results via script Scripting can be used to post-process the categorization output. Manipulating categorization output allows refining results in a way that rules alone cannot achieve. For example omit the child domains of domains already present in the results, transfer the score from one domain to another or, more simply, return only domains with significant scores. Manipulation must take place in the onCategorizer event handler function, which is executed when the Categorizer event is fired, that is after the evaluation of categorization rules. function onCategorizer () { // Put here statements that change categorization results } Sample taxonomy In this section of the book dedicated to the manipulation of categorization results, we will use this taxonomy as a reference for most of the examples\u2014the part in brackets is the domain label: 1 (Sport) 1.01 (martial art) 1.01.1 (aikido) 1.01.2 (judo) 1.01.3 (karate) 1.02 (athletics) 1.03 (baseball) 1.04 (football) 1.05 (gymnastics) 1.05.1 (artistic gymnastics) 1.05.2 (rhythmic gymnastics) 1.06 (american football) 1.07 (golf) 1.08 (hockey) 1.08.1 (ice hockey) 1.08.2 (field hockey) 1.08.3 (roller hockey) 1.09 (horse racing) 1.10 (swimming) 1.11 (volleyball) 1.12 (basketball) 1.13 (handball) 1.14 (water polo) 1.15 (rugby) 1.16 (fencing) 1.17 (skiing) 1.18 (tennis) The hidden results table You can think of the initial categorization results\u2014the outcome of the activation of categorization rules\u2014as a table like the one shown below. Domain ID Domain label Score Compound score Frequency 1 Sport 90 90 52.94% 1.01 martial art 60 60 35.29% 1.07 golf 10 10 5.88% 1.15 rugby 10 10 5.88% Score is the sum of the points that the rules being activated by the text have assigned to the domain. Unless the CHILD_TO_FATHER option is set, Compound score is a copy of Score . Both scores are influenced by the CHILD_TO_FATHER option. Frequency is a function of the score and it's computed as follows: and expressed as a percentage. So for example, the frequency of category 1 is: Final output and the influence of onCategorizer The final output of the text intelligence engine is a list of \"winner\" results. If the onCategorizer function is not defined, all the results in the hidden table will be considered \"winners\" and thus returned as output. If instead the function is defined, the engine will look at the predefined set named WINNERS to determine \"winning\" results. The set is initially empty: if the function's code doesn't populate it, there will be no winners and the engine will not return any results. In other words, if onCategorizer is defined, results present in the hidden table will all be considered \"losers\", unless they are referenced in the WINNERS set. \"Losing\" results are not returned in output, but are still visible, with appropriate settings, in the Studio development environment. Flow If you want to manipulate categorization results, define the onCategorizer function and put in it scripting code that: Copies the contents of the predefined set ALL in a user-defined set . Manipulates the user-defined set, possibly using other user-defined sets, with specific functions \u2014filters, intersections, deletions, score transformations, etc. Copies the results of the manipulation in the WINNERS predefined set.","title":"onCategorizer introduction"},{"location":"scripting/oncategorizer/#oncategorizer-introduction","text":"","title":"onCategorizer introduction"},{"location":"scripting/oncategorizer/#manipulating-categorization-results-via-script","text":"Scripting can be used to post-process the categorization output. Manipulating categorization output allows refining results in a way that rules alone cannot achieve. For example omit the child domains of domains already present in the results, transfer the score from one domain to another or, more simply, return only domains with significant scores. Manipulation must take place in the onCategorizer event handler function, which is executed when the Categorizer event is fired, that is after the evaluation of categorization rules. function onCategorizer () { // Put here statements that change categorization results }","title":"Manipulating categorization results via script"},{"location":"scripting/oncategorizer/#sample-taxonomy","text":"In this section of the book dedicated to the manipulation of categorization results, we will use this taxonomy as a reference for most of the examples\u2014the part in brackets is the domain label: 1 (Sport) 1.01 (martial art) 1.01.1 (aikido) 1.01.2 (judo) 1.01.3 (karate) 1.02 (athletics) 1.03 (baseball) 1.04 (football) 1.05 (gymnastics) 1.05.1 (artistic gymnastics) 1.05.2 (rhythmic gymnastics) 1.06 (american football) 1.07 (golf) 1.08 (hockey) 1.08.1 (ice hockey) 1.08.2 (field hockey) 1.08.3 (roller hockey) 1.09 (horse racing) 1.10 (swimming) 1.11 (volleyball) 1.12 (basketball) 1.13 (handball) 1.14 (water polo) 1.15 (rugby) 1.16 (fencing) 1.17 (skiing) 1.18 (tennis)","title":"Sample taxonomy"},{"location":"scripting/oncategorizer/#the-hidden-results-table","text":"You can think of the initial categorization results\u2014the outcome of the activation of categorization rules\u2014as a table like the one shown below. Domain ID Domain label Score Compound score Frequency 1 Sport 90 90 52.94% 1.01 martial art 60 60 35.29% 1.07 golf 10 10 5.88% 1.15 rugby 10 10 5.88% Score is the sum of the points that the rules being activated by the text have assigned to the domain. Unless the CHILD_TO_FATHER option is set, Compound score is a copy of Score . Both scores are influenced by the CHILD_TO_FATHER option. Frequency is a function of the score and it's computed as follows: and expressed as a percentage. So for example, the frequency of category 1 is:","title":"The hidden results table"},{"location":"scripting/oncategorizer/#final-output-and-the-influence-of-oncategorizer","text":"The final output of the text intelligence engine is a list of \"winner\" results. If the onCategorizer function is not defined, all the results in the hidden table will be considered \"winners\" and thus returned as output. If instead the function is defined, the engine will look at the predefined set named WINNERS to determine \"winning\" results. The set is initially empty: if the function's code doesn't populate it, there will be no winners and the engine will not return any results. In other words, if onCategorizer is defined, results present in the hidden table will all be considered \"losers\", unless they are referenced in the WINNERS set. \"Losing\" results are not returned in output, but are still visible, with appropriate settings, in the Studio development environment.","title":"Final output and the influence of onCategorizer"},{"location":"scripting/oncategorizer/#flow","text":"If you want to manipulate categorization results, define the onCategorizer function and put in it scripting code that: Copies the contents of the predefined set ALL in a user-defined set . Manipulates the user-defined set, possibly using other user-defined sets, with specific functions \u2014filters, intersections, deletions, score transformations, etc. Copies the results of the manipulation in the WINNERS predefined set.","title":"Flow"},{"location":"scripting/oncategorizer/functions/","text":"Functions introduction Set functions are used to inspect and manipulate sets of domains. Some functions return new sets, others return Boolean values. For example, the CLONE function returns a new set that is an exact copy of an existing set: var copyOfAll ; copyOfAll = CLONE ( ALL ); Meanwhile, the EMPTY function checks if a set is empty, returning a Boolean value. var copyOfAll ; copyOfAll = CLONE ( ALL ); if ( EMPTY ( copyOfAll )) { ... } Set functions can be used: To give a value to set variables. As arguments of other functions. In complex expressions. The available functions are listed in the following table and are described in the next pages of this section. Function Use CLAN Get domains from the project's taxonomy CLEAN Filter domains based on their score CLONE Copy a set DIFFERENCE Subtract a set from another DIMENSIONS Check the size of a set EMPTY Check if a set is empty EXIST Check if a set contains a domain EXTRACT Check if a set contains domains in a list EXTRACT2 Get a portion of a set based on position FAMILY Get a portion of a set based on project's taxonomy FILTER Get a portion of a set based on relative ranking FIRST Get the top ranking domain INSERT Add domains to a set INTERSECTION Get the domains that two sets have in common MULTIPLY Change the score of domains RATIO Compare the score of two domains or two sets RATIO2 Compare the score of a domain or set with a fraction of the score of another domain/set REMOVE Remove a domain from a set SCORED Get a domain based on its ranking SECOND Return the domain with the second highest score SET Create a new set from scratch THRESHOLD Filter set domains based on a comparison with the score of a given domain THRESHOLD2 Filter set domains based on a comparison with a fraction of the score of a given domain TRANSFORM Transfer scores between domains UNION Add a set to another","title":"Functions introduction"},{"location":"scripting/oncategorizer/functions/#functions-introduction","text":"Set functions are used to inspect and manipulate sets of domains. Some functions return new sets, others return Boolean values. For example, the CLONE function returns a new set that is an exact copy of an existing set: var copyOfAll ; copyOfAll = CLONE ( ALL ); Meanwhile, the EMPTY function checks if a set is empty, returning a Boolean value. var copyOfAll ; copyOfAll = CLONE ( ALL ); if ( EMPTY ( copyOfAll )) { ... } Set functions can be used: To give a value to set variables. As arguments of other functions. In complex expressions. The available functions are listed in the following table and are described in the next pages of this section. Function Use CLAN Get domains from the project's taxonomy CLEAN Filter domains based on their score CLONE Copy a set DIFFERENCE Subtract a set from another DIMENSIONS Check the size of a set EMPTY Check if a set is empty EXIST Check if a set contains a domain EXTRACT Check if a set contains domains in a list EXTRACT2 Get a portion of a set based on position FAMILY Get a portion of a set based on project's taxonomy FILTER Get a portion of a set based on relative ranking FIRST Get the top ranking domain INSERT Add domains to a set INTERSECTION Get the domains that two sets have in common MULTIPLY Change the score of domains RATIO Compare the score of two domains or two sets RATIO2 Compare the score of a domain or set with a fraction of the score of another domain/set REMOVE Remove a domain from a set SCORED Get a domain based on its ranking SECOND Return the domain with the second highest score SET Create a new set from scratch THRESHOLD Filter set domains based on a comparison with the score of a given domain THRESHOLD2 Filter set domains based on a comparison with a fraction of the score of a given domain TRANSFORM Transfer scores between domains UNION Add a set to another","title":"Functions introduction"},{"location":"scripting/oncategorizer/functions/clan/","text":"CLAN The CLAN function returns a new set containing the IDs of the domains that, in the project taxonomy, are in a given hierarchical relationship with a given domain. In the examples below, domain labels are also shown, in brackets, to facilitate understanding. For example, given the sample project taxonomy , consider this code: var sportChildren = CLAN ( SON , \"1.01\" ); var judoFather = CLAN ( FATHER , \"1.01.2\" ); The first instruction defines and populates the sportChildren with all the children of the 1.01 ( martial art ) domain, that is: 1.01.1 (aikido) 1.01.2 (judo) 1.01.3 (karate) The second instruction defines and populates the judoFather set containing the father of the 1.01.2 ( judo ) domain, that is: 1.01 (martial art) The syntax is: CLAN(relationship, domain) domain is a domain ID, relationship is the kinship between that domain and the other taxonomy domains. relationship values are described below. Relationship Description SON Select all the children of the given domain. FATHER Select the father of the given domain (if it exists). ANCESTOR Select all the ancestors\u2014for example father, grandfather, great-grandfather\u2014of the given domain. DESCENDANT Select all children, grandchildren and lower level descendants of the given domain. SIBLING Select the siblings of the given domain; \"founder\" domains (see the FOUNDER relationship below) are not considered siblings even though they all share the same level. RELATIVE Select all the all the descendants of the domain's founder. FOUNDER Select only the \"founder\" of the given domain, that is the highest-level ancestor.","title":"CLAN"},{"location":"scripting/oncategorizer/functions/clan/#clan","text":"The CLAN function returns a new set containing the IDs of the domains that, in the project taxonomy, are in a given hierarchical relationship with a given domain. In the examples below, domain labels are also shown, in brackets, to facilitate understanding. For example, given the sample project taxonomy , consider this code: var sportChildren = CLAN ( SON , \"1.01\" ); var judoFather = CLAN ( FATHER , \"1.01.2\" ); The first instruction defines and populates the sportChildren with all the children of the 1.01 ( martial art ) domain, that is: 1.01.1 (aikido) 1.01.2 (judo) 1.01.3 (karate) The second instruction defines and populates the judoFather set containing the father of the 1.01.2 ( judo ) domain, that is: 1.01 (martial art) The syntax is: CLAN(relationship, domain) domain is a domain ID, relationship is the kinship between that domain and the other taxonomy domains. relationship values are described below. Relationship Description SON Select all the children of the given domain. FATHER Select the father of the given domain (if it exists). ANCESTOR Select all the ancestors\u2014for example father, grandfather, great-grandfather\u2014of the given domain. DESCENDANT Select all children, grandchildren and lower level descendants of the given domain. SIBLING Select the siblings of the given domain; \"founder\" domains (see the FOUNDER relationship below) are not considered siblings even though they all share the same level. RELATIVE Select all the all the descendants of the domain's founder. FOUNDER Select only the \"founder\" of the given domain, that is the highest-level ancestor.","title":"CLAN"},{"location":"scripting/oncategorizer/functions/clean/","text":"CLEAN The CLEAN function returns a new set containing the domains of an existing set having a score higher than a specified value. For example, if a set variable workingSet has these contents (domain labels are also shown, in brackets, to facilitate understanding): 1.01 (martial art) 1.01.1 (aikido) 1.07 (golf) and the hidden results table is the one shown in the onCategorizer introduction , then this statement: var highScore = CLEAN ( workingSet , 50 ); will define and populate the highScore set with: 1.01 (martial art) because domain 1.01 is the only member of set workingSet having a score higher than 50. Domain 1.07 ( golf ), in fact, has a lower score (10) and 1.01.1 ( aikido ) has no score since it doesn't appear in the hidden results table . Since the only domains with a score are those listed in the hidden results table , only a domain with a matching entry in that table can be returned. The syntax is: CLEAN(set, value) where set is a set variable and value is a number.","title":"CLEAN"},{"location":"scripting/oncategorizer/functions/clean/#clean","text":"The CLEAN function returns a new set containing the domains of an existing set having a score higher than a specified value. For example, if a set variable workingSet has these contents (domain labels are also shown, in brackets, to facilitate understanding): 1.01 (martial art) 1.01.1 (aikido) 1.07 (golf) and the hidden results table is the one shown in the onCategorizer introduction , then this statement: var highScore = CLEAN ( workingSet , 50 ); will define and populate the highScore set with: 1.01 (martial art) because domain 1.01 is the only member of set workingSet having a score higher than 50. Domain 1.07 ( golf ), in fact, has a lower score (10) and 1.01.1 ( aikido ) has no score since it doesn't appear in the hidden results table . Since the only domains with a score are those listed in the hidden results table , only a domain with a matching entry in that table can be returned. The syntax is: CLEAN(set, value) where set is a set variable and value is a number.","title":"CLEAN"},{"location":"scripting/oncategorizer/functions/clone/","text":"CLONE The CLONE function returns a new set that is an exact copy of an existing set. For example, this statement: workingSet = CLONE ( ALL ); defines and populates workingSet with a copy of the contents of the predefined set ALL . The syntax is: CLONE(set) where set is a set variable. Note In the manipulation flow , CLONE is used both in the first instruction\u2014to create a working copy of the ALL set\u2014and in the last\u2014to copy the contents of a working set in the WINNERS set.","title":"CLONE"},{"location":"scripting/oncategorizer/functions/clone/#clone","text":"The CLONE function returns a new set that is an exact copy of an existing set. For example, this statement: workingSet = CLONE ( ALL ); defines and populates workingSet with a copy of the contents of the predefined set ALL . The syntax is: CLONE(set) where set is a set variable. Note In the manipulation flow , CLONE is used both in the first instruction\u2014to create a working copy of the ALL set\u2014and in the last\u2014to copy the contents of a working set in the WINNERS set.","title":"CLONE"},{"location":"scripting/oncategorizer/functions/difference/","text":"DIFFERENCE Given two sets A and B , the DIFFERENCE function returns a new \" A minus B \" set, that is what is left of A after removing the domains that A and B have in common. For example, given these two sets: setOne: 1 1.01 1.15 1.17 setTwo: 1.01 1.15 1.18 the statement: var diff = DIFFERENCE ( setOne , setTwo ); defines and populates the diff set this way: 1 1.17 The members of setTwo are subtracted from setOne . Domain 1.18 does not belong to setOne , so it cannot be subtracted. The syntax is: DIFFERENCE(set1, set2) where set1 and set2 are set variables.","title":"DIFFERENCE"},{"location":"scripting/oncategorizer/functions/difference/#difference","text":"Given two sets A and B , the DIFFERENCE function returns a new \" A minus B \" set, that is what is left of A after removing the domains that A and B have in common. For example, given these two sets: setOne: 1 1.01 1.15 1.17 setTwo: 1.01 1.15 1.18 the statement: var diff = DIFFERENCE ( setOne , setTwo ); defines and populates the diff set this way: 1 1.17 The members of setTwo are subtracted from setOne . Domain 1.18 does not belong to setOne , so it cannot be subtracted. The syntax is: DIFFERENCE(set1, set2) where set1 and set2 are set variables.","title":"DIFFERENCE"},{"location":"scripting/oncategorizer/functions/dimensions/","text":"DIMENSIONS The DIMENSIONS function returns the Boolean result of a condition applied to the size of a set. For example, in this snippet: var workingSet = CLONE ( ALL ); if ( DIMENSIONS ( workingSet , \">\" , 2 )) { ... } the DIMENSIONS function returns True if workingSet contains more than two domains. The syntax is: DIMENSIONS(set, operator, number) where set is a set variable, number is an integer number and operator is one of the following comparison operators: Operator Description = Equal <> Not equal > Greater than >= Greater than or equal < Less than <= Less than or equal The operator must be enclosed in quotation marks.","title":"DIMENSIONS"},{"location":"scripting/oncategorizer/functions/dimensions/#dimensions","text":"The DIMENSIONS function returns the Boolean result of a condition applied to the size of a set. For example, in this snippet: var workingSet = CLONE ( ALL ); if ( DIMENSIONS ( workingSet , \">\" , 2 )) { ... } the DIMENSIONS function returns True if workingSet contains more than two domains. The syntax is: DIMENSIONS(set, operator, number) where set is a set variable, number is an integer number and operator is one of the following comparison operators: Operator Description = Equal <> Not equal > Greater than >= Greater than or equal < Less than <= Less than or equal The operator must be enclosed in quotation marks.","title":"DIMENSIONS"},{"location":"scripting/oncategorizer/functions/empty/","text":"EMPTY The EMPTY function checks the contents of a set and returns a Boolean True if the set in empty, False otherwise. For example, this snippet: if ( EMPTY ( setOne )) { INSERT ( setOne , \"1.07\" ); } adds domain 1.07 to setOne if it's empty. The syntax is: EMPTY(set) where set is a set variable.","title":"EMPTY"},{"location":"scripting/oncategorizer/functions/empty/#empty","text":"The EMPTY function checks the contents of a set and returns a Boolean True if the set in empty, False otherwise. For example, this snippet: if ( EMPTY ( setOne )) { INSERT ( setOne , \"1.07\" ); } adds domain 1.07 to setOne if it's empty. The syntax is: EMPTY(set) where set is a set variable.","title":"EMPTY"},{"location":"scripting/oncategorizer/functions/exist/","text":"EXIST The EXIST function checks the contents of a set and returns a Boolean True if a given domain is contained in the set, a False otherwise. For example, this snippet: if ( EXIST ( workingSet , \"1.01\" )) { INSERT ( workingSet , [ \"1.01.1\" , \"1.01.2\" , \"1.01.3\" ]); } adds domains 1.01.1 , 1.01.2 and 1.01.3 to workingSet if it already contains domain 1.01 . The syntax is: EXIST(set, domain) where set is a set variable and domain is a domain ID enclosed in quotation marks.","title":"EXIST"},{"location":"scripting/oncategorizer/functions/exist/#exist","text":"The EXIST function checks the contents of a set and returns a Boolean True if a given domain is contained in the set, a False otherwise. For example, this snippet: if ( EXIST ( workingSet , \"1.01\" )) { INSERT ( workingSet , [ \"1.01.1\" , \"1.01.2\" , \"1.01.3\" ]); } adds domains 1.01.1 , 1.01.2 and 1.01.3 to workingSet if it already contains domain 1.01 . The syntax is: EXIST(set, domain) where set is a set variable and domain is a domain ID enclosed in quotation marks.","title":"EXIST"},{"location":"scripting/oncategorizer/functions/extract/","text":"EXTRACT and EXTRACT2 EXTRACT The EXTRACT function returns a new set containing the domains of an existing set matching a list of domains. For example, if workingSet has these contents: 1 1.01 1.15 1.17 1.18 this statement: var checked = EXTRACT ( workingSet , [ \"1.01\" , \"1.02\" ]); will define and populate the checked set in this way: 1.01 because 1.01 is the only domain of the list on the right that is present in workingSet . The syntax is: EXTRACT ( set , domain ) Or: EXTRACT ( set , [ domains ]) where set is a set variable, domain is a domain ID and domains is a comma-separated list of domain IDs. Each domain name must be enclosed in quotation marks. domain or domains are searched for in set and the function returns a new set containing the domains that are found. EXTRACT2 The EXTRACT2 function returns a new set containing a given consecutive domains numbers included in it starting from a certain position in the ranking. For example, if workingSet has these contents: 1 1.01 1.15 1.17 1.18 this statement: var topTwo = EXTRACT2 ( workingSet , 2 , 1 ); will define and populate the topTwo set in this way: 1 1.01 because 1 occupies position one (third argument of the function) inside workingSet , 1.01 is next to it and two (second argument of the functions) domains are required. Domain positions in a set EXTRACT2 is the only set function that takes into account the order of domains and their respective rankings from highest to lowest scoring within a set. All other functions are indifferent to these features. In fact, domains inside a set have positions that derive from the way the set is populated. The domains inside the predefined set ALL have positions reflecting their score, ranked from highest to lowest, inside the hidden results table . Because the domains are listed depending on their scores, as the scores decrease, the number associated with the position will increase. Note that, domains that receive the same score will have consecutive positions, but it will not be known in advance which domain will come first. When the CLONE function is used to create an exact copy of a set, the position of the domains inside the copy will be the same as the original set. When a new set is created with the SET function, domains inside the resulting set have positions reflecting the order in which they are listed in the list argument of the SET call. Domains added with the INSERT function are appended to the destination set, so they occupy the last positions inside it. Syntax The EXTRACT2 syntax is: EXTRACT2(set, count, position) where set is a set variable, count is the number of consecutive domains to consider and position is the position of the first domain to consider.","title":"EXTRACT and EXTRACT2"},{"location":"scripting/oncategorizer/functions/extract/#extract-and-extract2","text":"","title":"EXTRACT and EXTRACT2"},{"location":"scripting/oncategorizer/functions/extract/#extract","text":"The EXTRACT function returns a new set containing the domains of an existing set matching a list of domains. For example, if workingSet has these contents: 1 1.01 1.15 1.17 1.18 this statement: var checked = EXTRACT ( workingSet , [ \"1.01\" , \"1.02\" ]); will define and populate the checked set in this way: 1.01 because 1.01 is the only domain of the list on the right that is present in workingSet . The syntax is: EXTRACT ( set , domain ) Or: EXTRACT ( set , [ domains ]) where set is a set variable, domain is a domain ID and domains is a comma-separated list of domain IDs. Each domain name must be enclosed in quotation marks. domain or domains are searched for in set and the function returns a new set containing the domains that are found.","title":"EXTRACT"},{"location":"scripting/oncategorizer/functions/extract/#extract2","text":"The EXTRACT2 function returns a new set containing a given consecutive domains numbers included in it starting from a certain position in the ranking. For example, if workingSet has these contents: 1 1.01 1.15 1.17 1.18 this statement: var topTwo = EXTRACT2 ( workingSet , 2 , 1 ); will define and populate the topTwo set in this way: 1 1.01 because 1 occupies position one (third argument of the function) inside workingSet , 1.01 is next to it and two (second argument of the functions) domains are required.","title":"EXTRACT2"},{"location":"scripting/oncategorizer/functions/extract/#domain-positions-in-a-set","text":"EXTRACT2 is the only set function that takes into account the order of domains and their respective rankings from highest to lowest scoring within a set. All other functions are indifferent to these features. In fact, domains inside a set have positions that derive from the way the set is populated. The domains inside the predefined set ALL have positions reflecting their score, ranked from highest to lowest, inside the hidden results table . Because the domains are listed depending on their scores, as the scores decrease, the number associated with the position will increase. Note that, domains that receive the same score will have consecutive positions, but it will not be known in advance which domain will come first. When the CLONE function is used to create an exact copy of a set, the position of the domains inside the copy will be the same as the original set. When a new set is created with the SET function, domains inside the resulting set have positions reflecting the order in which they are listed in the list argument of the SET call. Domains added with the INSERT function are appended to the destination set, so they occupy the last positions inside it.","title":"Domain positions in a set"},{"location":"scripting/oncategorizer/functions/extract/#syntax","text":"The EXTRACT2 syntax is: EXTRACT2(set, count, position) where set is a set variable, count is the number of consecutive domains to consider and position is the position of the first domain to consider.","title":"Syntax"},{"location":"scripting/oncategorizer/functions/family/","text":"FAMILY The FAMILY function returns a new set containing the domains of an existing set that, in the project taxonomy, are descendants of a given domain. For example, given the sample project taxonomy , consider this snippet: var someSports = SET ([ \"1.05\" , \"1.07\" , \"1.08.1\" , \"1.08.3\" ]) var hockeyChildren = FAMILY ( someSports , \"1.08\" ) The second statement defines and populates the hockeyChildren set with all the domains of mySet that, in the project taxonomy, descend from domain 1.08 , so the contents of hockeyChildren will be: 1.08.1 (ice hockey) 1.08.3 (roller hockey) The syntax is: FAMILY(set, domain) where set is a set variable and domain is a domain ID enclosed in quotation marks.","title":"FAMILY"},{"location":"scripting/oncategorizer/functions/family/#family","text":"The FAMILY function returns a new set containing the domains of an existing set that, in the project taxonomy, are descendants of a given domain. For example, given the sample project taxonomy , consider this snippet: var someSports = SET ([ \"1.05\" , \"1.07\" , \"1.08.1\" , \"1.08.3\" ]) var hockeyChildren = FAMILY ( someSports , \"1.08\" ) The second statement defines and populates the hockeyChildren set with all the domains of mySet that, in the project taxonomy, descend from domain 1.08 , so the contents of hockeyChildren will be: 1.08.1 (ice hockey) 1.08.3 (roller hockey) The syntax is: FAMILY(set, domain) where set is a set variable and domain is a domain ID enclosed in quotation marks.","title":"FAMILY"},{"location":"scripting/oncategorizer/functions/filter/","text":"FILTER The FILTER function returns a new set containing the domains of an existing set that have a score higher than a fraction of that of other domains in the same set. In the examples below domain labels are also shown, in brackets, to facilitate understanding. For example, if the hidden results table is the one in the introduction , this snippet: var workingSet = SET ([ \"1.07\" , \"1.18\" , \"1.01\" ]); var mySet = FILTER ( workingSet , [ 16 ]); will define and populate mySet this way: 1.01 (martial art) 1.07 (golf) The reason is that the function considers the highest score attributed to domains of the set, that is 60, attributed to domain 1.01 , and returns all domains of the set whose score is greater than 16% of that score. This obviously includes domain 1.01 , which has the highest score, but also domain 1.07 , since 16% of 60 is 9.6 and domain 1.07 has a score of 10, which is above this threshold. Domain 1.18 is not returned, because it has no score, so it cannot be considered. In this other example: var workingSet = SET ([ \"1.07\" , \"1.18\" , \"1.01\" , \"1\" ]); var mySet = FILTER ( workingSet , [ 50 , 16 ]); the function returns: 1 (Sport) 1.01 (martial art) 1.07 (golf) because it considers the first number as a fraction of the score of the highest ranking domain and the second as a fraction of the lowest score relative to the domains selected by the first fraction. Thus, if the first fraction determines the return of 1 and 1.01 (since both domains have a score exceeding the 50% of the highest score, which is 90), the second, calculated with respect to the lowest score of the domains determined by the first, will cause the return of 1.07 because its score (10) is greater than 16% of the score of 1.07 (60). Since the only domains with a score are those listed in the hidden results table , only domains with a matching entry in that table can be returned. The syntax is: FILTER(set, [fractions]) where set is a set variable and fractions is a comma-separated list of integer or decimal numbers representing percentages. Fractions must be interpreted as explained above.","title":"FILTER"},{"location":"scripting/oncategorizer/functions/filter/#filter","text":"The FILTER function returns a new set containing the domains of an existing set that have a score higher than a fraction of that of other domains in the same set. In the examples below domain labels are also shown, in brackets, to facilitate understanding. For example, if the hidden results table is the one in the introduction , this snippet: var workingSet = SET ([ \"1.07\" , \"1.18\" , \"1.01\" ]); var mySet = FILTER ( workingSet , [ 16 ]); will define and populate mySet this way: 1.01 (martial art) 1.07 (golf) The reason is that the function considers the highest score attributed to domains of the set, that is 60, attributed to domain 1.01 , and returns all domains of the set whose score is greater than 16% of that score. This obviously includes domain 1.01 , which has the highest score, but also domain 1.07 , since 16% of 60 is 9.6 and domain 1.07 has a score of 10, which is above this threshold. Domain 1.18 is not returned, because it has no score, so it cannot be considered. In this other example: var workingSet = SET ([ \"1.07\" , \"1.18\" , \"1.01\" , \"1\" ]); var mySet = FILTER ( workingSet , [ 50 , 16 ]); the function returns: 1 (Sport) 1.01 (martial art) 1.07 (golf) because it considers the first number as a fraction of the score of the highest ranking domain and the second as a fraction of the lowest score relative to the domains selected by the first fraction. Thus, if the first fraction determines the return of 1 and 1.01 (since both domains have a score exceeding the 50% of the highest score, which is 90), the second, calculated with respect to the lowest score of the domains determined by the first, will cause the return of 1.07 because its score (10) is greater than 16% of the score of 1.07 (60). Since the only domains with a score are those listed in the hidden results table , only domains with a matching entry in that table can be returned. The syntax is: FILTER(set, [fractions]) where set is a set variable and fractions is a comma-separated list of integer or decimal numbers representing percentages. Fractions must be interpreted as explained above.","title":"FILTER"},{"location":"scripting/oncategorizer/functions/first/","text":"FIRST The FIRST function returns a new set containing the domains of an existing set having the highest score. In the examples below domain labels are also shown, in brackets, to facilitate understanding. For example, if the hidden results table is the one in the introduction , this snippet: var workingSet = SET ([ \"1.07\" , \"1.18\" , \"1.01\" ]); var mySet = FIRST ( workingSet ); will define and populate mySet this way: 1.01 (martial art) In this other example: var workingSet = SET ([ \"1.16\" , \"1.17\" , \"1.18\" ]); var mySet = FIRST ( workingSet ); the function returns an empty set because none of the workingSet domains have a score. Since the only domains with a score are those listed in the hidden results table , only a domain with a matching entry in that table can be returned. The syntax is: FIRST(set) where set is a set variable.","title":"FIRST"},{"location":"scripting/oncategorizer/functions/first/#first","text":"The FIRST function returns a new set containing the domains of an existing set having the highest score. In the examples below domain labels are also shown, in brackets, to facilitate understanding. For example, if the hidden results table is the one in the introduction , this snippet: var workingSet = SET ([ \"1.07\" , \"1.18\" , \"1.01\" ]); var mySet = FIRST ( workingSet ); will define and populate mySet this way: 1.01 (martial art) In this other example: var workingSet = SET ([ \"1.16\" , \"1.17\" , \"1.18\" ]); var mySet = FIRST ( workingSet ); the function returns an empty set because none of the workingSet domains have a score. Since the only domains with a score are those listed in the hidden results table , only a domain with a matching entry in that table can be returned. The syntax is: FIRST(set) where set is a set variable.","title":"FIRST"},{"location":"scripting/oncategorizer/functions/insert/","text":"INSERT The INSERT function adds domains to a specified set and also returns a new set that is a copy of the resulting set. For example, in this snippet: INSERT ( setOne , \"1.01\" ); setTwoPlus = INSERT ( setTwo , [ \"20000226\" , \"15000000\" ]); the first statement adds the 1.01 domain to setOne . The second statement adds domains 20000226 and 15000000 to setTwo , then it defines and populates setTwoPlus with the contents of setTwo . The syntax is: INSERT(set, domain) Or: INSERT(set, [domains]) where set is a set variable, domain is a domain ID and domains is a comma-separated list of domain IDs. Each domain ID must be enclosed in quotation marks.","title":"INSERT"},{"location":"scripting/oncategorizer/functions/insert/#insert","text":"The INSERT function adds domains to a specified set and also returns a new set that is a copy of the resulting set. For example, in this snippet: INSERT ( setOne , \"1.01\" ); setTwoPlus = INSERT ( setTwo , [ \"20000226\" , \"15000000\" ]); the first statement adds the 1.01 domain to setOne . The second statement adds domains 20000226 and 15000000 to setTwo , then it defines and populates setTwoPlus with the contents of setTwo . The syntax is: INSERT(set, domain) Or: INSERT(set, [domains]) where set is a set variable, domain is a domain ID and domains is a comma-separated list of domain IDs. Each domain ID must be enclosed in quotation marks.","title":"INSERT"},{"location":"scripting/oncategorizer/functions/intersection/","text":"INTERSECTION Given two sets A and B , the INTERSECTION function returns a new set containing the domains that A and B have in common. For example, given these two sets: setOne: 1 1.01 1.15 1.17 setTwo: 1.01 1.15 1.18 the statement: var common = INTERSECTION ( setOne , setTwo ); defines and populates the common sets this way: 1.01 1.15 because domains 1.01 and 1.15 are the only that belong both to setOne and setTwo . The syntax is: INTERSECTION(set1, set2) where set1 and set2 are set variables.","title":"INTERSECTION"},{"location":"scripting/oncategorizer/functions/intersection/#intersection","text":"Given two sets A and B , the INTERSECTION function returns a new set containing the domains that A and B have in common. For example, given these two sets: setOne: 1 1.01 1.15 1.17 setTwo: 1.01 1.15 1.18 the statement: var common = INTERSECTION ( setOne , setTwo ); defines and populates the common sets this way: 1.01 1.15 because domains 1.01 and 1.15 are the only that belong both to setOne and setTwo . The syntax is: INTERSECTION(set1, set2) where set1 and set2 are set variables.","title":"INTERSECTION"},{"location":"scripting/oncategorizer/functions/multiply/","text":"MULTIPLY The MULTIPLY function multiplies the score of a domain in an existing set by a value. The MULTIPLY function changes the value of Score in the hidden results table . For example, if a set variable workingSet has these contents: 1.01 1.01.1 1.07 and the hidden results table is the one in the introduction , then this statement: MULTIPLY ( workingSet , \"1.07\" , 2 ); will multiply by 2 the score of domain 1.07 (10) that will become 20. Since the only domains with a score are those listed in the hidden results table , only domains with a matching entry in that table are affected. The syntax is: MULTIPLY(set, domain, number) where set is a set variable, domain is a domain ID enclosed in quotation marks and number is the multiplying factor. It can be an integer or a decimal number.","title":"MULTIPLY"},{"location":"scripting/oncategorizer/functions/multiply/#multiply","text":"The MULTIPLY function multiplies the score of a domain in an existing set by a value. The MULTIPLY function changes the value of Score in the hidden results table . For example, if a set variable workingSet has these contents: 1.01 1.01.1 1.07 and the hidden results table is the one in the introduction , then this statement: MULTIPLY ( workingSet , \"1.07\" , 2 ); will multiply by 2 the score of domain 1.07 (10) that will become 20. Since the only domains with a score are those listed in the hidden results table , only domains with a matching entry in that table are affected. The syntax is: MULTIPLY(set, domain, number) where set is a set variable, domain is a domain ID enclosed in quotation marks and number is the multiplying factor. It can be an integer or a decimal number.","title":"MULTIPLY"},{"location":"scripting/oncategorizer/functions/ratio/","text":"RATIO and RATIO2 RATIO The RATIO function compares: The scores of two domains Or: The scores of two sets Or: The score of a domain or set with a given value and returns the result of the comparison as a Boolean value. For sets, the sum of the scores of their domains is considered. For example, this statement: RATIO ( \"1.01\" , \"=\" , 10 ) will return True, if the score of domain 1.01 is equal to 10 . Again for example, this statement: RATIO ( \"1.01\" , \"<\" , \"1\" ) will return True, if the score of domain 1.01 is less than the score of domain 1 . Another example using sets follows. If the categorization results are those listed in the hidden results table and two sets are defined in this way: setOne: 1.01 1.07 1.15 setTwo: 1 then: RATIO ( setOne , \">\" , setTwo ) will return False, because the sum of the scores of the domains in setOne (60 + 10 + 10 = 80) is lower than the sum for setTwo (90). The syntax is: RATIO(domain1, operator, domain2) Or: RATIO(set1, operator, set2) Or: RATIO(domain1, operator, value) Or: RATIO(set1, operator, value) where domain1 and domain2 are domain IDs, set1 and set2 are set variables, value is a number that represents a given score and operator is one of the following operators enclosed in quotation marks: Operator Description = Equal <> Not equal > Greater than >= Greater than or equal < Less than <= Less than or equal RATIO2 The RATIO2 function is like RATIO for comparing two domains or sets, with the addition of a threshold parameter. The score of the first domain or set is not directly compared with the score of the other, but with a given fraction of that score. For example, if the categorization results are those listed in the hidden results table then: RATIO2 ( \"1.01\" , \"<\" , 50 , \"1\" ) will compare the score of domain 1.01 (60) with the 50% of the score of domain 1 (90 / 2 = 45) and thus return False, because 60 is not less than 45. The syntax is: RATIO2(domain1, operator, fraction, domain2); Or: RATIO2(set1, operator, fraction, set2); where domain1 and domain2 are domain IDs, set1 and set2 are set variables, fraction is an integer or decimal number representing a percentage and operator is one of the following operators enclosed in quotation marks: Operator Description = Equal <> Not equal > Greater than >= Greater than or equal < Less than <= Less than or equal","title":"RATIO and RATIO2"},{"location":"scripting/oncategorizer/functions/ratio/#ratio-and-ratio2","text":"","title":"RATIO and RATIO2"},{"location":"scripting/oncategorizer/functions/ratio/#ratio","text":"The RATIO function compares: The scores of two domains Or: The scores of two sets Or: The score of a domain or set with a given value and returns the result of the comparison as a Boolean value. For sets, the sum of the scores of their domains is considered. For example, this statement: RATIO ( \"1.01\" , \"=\" , 10 ) will return True, if the score of domain 1.01 is equal to 10 . Again for example, this statement: RATIO ( \"1.01\" , \"<\" , \"1\" ) will return True, if the score of domain 1.01 is less than the score of domain 1 . Another example using sets follows. If the categorization results are those listed in the hidden results table and two sets are defined in this way: setOne: 1.01 1.07 1.15 setTwo: 1 then: RATIO ( setOne , \">\" , setTwo ) will return False, because the sum of the scores of the domains in setOne (60 + 10 + 10 = 80) is lower than the sum for setTwo (90). The syntax is: RATIO(domain1, operator, domain2) Or: RATIO(set1, operator, set2) Or: RATIO(domain1, operator, value) Or: RATIO(set1, operator, value) where domain1 and domain2 are domain IDs, set1 and set2 are set variables, value is a number that represents a given score and operator is one of the following operators enclosed in quotation marks: Operator Description = Equal <> Not equal > Greater than >= Greater than or equal < Less than <= Less than or equal","title":"RATIO"},{"location":"scripting/oncategorizer/functions/ratio/#ratio2","text":"The RATIO2 function is like RATIO for comparing two domains or sets, with the addition of a threshold parameter. The score of the first domain or set is not directly compared with the score of the other, but with a given fraction of that score. For example, if the categorization results are those listed in the hidden results table then: RATIO2 ( \"1.01\" , \"<\" , 50 , \"1\" ) will compare the score of domain 1.01 (60) with the 50% of the score of domain 1 (90 / 2 = 45) and thus return False, because 60 is not less than 45. The syntax is: RATIO2(domain1, operator, fraction, domain2); Or: RATIO2(set1, operator, fraction, set2); where domain1 and domain2 are domain IDs, set1 and set2 are set variables, fraction is an integer or decimal number representing a percentage and operator is one of the following operators enclosed in quotation marks: Operator Description = Equal <> Not equal > Greater than >= Greater than or equal < Less than <= Less than or equal","title":"RATIO2"},{"location":"scripting/oncategorizer/functions/remove/","text":"REMOVE The REMOVE function removes a domain from an existing set and also returns a new set that is a copy of the resulting set. For example, this statement: var setOneMinus = REMOVE ( setOne , \"1.01\" ); removes the 1.01 domain from setOne , then defines and populates setMinusOne with a copy of setOne . The syntax is: REMOVE(set, domain) where set is a set variable and domain is a domain ID enclosed in quotation marks.","title":"REMOVE"},{"location":"scripting/oncategorizer/functions/remove/#remove","text":"The REMOVE function removes a domain from an existing set and also returns a new set that is a copy of the resulting set. For example, this statement: var setOneMinus = REMOVE ( setOne , \"1.01\" ); removes the 1.01 domain from setOne , then defines and populates setMinusOne with a copy of setOne . The syntax is: REMOVE(set, domain) where set is a set variable and domain is a domain ID enclosed in quotation marks.","title":"REMOVE"},{"location":"scripting/oncategorizer/functions/scored/","text":"SCORED The SCORED function returns a new set containing the domain with a given ranking in an existing set. In the example below domain labels are also shown, in brackets, to facilitate understanding. For example, if the hidden results table is the one in the introduction , this snippet: var workingSet = CLONE ( ALL ); var silverMedal = SCORED ( workingSet , 2 ); will define and populate the silverMedal set this way: 1.01 (martial art) because domain 1.01 is in second place in the ranking. Since the only domains with a score are those listed in the hidden results table , only a domain with a matching entry in that table can be returned. The syntax is: SCORED(set, position) where set is a set variable and position is an integer number that indicates the place in the ranking. Note If the position does not exist, for example you specify the fifth position in a ranking of four elements, the function will return an empty set.","title":"SCORED"},{"location":"scripting/oncategorizer/functions/scored/#scored","text":"The SCORED function returns a new set containing the domain with a given ranking in an existing set. In the example below domain labels are also shown, in brackets, to facilitate understanding. For example, if the hidden results table is the one in the introduction , this snippet: var workingSet = CLONE ( ALL ); var silverMedal = SCORED ( workingSet , 2 ); will define and populate the silverMedal set this way: 1.01 (martial art) because domain 1.01 is in second place in the ranking. Since the only domains with a score are those listed in the hidden results table , only a domain with a matching entry in that table can be returned. The syntax is: SCORED(set, position) where set is a set variable and position is an integer number that indicates the place in the ranking. Note If the position does not exist, for example you specify the fifth position in a ranking of four elements, the function will return an empty set.","title":"SCORED"},{"location":"scripting/oncategorizer/functions/second/","text":"SECOND The SECOND function returns a new set containing the domains of an existing set that has the second highest score. In the examples below domain labels are also shown, in brackets, to facilitate understanding. For example, if the hidden results table is the one in the introduction , this snippet: var workingSet = SET ([ \"1.07\" , \"1.18\" , \"1.01\" ]); var mySet = SECOND ( workingSet ); will define and populate mySet this way: 1.07 (golf) In this other example: var workingSet = SET ([ \"1.16\" , \"1.17\" , \"1.18\" ]); var mySet = SECOND ( workingSet ); the function returns an empty set because none of the workingSet domains have a score. Since the only domains with a score are those listed in the hidden results table , only a domain with a matching entry in that table can be returned. The syntax is: SECOND(set) where set is a set variable.","title":"SECOND"},{"location":"scripting/oncategorizer/functions/second/#second","text":"The SECOND function returns a new set containing the domains of an existing set that has the second highest score. In the examples below domain labels are also shown, in brackets, to facilitate understanding. For example, if the hidden results table is the one in the introduction , this snippet: var workingSet = SET ([ \"1.07\" , \"1.18\" , \"1.01\" ]); var mySet = SECOND ( workingSet ); will define and populate mySet this way: 1.07 (golf) In this other example: var workingSet = SET ([ \"1.16\" , \"1.17\" , \"1.18\" ]); var mySet = SECOND ( workingSet ); the function returns an empty set because none of the workingSet domains have a score. Since the only domains with a score are those listed in the hidden results table , only a domain with a matching entry in that table can be returned. The syntax is: SECOND(set) where set is a set variable.","title":"SECOND"},{"location":"scripting/oncategorizer/functions/set/","text":"SET The SET function returns a new set containing the specified domains. For example, this statement: martialArt = SET ( \"1.01\" ); defines the martialArt set and populates it with domain 1.01 . This other example statement: cinema = SET ([ \"20000226\" , \"15000000\" ]); defines the cinema set and populates it with domains 20000226 and 15000000 . The syntax is: SET(domain) Or: SET([domains]) where domain is a domain ID and domains is a comma-separated list of domain IDs. Each domain name must be enclosed in quotation marks.","title":"SET"},{"location":"scripting/oncategorizer/functions/set/#set","text":"The SET function returns a new set containing the specified domains. For example, this statement: martialArt = SET ( \"1.01\" ); defines the martialArt set and populates it with domain 1.01 . This other example statement: cinema = SET ([ \"20000226\" , \"15000000\" ]); defines the cinema set and populates it with domains 20000226 and 15000000 . The syntax is: SET(domain) Or: SET([domains]) where domain is a domain ID and domains is a comma-separated list of domain IDs. Each domain name must be enclosed in quotation marks.","title":"SET"},{"location":"scripting/oncategorizer/functions/threshold/","text":"THRESHOLD and THRESHOLD2 THRESHOLD and THRESHOLD2 functions return a new set containing the domains of an existing set the scores of which satisfy a given condition with respect to the score of a specific domain. For example, in: var destinationSet = THRESHOLD ( sourceSet , \">=\" , \"1.01\" ); the THRESHOLD function returns the domains of sourceSet whose score is greater than or equal to that of the 1.01 domain, while in: var destinationSet = THRESHOLD2 ( sourceSet , \"<\" , 20 , \"1\" ); the THRESHOLD2 function returns the domains of sourceSet whose score is less than 20% of the score of domain 1 . Since the only domains with a score are those listed in the hidden results table , only domains with a matching entry in that table are considered. The THRESHOLD syntax is: THRESHOLD(set, operator, domain) where set is a set variable, operator is one of the comparison operators listed in the table below and domain is a domain ID. The THRESHOLD2 syntax is: THRESHOLD2(set, operator, fraction, domain) where set is a set variable, operator is one of the comparison operators listed in the table below, fraction is an integer or decimal number representing a percentage and domain is a domain ID. The domain ID and the operator must be enclosed in quotation marks. Operator Description = Equal <> Not equal > Greater than >= Greater than or equal < Less than <= Less than or equal","title":"THRESHOLD and THRESHOLD2"},{"location":"scripting/oncategorizer/functions/threshold/#threshold-and-threshold2","text":"THRESHOLD and THRESHOLD2 functions return a new set containing the domains of an existing set the scores of which satisfy a given condition with respect to the score of a specific domain. For example, in: var destinationSet = THRESHOLD ( sourceSet , \">=\" , \"1.01\" ); the THRESHOLD function returns the domains of sourceSet whose score is greater than or equal to that of the 1.01 domain, while in: var destinationSet = THRESHOLD2 ( sourceSet , \"<\" , 20 , \"1\" ); the THRESHOLD2 function returns the domains of sourceSet whose score is less than 20% of the score of domain 1 . Since the only domains with a score are those listed in the hidden results table , only domains with a matching entry in that table are considered. The THRESHOLD syntax is: THRESHOLD(set, operator, domain) where set is a set variable, operator is one of the comparison operators listed in the table below and domain is a domain ID. The THRESHOLD2 syntax is: THRESHOLD2(set, operator, fraction, domain) where set is a set variable, operator is one of the comparison operators listed in the table below, fraction is an integer or decimal number representing a percentage and domain is a domain ID. The domain ID and the operator must be enclosed in quotation marks. Operator Description = Equal <> Not equal > Greater than >= Greater than or equal < Less than <= Less than or equal","title":"THRESHOLD and THRESHOLD2"},{"location":"scripting/oncategorizer/functions/transform/","text":"TRANSFORM The TRANSFORM function transfers scores from one or more source domains to a destination one. Since the only domains with a score are those listed in the hidden results table , only domains with a matching entry in that table are considered. This function directly changes the scores inside the hidden results table. For example, this statement: TRANSFORM ([ \"1.01\" ], [ \"1.07\" , \"1.15\" ]); transfers the score of domains 1.07 and 1.15 to domain 1.01 . Assuming that before the execution of the instruction the scores were: Domain Score 1.01 60 1.07 10 1.15 10 after the execution of the statement the scores would be: Domain Score 1.01 80 1.07 0 1.15 0 Note Source domains lose their score, it will be zero at the end of the execution. The syntax is: TRANSFORM(domain1, domain2) Or: TRANSFORM(domain, [domains]) Or: TRANSFORM(domain, set) where domain , domain1 and domain2 are domain IDs, domains is a comma-separated list of domain IDs and set is a set variable. Each domain ID must be enclosed in quotation marks. domain2 , domains and set are (or contain) \"source\" domains, domain1 and domain are \"destination\" domains.","title":"TRANSFORM"},{"location":"scripting/oncategorizer/functions/transform/#transform","text":"The TRANSFORM function transfers scores from one or more source domains to a destination one. Since the only domains with a score are those listed in the hidden results table , only domains with a matching entry in that table are considered. This function directly changes the scores inside the hidden results table. For example, this statement: TRANSFORM ([ \"1.01\" ], [ \"1.07\" , \"1.15\" ]); transfers the score of domains 1.07 and 1.15 to domain 1.01 . Assuming that before the execution of the instruction the scores were: Domain Score 1.01 60 1.07 10 1.15 10 after the execution of the statement the scores would be: Domain Score 1.01 80 1.07 0 1.15 0 Note Source domains lose their score, it will be zero at the end of the execution. The syntax is: TRANSFORM(domain1, domain2) Or: TRANSFORM(domain, [domains]) Or: TRANSFORM(domain, set) where domain , domain1 and domain2 are domain IDs, domains is a comma-separated list of domain IDs and set is a set variable. Each domain ID must be enclosed in quotation marks. domain2 , domains and set are (or contain) \"source\" domains, domain1 and domain are \"destination\" domains.","title":"TRANSFORM"},{"location":"scripting/oncategorizer/functions/union/","text":"UNION Given two sets A and B , the UNION function returns a new set \" A -plus- B \", that is all the domains that are only in A plus all the domains that are only in B plus the domains that A and B have in common. For example, given these two sets: setOne: 1 1.01 1.15 1.17 setTwo: 1.01 1.15 1.18 the statement: var union = UNION ( setOne , setTwo ); defines and populates the union set this way: 1 1.01 1.15 1.17 1.18 The syntax is: UNION(set1, set2) where set1 and set2 are set variables.","title":"UNION"},{"location":"scripting/oncategorizer/functions/union/#union","text":"Given two sets A and B , the UNION function returns a new set \" A -plus- B \", that is all the domains that are only in A plus all the domains that are only in B plus the domains that A and B have in common. For example, given these two sets: setOne: 1 1.01 1.15 1.17 setTwo: 1.01 1.15 1.18 the statement: var union = UNION ( setOne , setTwo ); defines and populates the union set this way: 1 1.01 1.15 1.17 1.18 The syntax is: UNION(set1, set2) where set1 and set2 are set variables.","title":"UNION"},{"location":"scripting/oncategorizer/sets/","text":"Sets Sets are special scripting variables suitable for holding domain IDs. They can be predefined or user-defined. In the latter case, they can be created from scratch or derived from existing sets. Predefined sets There are two predefined sets: ALL WINNERS ALL ALL represents the initial categorization results. Its value is the set of IDs of the domains with a score, so its contents correspond to the first column of the hidden results table . In the manipulation flow , ALL is read only once at the beginning of the onCategorizer code. WINNERS WINNERS represents the final results of categorization after manipulation. If WINNERS contains a domain ID which has a corresponding row in the hidden results table , that row of the table will be returned as final result. Domain IDs without a corresponding entry in the hidden results table are ignored. In the manipulation flow , WINNERS is populated only once at the end of the onCategorizer code. User-defined sets User-defined sets are variables the developer uses as \"buckets\" for results manipulation. They must be declared, for example: var workingSet ; var mediaDoms ; They are populated with functions , for example: workingSet = CLONE ( ALL ); mediaDoms = INTERSECTION ( workingSet , mediaTaxonomy );","title":"Sets"},{"location":"scripting/oncategorizer/sets/#sets","text":"Sets are special scripting variables suitable for holding domain IDs. They can be predefined or user-defined. In the latter case, they can be created from scratch or derived from existing sets.","title":"Sets"},{"location":"scripting/oncategorizer/sets/#predefined-sets","text":"There are two predefined sets: ALL WINNERS","title":"Predefined sets"},{"location":"scripting/oncategorizer/sets/#all","text":"ALL represents the initial categorization results. Its value is the set of IDs of the domains with a score, so its contents correspond to the first column of the hidden results table . In the manipulation flow , ALL is read only once at the beginning of the onCategorizer code.","title":"ALL"},{"location":"scripting/oncategorizer/sets/#winners","text":"WINNERS represents the final results of categorization after manipulation. If WINNERS contains a domain ID which has a corresponding row in the hidden results table , that row of the table will be returned as final result. Domain IDs without a corresponding entry in the hidden results table are ignored. In the manipulation flow , WINNERS is populated only once at the end of the onCategorizer code.","title":"WINNERS"},{"location":"scripting/oncategorizer/sets/#user-defined-sets","text":"User-defined sets are variables the developer uses as \"buckets\" for results manipulation. They must be declared, for example: var workingSet ; var mediaDoms ; They are populated with functions , for example: workingSet = CLONE ( ALL ); mediaDoms = INTERSECTION ( workingSet , mediaTaxonomy );","title":"User-defined sets"},{"location":"scripting/onfinalize/","text":"onFinalize introduction The last step of analyzing a document in a text intelligence engine and in Studio is the finalization of the results to be output. After this step it is still possible to intervene on the results with scripting, by writing specific code in the onFinalize event handling function. The result object , which contains the final engine analysis results, is passed to this function as its argument and it can be modified here. function onFinalize ( result ) { // Put here the code to adjust the result object, i.e. the final output. return ( result ); }","title":"onFinalize introduction"},{"location":"scripting/onfinalize/#onfinalize-introduction","text":"The last step of analyzing a document in a text intelligence engine and in Studio is the finalization of the results to be output. After this step it is still possible to intervene on the results with scripting, by writing specific code in the onFinalize event handling function. The result object , which contains the final engine analysis results, is passed to this function as its argument and it can be modified here. function onFinalize ( result ) { // Put here the code to adjust the result object, i.e. the final output. return ( result ); }","title":"onFinalize introduction"},{"location":"scripting/onfinalize/result/","text":"The result object The result object is passed to the onFinalize function as its argument. It contains the results of the analysis together with statistical information. To modify the results, use the categorization and extraction properties of the match_info.rules property. Note In Studio, the serialization of this object is a JSON file that is automatically saved at the end of the analysis process in the gen folder of the project structure . Its name has the following format name: {test file name}.ctx.json .","title":"The result object"},{"location":"scripting/onfinalize/result/#the-result-object","text":"The result object is passed to the onFinalize function as its argument. It contains the results of the analysis together with statistical information. To modify the results, use the categorization and extraction properties of the match_info.rules property. Note In Studio, the serialization of this object is a JSON file that is automatically saved at the end of the analysis process in the gen folder of the project structure . Its name has the following format name: {test file name}.ctx.json .","title":"The result object"},{"location":"scripting/onfinalize/result/categorization/","text":"categorization property result.match_info.rules.categorization is an array containing the results of categorization. Each array item has the following properties: Property Description name Domain ID label Domain label score Domain score compound Domain's compound score, depending on child to father scoring mechanism frequency Domain score computed as a percentage of the sum of the scores of all the domains winner Boolean value that is true, if the domain is considered a winner , false otherwise rules Categorization rules that have been triggered rules is an array. Its items have the following properties: Property Description id Rule ID is a rule identification number created during the project building. It is a compiled rule index of an array where the rules are placed. It changes after every building. label Rule label , if any score Fraction of the domain's score that is due to this rule's activation scope Information about scope where the rule is activated scope is an array. Its items have the following properties: Property Description begin Scope initial position in the text end Scope final position in the text score Fraction of the domain's score that is due to the rule's activation in this scope operands Operands that triggered the rule in this scope operands is an array. Its items have the following properties: Property Description begin Initial text position matched by the operand end Final text position matched by the operand operand Operand ID is an operand identification number created during the project building. For example, consider the following text: How Jack Daniel's Tennessee Whiskey is made By Lutho Pasiya Time of article published Feb 24, 2020 If you are a whiskey lover, you will know that it is a spirit produced from fermented grain and aged in the wood. And a spirit is an alcoholic beverage in which the alcohol content has been increased by distillation. We had a chat with Jack Daniels Master Distiller, Chris Fletcher who shared with us the process of making one of the top-selling whiskey in the world which is Jack Daniel's. and these categorization rules: SCOPE SENTENCE { DOMAIN(08) // Labeled as FOOD in the taxonomy { ANCESTOR(102138622,100011707,103499072)//@SYN: #102138622# [food] //@SYN: #100011707# [restaurant] //@SYN: #103499072# [chef de cuisine] } } SCOPE SENTENCE { DOMAIN(06) // Labeled as SPIRITS in the taxonomy { ANCESTOR(100000154)//@SYN: #100000154# [alcoholic beverage] } } the categorization property has the following JSON serialization: `categorization \": [ { \" name \": \" 06 \", \" label \": \" SPIRITS \", \" score \": 90, \" compound \": 90, \" rank \": 0, \" frequency \": 90.0, \" winner \": true, \" rules \": [ { \" id \": 13, \" label \": \"\", \" score \": 90, \" scope \": [ { \" begin \": 0, \" end \": 42, \" score \": 20, \" operands \": [ { \" operand \": 13, \" begin \": 4, \" end \": 16 } ] }, { \" begin \": 0, \" end \": 42, \" score \": 20, \" operands \": [ { \" operand \": 13, \" begin \": 28, \" end \": 34 } ] }, { \" begin \": 99, \" end \": 211, \" score \": 10, \" operands \": [ { \" operand \": 13, \" begin \": 112, \" end \": 118 } ] }, { \" begin \": 213, \" end \": 314, \" score \": 10, \" operands \": [ { \" operand \": 13, \" begin \": 219, \" end \": 224 } ] }, { \" begin \": 213, \" end \": 314, \" score \": 10, \" operands \": [ { \" operand \": 13, \" begin \": 232, \" end \": 249 } ] }, { \" begin \": 213, \" end \": 314, \" score \": 10, \" operands \": [ { \" operand \": 13, \" begin \": 264, \" end \": 270 } ] }, { \" begin \": 316, \" end \": 488, \" score \": 10, \" operands \": [ { \" operand \": 13, \" begin \": 445, \" end \": 451 } ] } ] } ] }, { \" name \": \" 08 \", \" label \": \" FOOD \", \" score \": 10, \" compound \": 10, \" frequency \": 10.0, \" winner \": true, \" rules \": [ { \" id \": 9, \" label \": \"\", \" score \": 10, \" scope \": [ { \" begin \": 99, \" end \": 211, \" score \": 10, \" operands \": [ { \" operand \": 9, \" begin \": 185, \" end\": 189 } ] } ] } ] } ] In that context, the following code: function onFinalize ( result ) { var count = result . match_info . rules . categorization . length ; var category ; for ( i = 0 ; i < count ; i ++ ) { category = result . match_info . rules . categorization [ i ]; if ( category . label == \"SPIRITS\" ) { category . label = \"NEW SPIRITS\" ; } } return result ; } changes the label of category 06 from SPIRITS to NEW SPIRITS . The figures below show categorization results as they appear in Studio without and with the manipulation. This other example code: function onFinalize ( result ) { var count = result . match_info . rules . categorization . length ; var category ; for ( i = 0 ; i < count ; i ++ ) { category = result . match_info . rules . categorization [ i ]; if ( category . label == \"FOOD\" ) { category . score += 15 ; } } return result ; } adds some point to the score of the category labeled FOOD . The figures below show categorization results as they appear in Studio without and with the manipulation.","title":"categorization property"},{"location":"scripting/onfinalize/result/categorization/#categorization-property","text":"result.match_info.rules.categorization is an array containing the results of categorization. Each array item has the following properties: Property Description name Domain ID label Domain label score Domain score compound Domain's compound score, depending on child to father scoring mechanism frequency Domain score computed as a percentage of the sum of the scores of all the domains winner Boolean value that is true, if the domain is considered a winner , false otherwise rules Categorization rules that have been triggered rules is an array. Its items have the following properties: Property Description id Rule ID is a rule identification number created during the project building. It is a compiled rule index of an array where the rules are placed. It changes after every building. label Rule label , if any score Fraction of the domain's score that is due to this rule's activation scope Information about scope where the rule is activated scope is an array. Its items have the following properties: Property Description begin Scope initial position in the text end Scope final position in the text score Fraction of the domain's score that is due to the rule's activation in this scope operands Operands that triggered the rule in this scope operands is an array. Its items have the following properties: Property Description begin Initial text position matched by the operand end Final text position matched by the operand operand Operand ID is an operand identification number created during the project building. For example, consider the following text: How Jack Daniel's Tennessee Whiskey is made By Lutho Pasiya Time of article published Feb 24, 2020 If you are a whiskey lover, you will know that it is a spirit produced from fermented grain and aged in the wood. And a spirit is an alcoholic beverage in which the alcohol content has been increased by distillation. We had a chat with Jack Daniels Master Distiller, Chris Fletcher who shared with us the process of making one of the top-selling whiskey in the world which is Jack Daniel's. and these categorization rules: SCOPE SENTENCE { DOMAIN(08) // Labeled as FOOD in the taxonomy { ANCESTOR(102138622,100011707,103499072)//@SYN: #102138622# [food] //@SYN: #100011707# [restaurant] //@SYN: #103499072# [chef de cuisine] } } SCOPE SENTENCE { DOMAIN(06) // Labeled as SPIRITS in the taxonomy { ANCESTOR(100000154)//@SYN: #100000154# [alcoholic beverage] } } the categorization property has the following JSON serialization: `categorization \": [ { \" name \": \" 06 \", \" label \": \" SPIRITS \", \" score \": 90, \" compound \": 90, \" rank \": 0, \" frequency \": 90.0, \" winner \": true, \" rules \": [ { \" id \": 13, \" label \": \"\", \" score \": 90, \" scope \": [ { \" begin \": 0, \" end \": 42, \" score \": 20, \" operands \": [ { \" operand \": 13, \" begin \": 4, \" end \": 16 } ] }, { \" begin \": 0, \" end \": 42, \" score \": 20, \" operands \": [ { \" operand \": 13, \" begin \": 28, \" end \": 34 } ] }, { \" begin \": 99, \" end \": 211, \" score \": 10, \" operands \": [ { \" operand \": 13, \" begin \": 112, \" end \": 118 } ] }, { \" begin \": 213, \" end \": 314, \" score \": 10, \" operands \": [ { \" operand \": 13, \" begin \": 219, \" end \": 224 } ] }, { \" begin \": 213, \" end \": 314, \" score \": 10, \" operands \": [ { \" operand \": 13, \" begin \": 232, \" end \": 249 } ] }, { \" begin \": 213, \" end \": 314, \" score \": 10, \" operands \": [ { \" operand \": 13, \" begin \": 264, \" end \": 270 } ] }, { \" begin \": 316, \" end \": 488, \" score \": 10, \" operands \": [ { \" operand \": 13, \" begin \": 445, \" end \": 451 } ] } ] } ] }, { \" name \": \" 08 \", \" label \": \" FOOD \", \" score \": 10, \" compound \": 10, \" frequency \": 10.0, \" winner \": true, \" rules \": [ { \" id \": 9, \" label \": \"\", \" score \": 10, \" scope \": [ { \" begin \": 99, \" end \": 211, \" score \": 10, \" operands \": [ { \" operand \": 9, \" begin \": 185, \" end\": 189 } ] } ] } ] } ] In that context, the following code: function onFinalize ( result ) { var count = result . match_info . rules . categorization . length ; var category ; for ( i = 0 ; i < count ; i ++ ) { category = result . match_info . rules . categorization [ i ]; if ( category . label == \"SPIRITS\" ) { category . label = \"NEW SPIRITS\" ; } } return result ; } changes the label of category 06 from SPIRITS to NEW SPIRITS . The figures below show categorization results as they appear in Studio without and with the manipulation. This other example code: function onFinalize ( result ) { var count = result . match_info . rules . categorization . length ; var category ; for ( i = 0 ; i < count ; i ++ ) { category = result . match_info . rules . categorization [ i ]; if ( category . label == \"FOOD\" ) { category . score += 15 ; } } return result ; } adds some point to the score of the category labeled FOOD . The figures below show categorization results as they appear in Studio without and with the manipulation.","title":"categorization property"},{"location":"scripting/onfinalize/result/extraction/","text":"extraction property result.match_info.rules.extraction is an array containing the results of extraction. Each array item represents an extraction record and has the following properties: Property Description template Extraction's template fields Extraction's fields fields is an array. Each item represents a template's field and has the following properties: Property Description field Field name value Field value instance Field instances instance is an array. Each item represents an instance of the field and has the following properties: Property Description group_by When two instances of different fields of the same record have the same value for this property, they must be considered as an aggregate text Field instance text pos Zero based position of the field instance text len Length of the field instance text snt Sentence number snt_begin Sentence initial position in the text snt_end Sentence final position in the text syncon Syncon ID ancestor Ancestor ID rule_details Rule details rule_details is an array. Its items have the following properties: Property Description id Rule ID is a rule identification number created during the project building. It is a compiled rule index of an array where the rules are placed. It changes after every building. label Rule label , if any For example, consider the following text: BMW released Tuesday the details of an electric concept car, with production of the vehicle expected to start in 2021. In an interview with CNBC Tuesday, CEO Oliver Zipse described the BMW Concept i4 vehicle as bringing \"electromobility to the heart of the BMW brand\". The firm is one of several major manufacturers developing an electric vehicle offering to challenge electric car makers like Tesla. and the rule: SCOPE SENTENCE { IDENTIFY(BRANDS) { @BRAND[ANCESTOR(376882)] //@SYN: #376882# [tag_all_brands] } } the extraction property has the following JSON serialization: `extraction \": [ { \" template \": \" BRANDS \", \" fields \": [ { \" field \": \" BRAND \", \" value \": \" BMW \", \" instance \": [ { \" group_by \": 0, \" text \": \" BMW \", \" rule_details \": [ { \" id \": 1, \" label \": \"\" } ], \" pos \": 0, \" len \": 3, \" snt \": 1, \" snt_begin \": 0, \" snt_end \": 117, \" syncon \": 1039566, \" ancestor \": -1 } ] } ], }, { \" template \": \" BRANDS \", \" fields \": [ { \" field \": \" BRAND \", \" value \": \" Tesla (Veicoli) \", \" instance \": [ { \" group_by \": 1000000, \" text \": \" Tesla \", \" rule_details \": [ { \" id \": 1, \" label \": \"\" } ], \" pos \": 394, \" len \": 5, \" snt \": 3, \" snt_begin \": 269, \" snt_end \": 399, \" syncon \": 1001728, \" ancestor\": -1 } ] } ], In that context, the following code: function onFinalize ( result ) { var extractionsCount = result . match_info . rules . extraction . length ; var extraction ; var fieldsCount ; for ( i = 0 ; i < extractionsCount ; i ++ ) { extraction = result . match_info . rules . extraction [ i ]; fieldsCount = extraction . fields . length ; for ( j = 0 ; j < fieldsCount ; j ++ ) { if ( extraction . fields [ j ]. field == \"BRAND\" && extraction . fields [ j ]. value == \"Tesla (Veicoli)\" ) { extraction . fields [ j ]. value = \"Tesla (Vehicles)\" ; } } } return result ; } changes the value of the BRAND field from Tesla (Veicoli) to Tesla (Vehicles) . The figures below show extraction results as they appear in Studio without and with the manipulation.","title":"extraction property"},{"location":"scripting/onfinalize/result/extraction/#extraction-property","text":"result.match_info.rules.extraction is an array containing the results of extraction. Each array item represents an extraction record and has the following properties: Property Description template Extraction's template fields Extraction's fields fields is an array. Each item represents a template's field and has the following properties: Property Description field Field name value Field value instance Field instances instance is an array. Each item represents an instance of the field and has the following properties: Property Description group_by When two instances of different fields of the same record have the same value for this property, they must be considered as an aggregate text Field instance text pos Zero based position of the field instance text len Length of the field instance text snt Sentence number snt_begin Sentence initial position in the text snt_end Sentence final position in the text syncon Syncon ID ancestor Ancestor ID rule_details Rule details rule_details is an array. Its items have the following properties: Property Description id Rule ID is a rule identification number created during the project building. It is a compiled rule index of an array where the rules are placed. It changes after every building. label Rule label , if any For example, consider the following text: BMW released Tuesday the details of an electric concept car, with production of the vehicle expected to start in 2021. In an interview with CNBC Tuesday, CEO Oliver Zipse described the BMW Concept i4 vehicle as bringing \"electromobility to the heart of the BMW brand\". The firm is one of several major manufacturers developing an electric vehicle offering to challenge electric car makers like Tesla. and the rule: SCOPE SENTENCE { IDENTIFY(BRANDS) { @BRAND[ANCESTOR(376882)] //@SYN: #376882# [tag_all_brands] } } the extraction property has the following JSON serialization: `extraction \": [ { \" template \": \" BRANDS \", \" fields \": [ { \" field \": \" BRAND \", \" value \": \" BMW \", \" instance \": [ { \" group_by \": 0, \" text \": \" BMW \", \" rule_details \": [ { \" id \": 1, \" label \": \"\" } ], \" pos \": 0, \" len \": 3, \" snt \": 1, \" snt_begin \": 0, \" snt_end \": 117, \" syncon \": 1039566, \" ancestor \": -1 } ] } ], }, { \" template \": \" BRANDS \", \" fields \": [ { \" field \": \" BRAND \", \" value \": \" Tesla (Veicoli) \", \" instance \": [ { \" group_by \": 1000000, \" text \": \" Tesla \", \" rule_details \": [ { \" id \": 1, \" label \": \"\" } ], \" pos \": 394, \" len \": 5, \" snt \": 3, \" snt_begin \": 269, \" snt_end \": 399, \" syncon \": 1001728, \" ancestor\": -1 } ] } ], In that context, the following code: function onFinalize ( result ) { var extractionsCount = result . match_info . rules . extraction . length ; var extraction ; var fieldsCount ; for ( i = 0 ; i < extractionsCount ; i ++ ) { extraction = result . match_info . rules . extraction [ i ]; fieldsCount = extraction . fields . length ; for ( j = 0 ; j < fieldsCount ; j ++ ) { if ( extraction . fields [ j ]. field == \"BRAND\" && extraction . fields [ j ]. value == \"Tesla (Veicoli)\" ) { extraction . fields [ j ]. value = \"Tesla (Vehicles)\" ; } } } return result ; } changes the value of the BRAND field from Tesla (Veicoli) to Tesla (Vehicles) . The figures below show extraction results as they appear in Studio without and with the manipulation.","title":"extraction property"},{"location":"scripting/onprepare/","text":"onPrepare Introduction Scripting can be used to pre-process the text before it is submitted to analysis. Possible use cases for text pre-processing are: Wrong or repeated punctuation marks removal. Unnecessary white space\u2014blanks, newlines\u2014removal. Systematic OCR errors correction. Upper casing or lower casing of alphabetic characters. Special or unwanted characters and words removal, for example HTML tags. Number words to numeric form and viceversa conversion. Emoticons to words conversion. Emojis to words conversion. Chat words conversion. Text pre-processing can be accomplished using one or both of the following: Simple string manipulation The REX object It must take place in the onPrepare event handler function which is executed when the Prepare event is fired, that is after document preparation and immediately before the text is submitted to the analysis. The text argument of the onPrepare function contains the original text, pre-processing consists in changing it when appropriate and returning the modified value. The subsequent analysis phase takes the returned value by the function as its input text. function onPrepare ( text ) { // Put here statements that change the value of text when appropriate return ( text ); } Find more information on document preparation and the differences between run time environment and Studio in the Studio user manual article about the topic. Simple string manipulation For simple text manipulation operations you can use the properties and methods that all objects of type string have in the reference standard specification . Below there are examples of the most commonly used features. Note The position of characters and sub-strings in a string is zero based , meaning that the position of the first character in a string is 0, that of the second character is 1 and so on. This way, for example, the position of the last character in a string is the length of the string minus 1. Determine string length Every string object has a length property. It is used to count the number of characters in the string. For example, if the text argument of the onPrepare function is set to Hello world! , after this statement: var inputTextLenght = text . length ; the value of variable inputTextLenght will be 12. Extract a character To find and extract a character in a string, use the charAt() method which takes the character position within the string as its argument. For example, if the text argument of the onPrepare function is set to Hello world! , after this statement: var sixthChar = text . charAt ( 6 ); the value of variable sixthChar will be w . Find a sub-string To find a sub-string within a string, use the indexOf() method that has the sub-string to find as its argument. For example, if the text argument of the onPrepare function is set to Hello world! , after this statement: var mySubStrPos = text . indexOf ( \"world\" ); the value of variable mySubStrPos will be 7, which is the position of world inside the value of text . If the substring is not found, the function will return -1. Replace a sub-string To replace a sub-string, use the replace() method. For example, if the text argument of the onPrepare function is set to Hello world! , after this statement: var newText = text . replace ( \"world\" , \"moon\" ); the value of variable newText will be Hello moon! . Note The replace() method only replaces the first occurrence of the sub-string. Change case To change the case of a string to lowercase, use the toLowerCase() method. For example, if the text argument of the onPrepare function is set to Hello world! , after this statement: var myLowTxt = text . toLowerCase (); the value of variable myLowTxt will be hello world! . Similarly, to change the case to uppercase, use the toUpperCase() method.","title":"onPrepare"},{"location":"scripting/onprepare/#onprepare","text":"","title":"onPrepare"},{"location":"scripting/onprepare/#introduction","text":"Scripting can be used to pre-process the text before it is submitted to analysis. Possible use cases for text pre-processing are: Wrong or repeated punctuation marks removal. Unnecessary white space\u2014blanks, newlines\u2014removal. Systematic OCR errors correction. Upper casing or lower casing of alphabetic characters. Special or unwanted characters and words removal, for example HTML tags. Number words to numeric form and viceversa conversion. Emoticons to words conversion. Emojis to words conversion. Chat words conversion. Text pre-processing can be accomplished using one or both of the following: Simple string manipulation The REX object It must take place in the onPrepare event handler function which is executed when the Prepare event is fired, that is after document preparation and immediately before the text is submitted to the analysis. The text argument of the onPrepare function contains the original text, pre-processing consists in changing it when appropriate and returning the modified value. The subsequent analysis phase takes the returned value by the function as its input text. function onPrepare ( text ) { // Put here statements that change the value of text when appropriate return ( text ); } Find more information on document preparation and the differences between run time environment and Studio in the Studio user manual article about the topic.","title":"Introduction"},{"location":"scripting/onprepare/#simple-string-manipulation","text":"For simple text manipulation operations you can use the properties and methods that all objects of type string have in the reference standard specification . Below there are examples of the most commonly used features. Note The position of characters and sub-strings in a string is zero based , meaning that the position of the first character in a string is 0, that of the second character is 1 and so on. This way, for example, the position of the last character in a string is the length of the string minus 1.","title":"Simple string manipulation"},{"location":"scripting/onprepare/#determine-string-length","text":"Every string object has a length property. It is used to count the number of characters in the string. For example, if the text argument of the onPrepare function is set to Hello world! , after this statement: var inputTextLenght = text . length ; the value of variable inputTextLenght will be 12.","title":"Determine string length"},{"location":"scripting/onprepare/#extract-a-character","text":"To find and extract a character in a string, use the charAt() method which takes the character position within the string as its argument. For example, if the text argument of the onPrepare function is set to Hello world! , after this statement: var sixthChar = text . charAt ( 6 ); the value of variable sixthChar will be w .","title":"Extract a character"},{"location":"scripting/onprepare/#find-a-sub-string","text":"To find a sub-string within a string, use the indexOf() method that has the sub-string to find as its argument. For example, if the text argument of the onPrepare function is set to Hello world! , after this statement: var mySubStrPos = text . indexOf ( \"world\" ); the value of variable mySubStrPos will be 7, which is the position of world inside the value of text . If the substring is not found, the function will return -1.","title":"Find a sub-string"},{"location":"scripting/onprepare/#replace-a-sub-string","text":"To replace a sub-string, use the replace() method. For example, if the text argument of the onPrepare function is set to Hello world! , after this statement: var newText = text . replace ( \"world\" , \"moon\" ); the value of variable newText will be Hello moon! . Note The replace() method only replaces the first occurrence of the sub-string.","title":"Replace a sub-string"},{"location":"scripting/onprepare/#change-case","text":"To change the case of a string to lowercase, use the toLowerCase() method. For example, if the text argument of the onPrepare function is set to Hello world! , after this statement: var myLowTxt = text . toLowerCase (); the value of variable myLowTxt will be hello world! . Similarly, to change the case to uppercase, use the toUpperCase() method.","title":"Change case"},{"location":"scripting/ontagger/","text":"onTagger If you need to alter the tags assigned by the Disambiguator, the onTagger is the event handling function that you can use to \"tag by script\". onTagger intervenes in the normal tagging process and is normally executed immediately after the evaluation of the tagging rules both by the intelligence engine and by Studio. Tagging by script is done using specific methods of the predefined DIS object . This object has tagging and untagging methods, but also methods that give access to the results of the disambiguation, so it \"knows\" about the subdivisions of the text at different levels: paragraphs, sentences, phrases, tokens. The tagging/untagging methods allow you to add or remove tags to all the tokens of a given subdivision: adding a tag to a sentence, for example, adds the same tag to all the tokens of the sentence, removing a tag from a sentence removes the tag from all the tokens of the sentence. The effects of the different methods are illustrated below as can be seen in Studio's Semantic Analysis tool window. tagParagraph tagSentence tagPhrase tagToken","title":"onTagger"},{"location":"scripting/ontagger/#ontagger","text":"If you need to alter the tags assigned by the Disambiguator, the onTagger is the event handling function that you can use to \"tag by script\". onTagger intervenes in the normal tagging process and is normally executed immediately after the evaluation of the tagging rules both by the intelligence engine and by Studio. Tagging by script is done using specific methods of the predefined DIS object . This object has tagging and untagging methods, but also methods that give access to the results of the disambiguation, so it \"knows\" about the subdivisions of the text at different levels: paragraphs, sentences, phrases, tokens. The tagging/untagging methods allow you to add or remove tags to all the tokens of a given subdivision: adding a tag to a sentence, for example, adds the same tag to all the tokens of the sentence, removing a tag from a sentence removes the tag from all the tokens of the sentence. The effects of the different methods are illustrated below as can be seen in Studio's Semantic Analysis tool window. tagParagraph tagSentence tagPhrase tagToken","title":"onTagger"},{"location":"scripting/rex-object/","text":"REX The REX object Regular expressions are patterns that can match one or more characters in a string. REX is a pre-defined object available in the scripting language that can be used for regular expression-based find & replace operations. The REX object supports the same Perl compatible regular expressions used for the PATTERN attribute of the rules language. Its methods are described below. compile The first thing to do in order to use a regular expression is to compile it. This is achieved with the compile method of the REX object which has this syntax: REX.compile(regular_expression) where regular_expression is a string\u2014a constant or a variable\u2014representing a Perl compatible regular expression. This method returns an integer value which is the identifier used in find & replace methods to refer to the regular expression. anchoredMatch The anchoredMatch method checks if a regular expression matches the beginning of a string. It returns a Boolean value, which will be True, if the match is found and False otherwise. For example, after the execution of this snippet of code: // The value of variable text is \"Hello world!\" var rexId = REX . compile ( \"[:upper:]\" ); var isMatch = REX . anchoredMatch ( rexId , text ); the value of variable isMatch is True, because an uppercase letter ( H ) was found at the beginning of the value ( Hello world! ) of variable text . The syntax is: REX.anchoredMatch(regular_expression_identifier, string) where regular_expression_identifier is the identifier of a regular expression that's previously been compiled with the compile method and string is a string\u2014variable or constant\u2014in which to search the regular expression. partialMatch The partialMatch method checks if a regular expression has a match anywhere inside a string. It returns a Boolean value which is True if the match was found, False otherwise. For example, after the execution of this snippet of code: // The value of variable text is \"Hello world!\" var rexId = REX . compile ( \"[wm]o\" ); var isMatch = REX . partialMatch ( rexId , text ); the value of variable isMatch is True, because a match ( wo ) for the regular expression ( [wm]o ) was found inside the value of variable text . The syntax is: REX.partialMatch(regular_expression_identifier, string) where regular_expression_identifier is the identifier of a regular expression that's previously been compiled with the compile method and string is a string\u2014variable or constant\u2014in which to search the regular expression. match The match method checks if a regular expression matches a string starting the search at an offset position withing the string itself. It returns an array containing the position and the length of the match. For example, after the execution of this snippet of code: // The value of variable text is \"Hello world!\" var rexId = REX . compile ( \"[wm]o\" ); var match = REX . match ( rexId , text , 3 ); the value of variable match is this array: match[0] = 6 match[1] = 2 because a match of the regular expression [wm]o was found at position 6 of the value of text ( Hello world! ) and the length of the matched sub-string ( wo ) is 2. The syntax is: REX.match(regular_expression_identifier, string, offset) where regular_expression_identifier is the identifier of a regular expression that's previously been compiled with the compile method, string is a string\u2014variable or constant\u2014in which to search the regular expression and offset is an integer value or an integer variable representing the offset in the string from which to start searching the regular expression. When no match is found the value of the returned array is: [0] = -1 [1] = 0 substitute The substitute method replaces the part of a string that is matched by a regular expression with another string. It returns the resulting string. For example, after the execution of this snippet of code: // The value of variable text is \"Hello world!\" var rexId = REX . compile ( \"(?i)hello\" ); var replaced = REX . substitute ( rexId , text , \"Hi\" ); the value of variable replaced will be: Hi world! because the match of the regular expression (?i)hello ( Hello ) was replaced with the constant string Hi . The syntax is: REX.substitute(regular_expression_identifier, string, replacement_string) where regular_expression_identifier is the identifier of a regular expression that's previously been compiled with the compile method, string is a string\u2014variable or constant\u2014in which to search the regular expression and replacement_string is the replacement string\u2014either a constant or a variable. The method returns a new string representing the string after the replacement. close The close method releases the memory occupied by a regular expression. After the execution of the method, the regular expression is no longer available. The syntax is: REX.close(regular_expression_identifier) where regular_expression_identifier is the identifier of a regular expression that's previously been compiled with the compile method. Always use the close method to free up memory when you don't need a regular expression anymore. Scope of the REX object The REX object always has a global scope and can be used in any function of a script, it is particularly useful in the onPrepare function. If desired, it is also possible to give a regular expression identifier a global scope. This is accomplished by setting it with the REX.compile method in the initialize function . A globally scoped regular expression identifier can be used anywhere in the code, but remember to close it with the REX.close method in the shutdown function.","title":"REX"},{"location":"scripting/rex-object/#rex","text":"","title":"REX"},{"location":"scripting/rex-object/#the-rex-object","text":"Regular expressions are patterns that can match one or more characters in a string. REX is a pre-defined object available in the scripting language that can be used for regular expression-based find & replace operations. The REX object supports the same Perl compatible regular expressions used for the PATTERN attribute of the rules language. Its methods are described below.","title":"The REX object"},{"location":"scripting/rex-object/#compile","text":"The first thing to do in order to use a regular expression is to compile it. This is achieved with the compile method of the REX object which has this syntax: REX.compile(regular_expression) where regular_expression is a string\u2014a constant or a variable\u2014representing a Perl compatible regular expression. This method returns an integer value which is the identifier used in find & replace methods to refer to the regular expression.","title":"compile"},{"location":"scripting/rex-object/#anchoredmatch","text":"The anchoredMatch method checks if a regular expression matches the beginning of a string. It returns a Boolean value, which will be True, if the match is found and False otherwise. For example, after the execution of this snippet of code: // The value of variable text is \"Hello world!\" var rexId = REX . compile ( \"[:upper:]\" ); var isMatch = REX . anchoredMatch ( rexId , text ); the value of variable isMatch is True, because an uppercase letter ( H ) was found at the beginning of the value ( Hello world! ) of variable text . The syntax is: REX.anchoredMatch(regular_expression_identifier, string) where regular_expression_identifier is the identifier of a regular expression that's previously been compiled with the compile method and string is a string\u2014variable or constant\u2014in which to search the regular expression.","title":"anchoredMatch"},{"location":"scripting/rex-object/#partialmatch","text":"The partialMatch method checks if a regular expression has a match anywhere inside a string. It returns a Boolean value which is True if the match was found, False otherwise. For example, after the execution of this snippet of code: // The value of variable text is \"Hello world!\" var rexId = REX . compile ( \"[wm]o\" ); var isMatch = REX . partialMatch ( rexId , text ); the value of variable isMatch is True, because a match ( wo ) for the regular expression ( [wm]o ) was found inside the value of variable text . The syntax is: REX.partialMatch(regular_expression_identifier, string) where regular_expression_identifier is the identifier of a regular expression that's previously been compiled with the compile method and string is a string\u2014variable or constant\u2014in which to search the regular expression.","title":"partialMatch"},{"location":"scripting/rex-object/#match","text":"The match method checks if a regular expression matches a string starting the search at an offset position withing the string itself. It returns an array containing the position and the length of the match. For example, after the execution of this snippet of code: // The value of variable text is \"Hello world!\" var rexId = REX . compile ( \"[wm]o\" ); var match = REX . match ( rexId , text , 3 ); the value of variable match is this array: match[0] = 6 match[1] = 2 because a match of the regular expression [wm]o was found at position 6 of the value of text ( Hello world! ) and the length of the matched sub-string ( wo ) is 2. The syntax is: REX.match(regular_expression_identifier, string, offset) where regular_expression_identifier is the identifier of a regular expression that's previously been compiled with the compile method, string is a string\u2014variable or constant\u2014in which to search the regular expression and offset is an integer value or an integer variable representing the offset in the string from which to start searching the regular expression. When no match is found the value of the returned array is: [0] = -1 [1] = 0","title":"match"},{"location":"scripting/rex-object/#substitute","text":"The substitute method replaces the part of a string that is matched by a regular expression with another string. It returns the resulting string. For example, after the execution of this snippet of code: // The value of variable text is \"Hello world!\" var rexId = REX . compile ( \"(?i)hello\" ); var replaced = REX . substitute ( rexId , text , \"Hi\" ); the value of variable replaced will be: Hi world! because the match of the regular expression (?i)hello ( Hello ) was replaced with the constant string Hi . The syntax is: REX.substitute(regular_expression_identifier, string, replacement_string) where regular_expression_identifier is the identifier of a regular expression that's previously been compiled with the compile method, string is a string\u2014variable or constant\u2014in which to search the regular expression and replacement_string is the replacement string\u2014either a constant or a variable. The method returns a new string representing the string after the replacement.","title":"substitute"},{"location":"scripting/rex-object/#close","text":"The close method releases the memory occupied by a regular expression. After the execution of the method, the regular expression is no longer available. The syntax is: REX.close(regular_expression_identifier) where regular_expression_identifier is the identifier of a regular expression that's previously been compiled with the compile method. Always use the close method to free up memory when you don't need a regular expression anymore.","title":"close"},{"location":"scripting/rex-object/#scope-of-the-rex-object","text":"The REX object always has a global scope and can be used in any function of a script, it is particularly useful in the onPrepare function. If desired, it is also possible to give a regular expression identifier a global scope. This is accomplished by setting it with the REX.compile method in the initialize function . A globally scoped regular expression identifier can be used anywhere in the code, but remember to close it with the REX.close method in the shutdown function.","title":"Scope of the REX object"},{"location":"sections/","text":"Sections Introduction Some types of documents have a structure consisting of multiple sections. For example: E-mail messages: Subject Body Sender \"To\" recipients \"CC\" recipients Attachments Newspaper articles: Title Byline Lead Body Scientific papers: Title Abstract Keywords Main text References Taking sections into account in categorization and extraction projects may be important or even necessary. For example, an extraction project may require the extraction of data only from a given section, while a categorizer could perform better if more importance is given to the title text. Expert.ai languages \u200b\u200ballow you to declare expected sections, use them as the scope for rules and give them a weight in correspondence to their importance which is then automatically used by the scoring algorithm. However, the disambiguator analyzes plain text, so when the text of a structured document is copied and pasted, unstructured text will be returned with no indication of where sections begin and end. It would seem that the sections are totally lost in the text extraction. In reality, there are ways to preserve and provide sections information to the disambiguator. Sections are automatically detected when the original document\u2014an e-mail message, a PDF file, an XML file, etc.\u2014is processed to obtain its text. If the document processor is programmed for this purpose, it could also locate the sections and output their boundaries as side-by-side information in a format the disambiguator can understand. When the plain text is coupled with this information the disambiguator is able to recognize portions of text as belonging to a section or another. Side-by-side information is used at runtime by the text intelligence engine in the production environment, but, before that, it can be used in the development environment to set up and test sections management in the project. In the same environment, plain text test files can be manually annotated to indicate the start and end of sections. These annotations are also stored as side-by-side information in special project files. Sections information is provided from the outside, so sections can be considered as predefined areas of text. Segments, on the other hand, are dynamically created \"in memory\" at runtime by specific instructions of expert.ai languages, based on features of the plain text alone. Declaration In order to be used, sections must be declared in a rules file. The syntax is: SECTIONS { declaration [, declaration , ...] } where declaration is defined as: name [ ( options ) ] name must match the section name specified in the side-by-side information and must be preceded by the at sign ( @ ). Multiple options are separated by commas. For example: SECTIONS { @BODY(STANDARD,1SCORE), @TITLE(2SCORE) } There are two options: STANDARD qualifier #SCORE multiplier STANDARD qualifier The STANDARD qualifier marks the default section . This section is the implicit \"higher level scope\" for rules whose scope does not reference a section. For example, this rule: SCOPE SENTENCE { DOMAIN(dom1) { TYPE(NPH, NPR) } } has SENTENCE as its declared scope, therefore only sentences in the default section will be considered when evaluating the rule. Only one section can have the STANDARD qualifier. SCORE multiplier The score multiplier option has this syntax: #SCORE where # can be 0 or any positive integer. The score multiplier affects only categorization rules. Whenever a categorization rule is triggered by the text of a section with this option set, the rule's score will be multiplied by the value of # . The default value is 1, so if no score multiplier option is specified, the rule's score will not be changed. When # is 0, the rule's score will be multiplied by 0, becoming 0, as if the rule was not triggered. In this way the text of the section will be excluded from the categorization, as if it doesn't exist. Positive values of # are used to give a \"boost\" to the score, thus attributing more relevance to rules' hits when they occur in the section. They are usually specified for heading sections such as titles. Sections as rules' scope Sections can be specified as the scope of categorization and extraction rules. Rules with a section scope are triggered only by the text of the specified section. For example, this rule: SCOPE SECTION(TITLE) { IDENTIFY(VIP) { @NAME[TYPE(NPH)] } } matches and extracts people's names from the TITLE section only, and places them in the NAME field of VIP records. Implicit section Internally, any expert.ai-based text intelligence engine requires sections, meaning that it expects that any given text will always belong to some section. On the other hand, original documents may not be structured and/or categorization and extraction projects may not need sections. Therefore users will not be required to declare and use sections, if they are not needed. The solution to this apparent contradiction is the implicit section . When a user decides to omit the sections declaration, the engine will work as if a BODY section with the STANDARD qualifier was declared and hence, all the text was included in the BODY section. In other words, if sections are not required for a project, they could be ignored and the project will work as expected. However, if they are used: If BODY section is declared, this declaration will override the implicit declaration. If a BODY section is not declared because the documents do not contain one, then, any text outside sections will be considered as part of the implicit BODY section.","title":"Sections"},{"location":"sections/#sections","text":"","title":"Sections"},{"location":"sections/#introduction","text":"Some types of documents have a structure consisting of multiple sections. For example: E-mail messages: Subject Body Sender \"To\" recipients \"CC\" recipients Attachments Newspaper articles: Title Byline Lead Body Scientific papers: Title Abstract Keywords Main text References Taking sections into account in categorization and extraction projects may be important or even necessary. For example, an extraction project may require the extraction of data only from a given section, while a categorizer could perform better if more importance is given to the title text. Expert.ai languages \u200b\u200ballow you to declare expected sections, use them as the scope for rules and give them a weight in correspondence to their importance which is then automatically used by the scoring algorithm. However, the disambiguator analyzes plain text, so when the text of a structured document is copied and pasted, unstructured text will be returned with no indication of where sections begin and end. It would seem that the sections are totally lost in the text extraction. In reality, there are ways to preserve and provide sections information to the disambiguator. Sections are automatically detected when the original document\u2014an e-mail message, a PDF file, an XML file, etc.\u2014is processed to obtain its text. If the document processor is programmed for this purpose, it could also locate the sections and output their boundaries as side-by-side information in a format the disambiguator can understand. When the plain text is coupled with this information the disambiguator is able to recognize portions of text as belonging to a section or another. Side-by-side information is used at runtime by the text intelligence engine in the production environment, but, before that, it can be used in the development environment to set up and test sections management in the project. In the same environment, plain text test files can be manually annotated to indicate the start and end of sections. These annotations are also stored as side-by-side information in special project files. Sections information is provided from the outside, so sections can be considered as predefined areas of text. Segments, on the other hand, are dynamically created \"in memory\" at runtime by specific instructions of expert.ai languages, based on features of the plain text alone.","title":"Introduction"},{"location":"sections/#declaration","text":"In order to be used, sections must be declared in a rules file. The syntax is: SECTIONS { declaration [, declaration , ...] } where declaration is defined as: name [ ( options ) ] name must match the section name specified in the side-by-side information and must be preceded by the at sign ( @ ). Multiple options are separated by commas. For example: SECTIONS { @BODY(STANDARD,1SCORE), @TITLE(2SCORE) } There are two options: STANDARD qualifier #SCORE multiplier","title":"Declaration"},{"location":"sections/#standard-qualifier","text":"The STANDARD qualifier marks the default section . This section is the implicit \"higher level scope\" for rules whose scope does not reference a section. For example, this rule: SCOPE SENTENCE { DOMAIN(dom1) { TYPE(NPH, NPR) } } has SENTENCE as its declared scope, therefore only sentences in the default section will be considered when evaluating the rule. Only one section can have the STANDARD qualifier.","title":"STANDARD qualifier"},{"location":"sections/#score-multiplier","text":"The score multiplier option has this syntax: #SCORE where # can be 0 or any positive integer. The score multiplier affects only categorization rules. Whenever a categorization rule is triggered by the text of a section with this option set, the rule's score will be multiplied by the value of # . The default value is 1, so if no score multiplier option is specified, the rule's score will not be changed. When # is 0, the rule's score will be multiplied by 0, becoming 0, as if the rule was not triggered. In this way the text of the section will be excluded from the categorization, as if it doesn't exist. Positive values of # are used to give a \"boost\" to the score, thus attributing more relevance to rules' hits when they occur in the section. They are usually specified for heading sections such as titles.","title":"SCORE multiplier"},{"location":"sections/#sections-as-rules-scope","text":"Sections can be specified as the scope of categorization and extraction rules. Rules with a section scope are triggered only by the text of the specified section. For example, this rule: SCOPE SECTION(TITLE) { IDENTIFY(VIP) { @NAME[TYPE(NPH)] } } matches and extracts people's names from the TITLE section only, and places them in the NAME field of VIP records.","title":"Sections as rules' scope"},{"location":"sections/#implicit-section","text":"Internally, any expert.ai-based text intelligence engine requires sections, meaning that it expects that any given text will always belong to some section. On the other hand, original documents may not be structured and/or categorization and extraction projects may not need sections. Therefore users will not be required to declare and use sections, if they are not needed. The solution to this apparent contradiction is the implicit section . When a user decides to omit the sections declaration, the engine will work as if a BODY section with the STANDARD qualifier was declared and hence, all the text was included in the BODY section. In other words, if sections are not required for a project, they could be ignored and the project will work as expected. However, if they are used: If BODY section is declared, this declaration will override the implicit declaration. If a BODY section is not declared because the documents do not contain one, then, any text outside sections will be considered as part of the implicit BODY section.","title":"Implicit section"},{"location":"sections-and-segments/","text":"Sections and segments overview Within a categorization and/or extraction project, sections and segments are a way of defining and recognizing specific portions of text in a document. The usage of sectioned or segmented documents is useful in several cases. This is especially true when categorization or extraction rules must act upon blocks of text different from those automatically recognized by the semantic disambiguator text analysis (for example paragraphs, sentences, clauses and phrases). This is typically the case of those documents whose original (and usually highly recognizable) layout or structure is critical for the correct identification and/or retrieval of information. In fact, sometimes, the key element of a document must not be searched for throughout the text. Instead, it may be located in a very precise point of a document, and therefore it cannot be searched for in any sentence, clause or phrase. It must be pinpointed in a specific sentence (or group of sentences) which a particular position or structure in the document considered as a whole. Sections and segments have different characteristics and have been designed to respond to different operational needs. A developer must determine which solution is the best based on the nature of the text blocks to be recognized and the objectives to be achieved.","title":"Sections and segments overview"},{"location":"sections-and-segments/#sections-and-segments-overview","text":"Within a categorization and/or extraction project, sections and segments are a way of defining and recognizing specific portions of text in a document. The usage of sectioned or segmented documents is useful in several cases. This is especially true when categorization or extraction rules must act upon blocks of text different from those automatically recognized by the semantic disambiguator text analysis (for example paragraphs, sentences, clauses and phrases). This is typically the case of those documents whose original (and usually highly recognizable) layout or structure is critical for the correct identification and/or retrieval of information. In fact, sometimes, the key element of a document must not be searched for throughout the text. Instead, it may be located in a very precise point of a document, and therefore it cannot be searched for in any sentence, clause or phrase. It must be pinpointed in a specific sentence (or group of sentences) which a particular position or structure in the document considered as a whole. Sections and segments have different characteristics and have been designed to respond to different operational needs. A developer must determine which solution is the best based on the nature of the text blocks to be recognized and the objectives to be achieved.","title":"Sections and segments overview"},{"location":"segments/","text":"Segments Documents segmentation is one of the two techniques a developer can use to create custom textual subdivisions in an input document (the other being sectioning). This technique is particularly useful when the original text layout or structure turns out to be fundamental for the correct identification and/or retrieval of information. Segments are dynamic text blocks which are identified during the processing of input texts by means of custom linguistic rules. For example, consider a plain-text input document such as the following: Ingredients 1 teaspoon olive oil 1 cup diced zucchini 1/2 cup minced onion 1 clove garlic, peeled and minced 2 cups diced fresh tomatoes 2 tablespoons chopped fresh basil 1/4 teaspoon salt 1/4 teaspoon ground black pepper 4 (6 ounce) halibut steaks 1/3 cup crumbled feta cheese PREP 15 mins COOK 15 mins READY IN 30 mins Directions 1.preheat oven to 450 degrees F (230 degrees C). Lightly grease a shallow baking dish. 2.heat olive oil in a medium saucepan over medium heat and stir in zucchini, onion, and garlic. Cook and stir 5 minutes or until tender. Remove saucepan from heat and mix in tomatoes, basil, salt, and pepper. 3.arrange halibut steaks in a single layer in the prepared baking dish. Spoon equal amounts of the zucchini mixture over each steak. Top with feta cheese. 4.bake 15 minutes in the preheated oven, or until fish is easily flaked with a fork. We can agree that such a document is recognizable at first glance as a recipe. It\u2019s easy for a human being to browse the text content and take a quick glimpse at the layout to understand where the ingredients are listed and where to look for cooking directions. However, this document is not electronically marked up, so it isn't as easy for a text processing tool to recognize these same elements. Text segmentation allows the user to reconstruct this mark-up, so that the visual structure can be \u201ctranslated\u201d to a machine-readable format and used for automatic text processing. The rules used to identify dynamic segments are similar to categorization rules. However, unlike categorization rules, when a segmentation rule identifies a concept in a text, it will neither provide a score to a category, nor link a document to any category. Segmentation rules are created to identify the boundaries of a relevant text block by searching for specific terms and phrases and using them as reference points. The concepts identified by segmentation rules act as \u201cCartesian coordinates\u201d which help the developer and the software to outline the segments contained in the entire text. Segments are identified by meaningful names\u2014usually chosen by the developer\u2014so that they can easily be referenced throughout a project. Before starting with the development of segmentation rules, every segment must be declared in a .cr file. The default file config.cr already contains a declaration: SEGMENTS { @SEGMENT1(1.0), @SEGMENT2(1.0) } Note The parameter inside the round brackets is just for legacy matters. Once the segments\u2019 names have been declared, it is possible to develop as many segmentation rules as needed. The final goal of document segmentation is to be able to use the dynamic segments as scope options for both categorization and extraction rules. In other words, a rule can be set so that it is applied only to one or more text blocks which were tagged as segments. This will perform a kind of preliminary selection on the whole text in order to identify the portion that is more likely to contain the relevant information to be categorized or extracted.","title":"Segments"},{"location":"segments/#segments","text":"Documents segmentation is one of the two techniques a developer can use to create custom textual subdivisions in an input document (the other being sectioning). This technique is particularly useful when the original text layout or structure turns out to be fundamental for the correct identification and/or retrieval of information. Segments are dynamic text blocks which are identified during the processing of input texts by means of custom linguistic rules. For example, consider a plain-text input document such as the following: Ingredients 1 teaspoon olive oil 1 cup diced zucchini 1/2 cup minced onion 1 clove garlic, peeled and minced 2 cups diced fresh tomatoes 2 tablespoons chopped fresh basil 1/4 teaspoon salt 1/4 teaspoon ground black pepper 4 (6 ounce) halibut steaks 1/3 cup crumbled feta cheese PREP 15 mins COOK 15 mins READY IN 30 mins Directions 1.preheat oven to 450 degrees F (230 degrees C). Lightly grease a shallow baking dish. 2.heat olive oil in a medium saucepan over medium heat and stir in zucchini, onion, and garlic. Cook and stir 5 minutes or until tender. Remove saucepan from heat and mix in tomatoes, basil, salt, and pepper. 3.arrange halibut steaks in a single layer in the prepared baking dish. Spoon equal amounts of the zucchini mixture over each steak. Top with feta cheese. 4.bake 15 minutes in the preheated oven, or until fish is easily flaked with a fork. We can agree that such a document is recognizable at first glance as a recipe. It\u2019s easy for a human being to browse the text content and take a quick glimpse at the layout to understand where the ingredients are listed and where to look for cooking directions. However, this document is not electronically marked up, so it isn't as easy for a text processing tool to recognize these same elements. Text segmentation allows the user to reconstruct this mark-up, so that the visual structure can be \u201ctranslated\u201d to a machine-readable format and used for automatic text processing. The rules used to identify dynamic segments are similar to categorization rules. However, unlike categorization rules, when a segmentation rule identifies a concept in a text, it will neither provide a score to a category, nor link a document to any category. Segmentation rules are created to identify the boundaries of a relevant text block by searching for specific terms and phrases and using them as reference points. The concepts identified by segmentation rules act as \u201cCartesian coordinates\u201d which help the developer and the software to outline the segments contained in the entire text. Segments are identified by meaningful names\u2014usually chosen by the developer\u2014so that they can easily be referenced throughout a project. Before starting with the development of segmentation rules, every segment must be declared in a .cr file. The default file config.cr already contains a declaration: SEGMENTS { @SEGMENT1(1.0), @SEGMENT2(1.0) } Note The parameter inside the round brackets is just for legacy matters. Once the segments\u2019 names have been declared, it is possible to develop as many segmentation rules as needed. The final goal of document segmentation is to be able to use the dynamic segments as scope options for both categorization and extraction rules. In other words, a rule can be set so that it is applied only to one or more text blocks which were tagged as segments. This will perform a kind of preliminary selection on the whole text in order to identify the portion that is more likely to contain the relevant information to be categorized or extracted.","title":"Segments"},{"location":"segments/removal/","text":"Segments removal Segments removal is the last optional step in the process of segments creation and management. After all the segments boundaries have been processed so that all possible segments have been created, it is possible to reduce the number of segments, by setting hierarchy rules. The block of instructions called SEGMENTS HIERARCHY considers segments in pairs of two. It is used to manage and resolve conflicts among different segments so that one of the two is removed. Note This feature is not to be confused with the segments priority type of instruction. The segments priority options remodel the boundaries of two conflicting segments, while the segments hierarchy removes one of two segments. The syntax is the following: SEGMENTS HIERARCHY { segment_name1 //operator// segment_name2 } SEGMENTS HIERARCHY is a syntax keyword and must be written in capitals as shown above, while segment_name1 and segment_name2 refer to the segments names the instruction applies to. Operator refers to one of the two operators available for the segments priority instruction: REMOVES ERASES Removes The operator REMOVES is used to remove a segment when it overlaps with another segment. In fact, if we set the SEGMENTS HIERARCHY in the following way: SEGMENTS HIERARCHY { A REMOVES B } if segment A partially or totally overlaps segment B , then segment B will be removed. Erases The operator ERASES is used to erase a segment when istances of another segment are present. In fact, if we set the SEGMENTS HIERARCHY in the following way: SEGMENTS HIERARCHY { A ERASES B } if at least one instance of segment A is present, then all instances of segment B will be erased.","title":"Segments removal"},{"location":"segments/removal/#segments-removal","text":"Segments removal is the last optional step in the process of segments creation and management. After all the segments boundaries have been processed so that all possible segments have been created, it is possible to reduce the number of segments, by setting hierarchy rules. The block of instructions called SEGMENTS HIERARCHY considers segments in pairs of two. It is used to manage and resolve conflicts among different segments so that one of the two is removed. Note This feature is not to be confused with the segments priority type of instruction. The segments priority options remodel the boundaries of two conflicting segments, while the segments hierarchy removes one of two segments. The syntax is the following: SEGMENTS HIERARCHY { segment_name1 //operator// segment_name2 } SEGMENTS HIERARCHY is a syntax keyword and must be written in capitals as shown above, while segment_name1 and segment_name2 refer to the segments names the instruction applies to. Operator refers to one of the two operators available for the segments priority instruction: REMOVES ERASES","title":"Segments removal"},{"location":"segments/removal/#removes","text":"The operator REMOVES is used to remove a segment when it overlaps with another segment. In fact, if we set the SEGMENTS HIERARCHY in the following way: SEGMENTS HIERARCHY { A REMOVES B } if segment A partially or totally overlaps segment B , then segment B will be removed.","title":"Removes"},{"location":"segments/removal/#erases","text":"The operator ERASES is used to erase a segment when istances of another segment are present. In fact, if we set the SEGMENTS HIERARCHY in the following way: SEGMENTS HIERARCHY { A ERASES B } if at least one instance of segment A is present, then all instances of segment B will be erased.","title":"Erases"},{"location":"segments/syntax/","text":"Segments rules' syntax The fundamental aim of segmentation rules is to define dynamic segment boundaries. There are two main ways to identify a segment\u2019s boundaries: By specifying a linguistic condition and a scope. The instance of the segment will share the same position of the portion of text defined by the scope. By specifying both the linguistic condition that will allow the segment begin and the linguistic condition that will allow the segment end. The syntax of a simple segmentation rule is the following: SCOPE scope_option { SEGMENT(segment_name) { //condition// } } For example, consider the following sample text: About a month ago I was diagnosed with \"pre-diabetes\" after a blood test. I had complained to my doctor of constant tiredness and lack of energy throughout the day. I will be the first to admit my diet is lousy - pizzas, burgers, chocolate, takeaways, fizzy drinks are all vices of mine. To help me monitor this I purchased this accu-check gadget and although the concept of taking my own blood samples was a bit daunting, it really is very easy indeed. Unfortunately, in the last days my blood glucose monitor seems to give incorrect readings, I tried several times turning it off and on again, but it still doesn't work. and suppose that for the use case, a portion of text which deals with malfunctions must be identified. The following rule: SCOPE PARAGRAPH { SEGMENT(MALFUNCTION) { KEYWORD(\"n't\",\"not\") > LEMMA (\"work\") } } identifies the segment MALFUNCTION using a single point of reference: the presence in the text of the lemma work, preceded by a negation. Moreover, the boundaries of the segment will share the same position of the portion of text defined by the scope. In this case, the boundaries are provided by the scope option PARAGRAPH . The highlighted text corresponds to the instance of the segment: About a month ago I was diagnosed with \"pre-diabetes\" after a blood test. I had complained to my doctor of constant tiredness and lack of energy throughout the day. I will be the first to admit my diet is lousy - pizzas, burgers, chocolate, takeaways, fizzy drinks are all vices of mine. To help me monitor this I purchased this accu-check gadget and although the concept of taking my own blood samples was a bit daunting, it really is very easy indeed. Unfortunately, in the last days my blood glucose monitor seems to give incorrect readings, I tried several times turning it off and on again, but it still doesn't work. BEGIN and END A more comprehensive way of defining a whole block would be to find its beginning and its end. Consider the following syntax: SCOPE scope_option { SEGMENT(segment_name|BEGIN) { //condition// } SEGMENT(segment_name|END) { //condition// } } The user can decide where a segment begins and where it must end by defining (at least) two rules per segment in which the syntax keywords BEGIN and END are used after the segment name in each of the rules. In the following sample text, the re-insured sum starts with the heading SUM REINSURED and ends where the LIMITS section starts. Contract of Reinsurance SUM REINSURED USD 200,000,000 per occurrence (combined single limit or Damage and Business Interruption) LIMITS Contingent business interruption USD 125,000 DEDUCTIBLES Earthquake, Earth Movement or Volcanic Eruption 5% of loss amount, minimum USD 125,000 and maximum USD 425,000 combined Property Damage and Business Interruption This can be expressed with the following rules: SCOPE SENTENCE { SEGMENT(SUM_REINSURED|BEGIN) { KEYWORD(\"SUM REINSURED\") } SEGMENT(SUM_REINSURED|END) { KEYWORD(\"LIMITS\") } } In this case, two points of reference have been used to create the segment, where the first condition, marked as BEGIN , sets the opening boundary, while the second, marked as END , sets the closure. The portion of text highlighted in yellow corresponds to the instance of the segment: Contract of Reinsurance SUM REINSURED USD 200,000,000 per occurrence (combined single limit or Damage and Business Interruption) LIMITS Contingent business interruption USD 125,000 DEDUCTIBLES Earthquake, Earth Movement or Volcanic Eruption 5% of loss amount, minimum USD 125,000 and maximum USD 425,000 combined Property Damage and Business Interruption To use segmentation rules most effectively, it is important that they are set up to identify concepts that often recur in the set of documents to be processed for a given project. With the exception of sporadic special cases, where the beginnings and the endings of segments can be identified with almost ad hoc rules, a good set of segmentation rules must be in some way predictive, so that they can also encompass variants of known forms and layouts. BEFORE and AFTER Advanced segmentation syntax allows the developer to single out phraseology that precedes or follows the segment to be detected by using the keywords BEFORE or AFTER as follows: SCOPE scope_option { SEGMENT(segment_name|BEGIN_option) { //condition// } SEGMENT(segment_name|END_option) { //condition// } } where BEGIN_option and END_option correspond to one of the following conditions: BEGIN_BEFORE : the segment begins with the sentence before the sentence matched by the linguistic condition. - BEGIN_AFTER : the segment begins with the sentence after the sentence matched by the linguistic condition. END_BEFORE : the segment ends with the sentence before the sentence matched by the linguistic condition. - END_AFTER : the segment ends with the sentence after the sentence matched by the linguistic condition. Segmentation rules score When working with segments, it is possible to define several rules for each boundary, as the number of opening and closing conditions may vary according to the type of document. In some cases, some concepts identified by means of segmentation rules can represent stronger points of reference to define a segment boundaries than others. It is possible to highlight this difference in the rules and mark some concepts as more relevant while others as less relevant. This can be achieved by adding a score option to the rules using the following syntax: SCOPE scope_option { SEGMENT(segment_name|boundary_type_option:score_option) { //condition// } } The name of the segment must be followed by the boundary type defined by the rule as well as one of the score options. Score options can be of two types: Default score option Custom score option Default score option Segmentation and categorization rules share the same default score options listed in the table below: Option Description NORMAL The default/implicit score option LOW Lower than the default HIGH Higher than the default The options LOW and HIGH allow the user to assign a a slightly different score to a boundary compared to the default option and they can also be used to assign a higher or lower relevance of a boundary compared to another. The correct use of these options must consider: The use of the default score in most cases. The use of HIGH to give emphasis to a particular rule, for example one containing a concept or combination of concepts which is not ambiguous and will certainly result in a valid boundary (e.g. the main or most frequent beginning or end of a segment). The use of LOW to give less importance to a rule, for example one containing a slightly ambiguous concept which you are neither willing to exclude a priori nor willing to rely on in every case (for example special-case or unusual segments beginning or end). Custom score option Similar to categorization rules, it is possible to create custom score options. They can be defined in the config.cr file and they can be shared among both categorization and segmentation rules. The syntax is: SCORES { @score_option_name:points, ... } For example: SCORES { @LOWER:1, @HIGHER:20 } Once defined, the names of the new options can be used in the segmentation rules to allow for a greater variability of rules score. Scope options in segmentation rules As for categorization and extraction rules, every segment rule needs a SCOPE option to be chosen in order to define two elements: The portion of text in which a single rule or a group of rules will act upon. The portion of text on which the segment will be extended. Any of the standard or custom scope options available can be used. However, there are some restrictions specific to segmentation rules that must be detailed. The SCOPE options: SENTENCE / PARAGRAPH / CLAUSE / PHRASE can always be used. The SCOPE options: SECTION / SEGMENT / CLAUSE (clause_type) / PHRASE (phrase_type) can be used except in those cases where the BEGIN or END statements are used to separately define the boundaries of a segment. Phrase and clause PHRASE and CLAUSE scope options can be used in the cases specified above. Additionally, they must only be intended as portions of text where a segmentation rule has to be verified. In fact, since segments' extensions can't disregard sentence boundaries (for example segments can not be shorter than a sentence), CLAUSE and PHRASE scope options do not determine the portion of text on which the segment will be extended. Sentence and paragraph The SCOPE options SENTENCE and PARAGRAPH can be used in any of the ways described in the cases specified above. However, when the following syntax is used: SCOPE PARAGRAPH|SENTENCE*n. { //segmentation rule or list of rules// } A distinction must be made between the programmed scope and the real scope of a rule, where \"programmed scope\" is the most extended portion of text on which a rule acts upon, and \"real scope\" is the portion of text that is really included in the segment. For example, if we define a rule scope in the following way SCOPE SENTENCE*3 { SEGMENT (segment_name) { //condition// } } we are declaring that the rule condition has to be verified within three consecutive sentences of the input document. Actually, three sentences are the maximum possible scope for the rule to be verified. The rule could also be verified in a single sentence or in two sentences, depending where the elements specified in the condition are found. Therefore, notwithstanding the maximum scope declared in a rule, the real scope is determined by the portion of text really containing the concepts that the rule looks for. Section and segment The use of SECTION and SEGMENT scope options has a peculiar meaning when defining segmentation rules. In fact, when using these options for categorization or extraction rules, the user\u2019s aim is to look for concepts in a specific portion of text. When defining segmentation rules, on the other hand, the output of a rule acting within a section or another previously defined segment is a new segment created within the section or segment specified in the rule SCOPE . The possible aims to be achieved by means of this technique are two: Create nested segments. Upgrade a whole section or a whole segment to a new segment. Nested segments Using the scope option SEGMENT it is possible to define dynamic segments within other previously created segments. The syntax is the following: SCOPE scope_option { SEGMENT (segment_name1) { //condition// } } SCOPE SENTENCE IN SEGMENT (segment_name1) { SEGMENT (segment_name2) { //condition// } } The first rule (or set of rules) defines a segment using any scope options other than SEGMENT . The second rule uses the first segment as scope in order to define, within the first segment itself, another segment, nested in the first one. Circular References When defining nested segments it is fundamental to pay attention not to define circular references. Should it occur, the software will be unable to assign the correct order to the segmentations rules, thus making it impossible to execute them. Consider the following examples: SCOPE SENTENCE { SEGMENT (segment_name1) { //condition// } } SCOPE SENTENCE IN SEGMENT (segment1) { SEGMENT (segment2) { //condition// } } SCOPE SENTENCE IN SEGMENT (segment2) { SEGMENT (segment3) { //condition// } } SCOPE SENTENCE IN SEGMENT (segment3) { SEGMENT (segment_name1) { //condition// } } The rules above define: Segment1 first. Then segment2 is defined within segment1. Then segment3 is defined within segment2. At the end, segment1 is defined within segment3. The last rule invalidates the whole set because it introduces a circular reference in the code. This would generate an error and no rule would be compiled and applied. Sections and segments promotion By using segmentation rules it is possible to promote a whole section or segment to a new segment which coincides with the original section or segment. In other words, it is possible to generate a segment identical in position and extension to another segment or section in order to create a sort of \u201cduplicate\u201d of an existing segment or section. This technique is useful when different operations must be performed within a single section or segment (linguistic rules, filters, post-processing\u2026) and the developer needs to differentiate a document portion where these actions need to be performed. This can be achieved only when the new segment includes the entire original section or segment, not just a part of it. For example, the following sample rule: SCOPE SECTION(HEADLINE) { SEGMENT(BOLD) { //condition// } } is correct and accepted because the entire HEADLINE section is going to be part of the new segment BOLD .","title":"Segments rules' syntax"},{"location":"segments/syntax/#segments-rules-syntax","text":"The fundamental aim of segmentation rules is to define dynamic segment boundaries. There are two main ways to identify a segment\u2019s boundaries: By specifying a linguistic condition and a scope. The instance of the segment will share the same position of the portion of text defined by the scope. By specifying both the linguistic condition that will allow the segment begin and the linguistic condition that will allow the segment end. The syntax of a simple segmentation rule is the following: SCOPE scope_option { SEGMENT(segment_name) { //condition// } } For example, consider the following sample text: About a month ago I was diagnosed with \"pre-diabetes\" after a blood test. I had complained to my doctor of constant tiredness and lack of energy throughout the day. I will be the first to admit my diet is lousy - pizzas, burgers, chocolate, takeaways, fizzy drinks are all vices of mine. To help me monitor this I purchased this accu-check gadget and although the concept of taking my own blood samples was a bit daunting, it really is very easy indeed. Unfortunately, in the last days my blood glucose monitor seems to give incorrect readings, I tried several times turning it off and on again, but it still doesn't work. and suppose that for the use case, a portion of text which deals with malfunctions must be identified. The following rule: SCOPE PARAGRAPH { SEGMENT(MALFUNCTION) { KEYWORD(\"n't\",\"not\") > LEMMA (\"work\") } } identifies the segment MALFUNCTION using a single point of reference: the presence in the text of the lemma work, preceded by a negation. Moreover, the boundaries of the segment will share the same position of the portion of text defined by the scope. In this case, the boundaries are provided by the scope option PARAGRAPH . The highlighted text corresponds to the instance of the segment: About a month ago I was diagnosed with \"pre-diabetes\" after a blood test. I had complained to my doctor of constant tiredness and lack of energy throughout the day. I will be the first to admit my diet is lousy - pizzas, burgers, chocolate, takeaways, fizzy drinks are all vices of mine. To help me monitor this I purchased this accu-check gadget and although the concept of taking my own blood samples was a bit daunting, it really is very easy indeed. Unfortunately, in the last days my blood glucose monitor seems to give incorrect readings, I tried several times turning it off and on again, but it still doesn't work.","title":"Segments rules' syntax"},{"location":"segments/syntax/#begin-and-end","text":"A more comprehensive way of defining a whole block would be to find its beginning and its end. Consider the following syntax: SCOPE scope_option { SEGMENT(segment_name|BEGIN) { //condition// } SEGMENT(segment_name|END) { //condition// } } The user can decide where a segment begins and where it must end by defining (at least) two rules per segment in which the syntax keywords BEGIN and END are used after the segment name in each of the rules. In the following sample text, the re-insured sum starts with the heading SUM REINSURED and ends where the LIMITS section starts. Contract of Reinsurance SUM REINSURED USD 200,000,000 per occurrence (combined single limit or Damage and Business Interruption) LIMITS Contingent business interruption USD 125,000 DEDUCTIBLES Earthquake, Earth Movement or Volcanic Eruption 5% of loss amount, minimum USD 125,000 and maximum USD 425,000 combined Property Damage and Business Interruption This can be expressed with the following rules: SCOPE SENTENCE { SEGMENT(SUM_REINSURED|BEGIN) { KEYWORD(\"SUM REINSURED\") } SEGMENT(SUM_REINSURED|END) { KEYWORD(\"LIMITS\") } } In this case, two points of reference have been used to create the segment, where the first condition, marked as BEGIN , sets the opening boundary, while the second, marked as END , sets the closure. The portion of text highlighted in yellow corresponds to the instance of the segment: Contract of Reinsurance SUM REINSURED USD 200,000,000 per occurrence (combined single limit or Damage and Business Interruption) LIMITS Contingent business interruption USD 125,000 DEDUCTIBLES Earthquake, Earth Movement or Volcanic Eruption 5% of loss amount, minimum USD 125,000 and maximum USD 425,000 combined Property Damage and Business Interruption To use segmentation rules most effectively, it is important that they are set up to identify concepts that often recur in the set of documents to be processed for a given project. With the exception of sporadic special cases, where the beginnings and the endings of segments can be identified with almost ad hoc rules, a good set of segmentation rules must be in some way predictive, so that they can also encompass variants of known forms and layouts.","title":"BEGIN and END"},{"location":"segments/syntax/#before-and-after","text":"Advanced segmentation syntax allows the developer to single out phraseology that precedes or follows the segment to be detected by using the keywords BEFORE or AFTER as follows: SCOPE scope_option { SEGMENT(segment_name|BEGIN_option) { //condition// } SEGMENT(segment_name|END_option) { //condition// } } where BEGIN_option and END_option correspond to one of the following conditions: BEGIN_BEFORE : the segment begins with the sentence before the sentence matched by the linguistic condition. - BEGIN_AFTER : the segment begins with the sentence after the sentence matched by the linguistic condition. END_BEFORE : the segment ends with the sentence before the sentence matched by the linguistic condition. - END_AFTER : the segment ends with the sentence after the sentence matched by the linguistic condition.","title":"BEFORE and AFTER"},{"location":"segments/syntax/#segmentation-rules-score","text":"When working with segments, it is possible to define several rules for each boundary, as the number of opening and closing conditions may vary according to the type of document. In some cases, some concepts identified by means of segmentation rules can represent stronger points of reference to define a segment boundaries than others. It is possible to highlight this difference in the rules and mark some concepts as more relevant while others as less relevant. This can be achieved by adding a score option to the rules using the following syntax: SCOPE scope_option { SEGMENT(segment_name|boundary_type_option:score_option) { //condition// } } The name of the segment must be followed by the boundary type defined by the rule as well as one of the score options. Score options can be of two types: Default score option Custom score option","title":"Segmentation rules score"},{"location":"segments/syntax/#default-score-option","text":"Segmentation and categorization rules share the same default score options listed in the table below: Option Description NORMAL The default/implicit score option LOW Lower than the default HIGH Higher than the default The options LOW and HIGH allow the user to assign a a slightly different score to a boundary compared to the default option and they can also be used to assign a higher or lower relevance of a boundary compared to another. The correct use of these options must consider: The use of the default score in most cases. The use of HIGH to give emphasis to a particular rule, for example one containing a concept or combination of concepts which is not ambiguous and will certainly result in a valid boundary (e.g. the main or most frequent beginning or end of a segment). The use of LOW to give less importance to a rule, for example one containing a slightly ambiguous concept which you are neither willing to exclude a priori nor willing to rely on in every case (for example special-case or unusual segments beginning or end).","title":"Default score option"},{"location":"segments/syntax/#custom-score-option","text":"Similar to categorization rules, it is possible to create custom score options. They can be defined in the config.cr file and they can be shared among both categorization and segmentation rules. The syntax is: SCORES { @score_option_name:points, ... } For example: SCORES { @LOWER:1, @HIGHER:20 } Once defined, the names of the new options can be used in the segmentation rules to allow for a greater variability of rules score.","title":"Custom score option"},{"location":"segments/syntax/#scope-options-in-segmentation-rules","text":"As for categorization and extraction rules, every segment rule needs a SCOPE option to be chosen in order to define two elements: The portion of text in which a single rule or a group of rules will act upon. The portion of text on which the segment will be extended. Any of the standard or custom scope options available can be used. However, there are some restrictions specific to segmentation rules that must be detailed. The SCOPE options: SENTENCE / PARAGRAPH / CLAUSE / PHRASE can always be used. The SCOPE options: SECTION / SEGMENT / CLAUSE (clause_type) / PHRASE (phrase_type) can be used except in those cases where the BEGIN or END statements are used to separately define the boundaries of a segment.","title":"Scope options in segmentation rules"},{"location":"segments/syntax/#phrase-and-clause","text":"PHRASE and CLAUSE scope options can be used in the cases specified above. Additionally, they must only be intended as portions of text where a segmentation rule has to be verified. In fact, since segments' extensions can't disregard sentence boundaries (for example segments can not be shorter than a sentence), CLAUSE and PHRASE scope options do not determine the portion of text on which the segment will be extended.","title":"Phrase and clause"},{"location":"segments/syntax/#sentence-and-paragraph","text":"The SCOPE options SENTENCE and PARAGRAPH can be used in any of the ways described in the cases specified above. However, when the following syntax is used: SCOPE PARAGRAPH|SENTENCE*n. { //segmentation rule or list of rules// } A distinction must be made between the programmed scope and the real scope of a rule, where \"programmed scope\" is the most extended portion of text on which a rule acts upon, and \"real scope\" is the portion of text that is really included in the segment. For example, if we define a rule scope in the following way SCOPE SENTENCE*3 { SEGMENT (segment_name) { //condition// } } we are declaring that the rule condition has to be verified within three consecutive sentences of the input document. Actually, three sentences are the maximum possible scope for the rule to be verified. The rule could also be verified in a single sentence or in two sentences, depending where the elements specified in the condition are found. Therefore, notwithstanding the maximum scope declared in a rule, the real scope is determined by the portion of text really containing the concepts that the rule looks for.","title":"Sentence and paragraph"},{"location":"segments/syntax/#section-and-segment","text":"The use of SECTION and SEGMENT scope options has a peculiar meaning when defining segmentation rules. In fact, when using these options for categorization or extraction rules, the user\u2019s aim is to look for concepts in a specific portion of text. When defining segmentation rules, on the other hand, the output of a rule acting within a section or another previously defined segment is a new segment created within the section or segment specified in the rule SCOPE . The possible aims to be achieved by means of this technique are two: Create nested segments. Upgrade a whole section or a whole segment to a new segment.","title":"Section and segment"},{"location":"segments/syntax/#nested-segments","text":"Using the scope option SEGMENT it is possible to define dynamic segments within other previously created segments. The syntax is the following: SCOPE scope_option { SEGMENT (segment_name1) { //condition// } } SCOPE SENTENCE IN SEGMENT (segment_name1) { SEGMENT (segment_name2) { //condition// } } The first rule (or set of rules) defines a segment using any scope options other than SEGMENT . The second rule uses the first segment as scope in order to define, within the first segment itself, another segment, nested in the first one.","title":"Nested segments"},{"location":"segments/syntax/#circular-references","text":"When defining nested segments it is fundamental to pay attention not to define circular references. Should it occur, the software will be unable to assign the correct order to the segmentations rules, thus making it impossible to execute them. Consider the following examples: SCOPE SENTENCE { SEGMENT (segment_name1) { //condition// } } SCOPE SENTENCE IN SEGMENT (segment1) { SEGMENT (segment2) { //condition// } } SCOPE SENTENCE IN SEGMENT (segment2) { SEGMENT (segment3) { //condition// } } SCOPE SENTENCE IN SEGMENT (segment3) { SEGMENT (segment_name1) { //condition// } } The rules above define: Segment1 first. Then segment2 is defined within segment1. Then segment3 is defined within segment2. At the end, segment1 is defined within segment3. The last rule invalidates the whole set because it introduces a circular reference in the code. This would generate an error and no rule would be compiled and applied.","title":"Circular References"},{"location":"segments/syntax/#sections-and-segments-promotion","text":"By using segmentation rules it is possible to promote a whole section or segment to a new segment which coincides with the original section or segment. In other words, it is possible to generate a segment identical in position and extension to another segment or section in order to create a sort of \u201cduplicate\u201d of an existing segment or section. This technique is useful when different operations must be performed within a single section or segment (linguistic rules, filters, post-processing\u2026) and the developer needs to differentiate a document portion where these actions need to be performed. This can be achieved only when the new segment includes the entire original section or segment, not just a part of it. For example, the following sample rule: SCOPE SECTION(HEADLINE) { SEGMENT(BOLD) { //condition// } } is correct and accepted because the entire HEADLINE section is going to be part of the new segment BOLD .","title":"Sections and segments promotion"},{"location":"segments/transforming-boundaries/","text":"Transforming boundaries into segments Once the dynamic segmentation rules have identified the possible segment boundaries, these must be processed in order to obtain the segments in their final form. This means that segments must be set up in pairs made of a left and a right boundary so that these two points can ideally be connected to outline the segment. However, sometimes the boundaries might not have a coherent arrangement or they might not be sufficient to create segments. In fact, segmentation rules can determine several potentially valid boundaries for each segment; but it may also occur that they are not able to define one of the two boundaries necessary for any segment to be created. There is a standard algorithm governing the boundaries elaboration phase, which works without any intervention from the developer. However, it is possible to use some optional instructions with each segment in order to orient the process towards a desired result. There are two groups of instructions: the first allows the user to set some additional segment definition options (see Segments Definition); the second manages priorities and resolves conflicts among pairs of segments (see Segments Priority). Segments definition Segments definition is an optional block of instructions to be used to define a set of constraints that affects the transformation of boundaries into dynamic segments. The syntax to use for the segments definition is the following: SEGMENTS DEFINITION { SEGMENT(segment_name) { //constraint// } } SEGMENTS DEFINITION is a syntax keyword and must be written in capitals as shown above. SEGMENT is the command that indicates which segment the instructions refer to. The segment\u2019s name will be specified in parentheses after the syntax keyword SEGMENT and must respect the spelling and case used in the Section/Segment definition window, where the segment itself was declared. In the above syntax, constraint refers to one of the options available for the segments definition instruction. They are: Boundary Closure Length Interval The constraints have to be defined for each segment separately. Each set of constraints has to be contained in a SEGMENTS DEFINITION block, and each block can contain the constraints for one or more segments. It is possible to define one or more blocks of SEGMENTS DEFINITION instructions anywhere in a project source files but the constraints for a single segment can appear only once in a project. Each block of constraints associated to a single segment can specify one, several or all of the available constraint types. Boundary When a segment is created by defining its opening and closing conditions through segmentation rules, it is common that a single segment ends up having multiple rules that usually identify several possible beginnings and ends. Most of the times, it is useful (if not necessary) to choose just one between two potential beginnings and ends. The segments definition option BOUNDARY defines which principle has to be followed when choosing a preferred beginning or end for a given segment. It is possible to set this behavior either based on the boundaries\u2019 position or based on their score. The syntax to set a segment BOUNDARY is the following: SEGMENTS DEFINITION { SEGMENT(segment_name) { BOUNDARY //side// = //option// } } Where side can be: END BEGIN not specified If a BOUNDARY side is specified, it will be possible to set the behavior of the beginning and end boundaries. The option may be set for both boundaries, or for just one of the two. If the BEGIN side is not specified, the option used will be valid for both boundaries. Option refers to one of the following: Option Description POSITION:FIRST Take the boundary which occurs first among all potential boundaries POSITION:LAST Take the boundary which occurs last among all potential boundaries SCORE:HIGHEST Take the boundary with the highest score SCORE:LOWEST Take the boundary with the lowest score SCORE:HIGHEST THEN POSITION:FIRST Take into account the boundary with the highest score, if more than one is eligible then it takes the boundary which occurs first SCORE:HIGHEST THEN POSITION:LAST Take into account the boundary with the highest score, if more than one is eligible then it takes the boundary which occurs last If the option BOUNDARY has not been set for a given segment, the default behavior is to validate the boundaries that generate the longest segment instance. Closure The fundamental aim of segmentation rules is to define dynamic segment boundaries. The BEGIN and END syntax allows the user to decide exactly where a segment has to begin and where it has to end. However, there could be cases in which lone boundaries might occur: they are called \"orphan\" boundaries. The segments definition option CLOSURE allows the user to define an automatic identification of the missing boundary and therefore an automatic closure of segment. The syntax to set a segment CLOSURE is the following: SEGMENTS DEFINITION { SEGMENT(segment_name) { CLOSURE //boundary_type// = //option// } } Where boundary_type can be: END BEGIN If a CLOSURE boundary type is specified, the behaviors of the BEGIN and END boundaries could be separately set. The automatic closure can be defined for both boundaries or for only one of the two. CLOSURE BEGIN will set the condition for the automatic identification of the segment closure. CLOSURE END will set the condition for the automatic identification of the segment beginning. Option refers to one of the following: Option Description SENTENCE If the orphan boundary type is BEGIN , the missing boundary will be identified in the sentence following the orphan boundary. If the orphan boundary type is END , the missing boundary will be identified in the sentence preceding the orphan boundary. SENTENCE*n If the orphan boundary type is BEGIN , the missing boundary will be identified in n sentences following the orphan boundary. If the orphan boundary type is END , the missing boundary will be identified in n sentences preceding the orphan boundary. PARAGRAPH If the orphan boundary type is BEGIN , the missing boundary will be identified in the paragraph following the orphan boundary. If the orphan boundary type is END , the missing boundary will be identified in the paragraph preceding the orphan boundary. PARAGRAPH*n If the orphan boundary type is BEGIN , the missing boundary will be identified in n paragraphs following the orphan boundary. If the orphan boundary type is END , the missing boundary will be identified in n paragraphs preceding the orphan boundary. SECTION If the orphan boundary type is BEGIN , the missing boundary will be identified with the end of the section containing the orphan boundary. If the orphan boundary type is END , the missing boundary will be identified with the beginning of the section containing the orphan boundary. SECTION*n If the orphan boundary type is BEGIN , the missing boundary will be identified with the n sections containing the orphan boundary. If the orphan boundary type is END , the missing boundary will be identified with the beginning of the n section containing the orphan boundary. Warning A segment can not span over two different sections. When an orphan boundary is found within a section and the input document contains more than one section, the closure options SENTENCE or PARAGRAPH might not behave as expected. In fact, if the defined closure falls out of the section containing the orphan boundary, then the closure option will not be respected and the segment will be closed where the section ends or begins. Length The segments definition option LENGTH defines the maximum extension a segment can span over. The unit of measure is the number of sentences or paragraphs between the two boundaries of a segment. This is useful to avoid triggering unwanted segments with boundaries which are too far apart from each other. The syntax to set a segment LENGTH is the following: SEGMENTS DEFINITION { SEGMENT(segment_name) { LENGTH = //option// } } Where option refers to either SENTENCE or PARAGRAPH . In both cases it is also possible to specify the number of sentences or paragraphs that the segment must extend over. The syntax is the following: SEGMENTS DEFINITION { SEGMENT(segment_name) { LENGTH //boundary_type// = SENTENCE|PARAGRAPH*n } } If the LENGTH option is not specified in the SEGMENT DEFINITION , any segment of any length will be considered to be valid. Otherwise, segments exceeding the length constraints will be removed. Interval Segments can be instantiated once or multiple times in each document, depending on the document, on the rules and on the project needs. The segments definition option INTERVAL defines this behavior. The syntax to set a segment INTERVAL is the following: SEGMENTS DEFINITION { SEGMENT(segment_name) { INTERVAL = MULTIPLE|SINGLE } } If the INTERVAL is set to MULTIPLE , all instances of the segment found within a document will be returned. If the INTERVAL is set to SINGLE , on the other hand, only one instance of a given segment will be chosen among those available. If SINGLE is chosen, it will also be necessary to declare which instance has to be chosen if several of them have are present. The syntax is the following: SEGMENTS DEFINITION { SEGMENT(segment_name) { INTERVAL = SINGLE://option// } } Where option refers to one of those available: FIRST : the first segment instance in the text. LAST : the last segment instance in the text. LONGEST : the longest segment instance in the text, that is to say the segment containing the highest number of sentences or characters when two or more segments share the same number of sentences. SHORTEST : the shortest segment instance in the text, that is to say the segment containing the lowest number of sentences or characters when two or more segments share the same number of sentences. If a segment interval is not defined in the segments definition, the segment will admit multiple instances by default. Segments priority Segments priority is an optional block of instructions that allows users to set a scale of priority among two segments and define, manage and solve conflicts among different overlapping segments, so that the segments themselves can be reshaped to avoid any overlap. The syntax of SEGMENT PRIORITY is the following: SEGMENTS PRIORITY { segment_name1 //ALWAYS// segment_name2 } SEGMENT PRIORITY is a syntax keyword and must be written in capitals as shown above. segment_name1 and segment_name2 refer to the segment names the instruction applies to. The names must be spelled in the same exact way as they are when the segments are declared. ALWAYS is a syntax keyword and must be written in capitals as shown above. The operator ALWAYS is used to solve conflicts by identifying the first segment in the expression as always dominant over the second, when the two overlap. In fact, if we set the SEGMENT PRIORITY as follows: SEGMENTS PRIORITY { A ALWAYS B } the segment A will maintain its original extension whereas the segment B will be cropped: its beginning has been pushed forward compared to its original extension so that it now starts where the segment A ends, thus removing the original overlap. If the priority scheme is inverted: SEGMENTS PRIORITY { B ALWAYS A } the end of the segment A will be pushed backwards to avoid any overlap with the segment B, which will maintain its original beginning. Segment boundaries elaboration process By definition, all types of segments lie on separate planes and do not influence each other. Therefore, segments are validated separately. In the end, if any segment priority instruction has been defined, the segments affected by these instructions will be reshaped based on the specified criteria. The process that leads to segments creation and validation is composed of four phases. Phase A: boundaries and closures validation Segments are validated by taking into account BOUNDARY and CLOSURE criteria, thus verifying the following conditions: If the segments definition does not contain instructions for BOUNDARY and CLOSURE , a default BOUNDARY instruction will be applied, that is to say BOUNDARY BEGIN = POSITION FIRST and BOUNDARY END = POSITION LAST . Any orphan boundaries will not be taken into account as no CLOSURE instruction will be applied. If the segments definition does not contain BOUNDARY instructions, but contains CLOSURE instructions, a default BOUNDARY instruction will be applied, that is to say BOUNDARY BEGIN = POSITION FIRST and BOUNDARY END = POSITION LAST . All orphan boundaries will be closed according to the CLOSURE instructions specified: the segments resulting from this process can neither overlap each other, nor can they overlap the segments already defined by the default BOUNDARY instructions. If the segments definition contains BOUNDARY instructions, but does not contain CLOSURE instructions, all BOUNDARY instructions will be applied and orphan boundaries will not be taken into account as no CLOSURE instruction is present. If the segments definition contains both BOUNDARY and CLOSURE instructions, first all BOUNDARY instructions will be applied, then the specified CLOSURE will be applied to the remaining orphan boundaries. Phase B: length filter LENGTH filters (if any) are applied and all the segments that do not satisfy them are discarded. Phase C: interval control Segments for which INTERVAL criteria have been specified are reworked if it is necessary to generate a single segment instance. Phase D: priority reshaping Segments belonging to a group with PRIORITY criteria are reshaped.","title":"Transforming boundaries into segments"},{"location":"segments/transforming-boundaries/#transforming-boundaries-into-segments","text":"Once the dynamic segmentation rules have identified the possible segment boundaries, these must be processed in order to obtain the segments in their final form. This means that segments must be set up in pairs made of a left and a right boundary so that these two points can ideally be connected to outline the segment. However, sometimes the boundaries might not have a coherent arrangement or they might not be sufficient to create segments. In fact, segmentation rules can determine several potentially valid boundaries for each segment; but it may also occur that they are not able to define one of the two boundaries necessary for any segment to be created. There is a standard algorithm governing the boundaries elaboration phase, which works without any intervention from the developer. However, it is possible to use some optional instructions with each segment in order to orient the process towards a desired result. There are two groups of instructions: the first allows the user to set some additional segment definition options (see Segments Definition); the second manages priorities and resolves conflicts among pairs of segments (see Segments Priority).","title":"Transforming boundaries into segments"},{"location":"segments/transforming-boundaries/#segments-definition","text":"Segments definition is an optional block of instructions to be used to define a set of constraints that affects the transformation of boundaries into dynamic segments. The syntax to use for the segments definition is the following: SEGMENTS DEFINITION { SEGMENT(segment_name) { //constraint// } } SEGMENTS DEFINITION is a syntax keyword and must be written in capitals as shown above. SEGMENT is the command that indicates which segment the instructions refer to. The segment\u2019s name will be specified in parentheses after the syntax keyword SEGMENT and must respect the spelling and case used in the Section/Segment definition window, where the segment itself was declared. In the above syntax, constraint refers to one of the options available for the segments definition instruction. They are: Boundary Closure Length Interval The constraints have to be defined for each segment separately. Each set of constraints has to be contained in a SEGMENTS DEFINITION block, and each block can contain the constraints for one or more segments. It is possible to define one or more blocks of SEGMENTS DEFINITION instructions anywhere in a project source files but the constraints for a single segment can appear only once in a project. Each block of constraints associated to a single segment can specify one, several or all of the available constraint types.","title":"Segments definition"},{"location":"segments/transforming-boundaries/#boundary","text":"When a segment is created by defining its opening and closing conditions through segmentation rules, it is common that a single segment ends up having multiple rules that usually identify several possible beginnings and ends. Most of the times, it is useful (if not necessary) to choose just one between two potential beginnings and ends. The segments definition option BOUNDARY defines which principle has to be followed when choosing a preferred beginning or end for a given segment. It is possible to set this behavior either based on the boundaries\u2019 position or based on their score. The syntax to set a segment BOUNDARY is the following: SEGMENTS DEFINITION { SEGMENT(segment_name) { BOUNDARY //side// = //option// } } Where side can be: END BEGIN not specified If a BOUNDARY side is specified, it will be possible to set the behavior of the beginning and end boundaries. The option may be set for both boundaries, or for just one of the two. If the BEGIN side is not specified, the option used will be valid for both boundaries. Option refers to one of the following: Option Description POSITION:FIRST Take the boundary which occurs first among all potential boundaries POSITION:LAST Take the boundary which occurs last among all potential boundaries SCORE:HIGHEST Take the boundary with the highest score SCORE:LOWEST Take the boundary with the lowest score SCORE:HIGHEST THEN POSITION:FIRST Take into account the boundary with the highest score, if more than one is eligible then it takes the boundary which occurs first SCORE:HIGHEST THEN POSITION:LAST Take into account the boundary with the highest score, if more than one is eligible then it takes the boundary which occurs last If the option BOUNDARY has not been set for a given segment, the default behavior is to validate the boundaries that generate the longest segment instance.","title":"Boundary"},{"location":"segments/transforming-boundaries/#closure","text":"The fundamental aim of segmentation rules is to define dynamic segment boundaries. The BEGIN and END syntax allows the user to decide exactly where a segment has to begin and where it has to end. However, there could be cases in which lone boundaries might occur: they are called \"orphan\" boundaries. The segments definition option CLOSURE allows the user to define an automatic identification of the missing boundary and therefore an automatic closure of segment. The syntax to set a segment CLOSURE is the following: SEGMENTS DEFINITION { SEGMENT(segment_name) { CLOSURE //boundary_type// = //option// } } Where boundary_type can be: END BEGIN If a CLOSURE boundary type is specified, the behaviors of the BEGIN and END boundaries could be separately set. The automatic closure can be defined for both boundaries or for only one of the two. CLOSURE BEGIN will set the condition for the automatic identification of the segment closure. CLOSURE END will set the condition for the automatic identification of the segment beginning. Option refers to one of the following: Option Description SENTENCE If the orphan boundary type is BEGIN , the missing boundary will be identified in the sentence following the orphan boundary. If the orphan boundary type is END , the missing boundary will be identified in the sentence preceding the orphan boundary. SENTENCE*n If the orphan boundary type is BEGIN , the missing boundary will be identified in n sentences following the orphan boundary. If the orphan boundary type is END , the missing boundary will be identified in n sentences preceding the orphan boundary. PARAGRAPH If the orphan boundary type is BEGIN , the missing boundary will be identified in the paragraph following the orphan boundary. If the orphan boundary type is END , the missing boundary will be identified in the paragraph preceding the orphan boundary. PARAGRAPH*n If the orphan boundary type is BEGIN , the missing boundary will be identified in n paragraphs following the orphan boundary. If the orphan boundary type is END , the missing boundary will be identified in n paragraphs preceding the orphan boundary. SECTION If the orphan boundary type is BEGIN , the missing boundary will be identified with the end of the section containing the orphan boundary. If the orphan boundary type is END , the missing boundary will be identified with the beginning of the section containing the orphan boundary. SECTION*n If the orphan boundary type is BEGIN , the missing boundary will be identified with the n sections containing the orphan boundary. If the orphan boundary type is END , the missing boundary will be identified with the beginning of the n section containing the orphan boundary. Warning A segment can not span over two different sections. When an orphan boundary is found within a section and the input document contains more than one section, the closure options SENTENCE or PARAGRAPH might not behave as expected. In fact, if the defined closure falls out of the section containing the orphan boundary, then the closure option will not be respected and the segment will be closed where the section ends or begins.","title":"Closure"},{"location":"segments/transforming-boundaries/#length","text":"The segments definition option LENGTH defines the maximum extension a segment can span over. The unit of measure is the number of sentences or paragraphs between the two boundaries of a segment. This is useful to avoid triggering unwanted segments with boundaries which are too far apart from each other. The syntax to set a segment LENGTH is the following: SEGMENTS DEFINITION { SEGMENT(segment_name) { LENGTH = //option// } } Where option refers to either SENTENCE or PARAGRAPH . In both cases it is also possible to specify the number of sentences or paragraphs that the segment must extend over. The syntax is the following: SEGMENTS DEFINITION { SEGMENT(segment_name) { LENGTH //boundary_type// = SENTENCE|PARAGRAPH*n } } If the LENGTH option is not specified in the SEGMENT DEFINITION , any segment of any length will be considered to be valid. Otherwise, segments exceeding the length constraints will be removed.","title":"Length"},{"location":"segments/transforming-boundaries/#interval","text":"Segments can be instantiated once or multiple times in each document, depending on the document, on the rules and on the project needs. The segments definition option INTERVAL defines this behavior. The syntax to set a segment INTERVAL is the following: SEGMENTS DEFINITION { SEGMENT(segment_name) { INTERVAL = MULTIPLE|SINGLE } } If the INTERVAL is set to MULTIPLE , all instances of the segment found within a document will be returned. If the INTERVAL is set to SINGLE , on the other hand, only one instance of a given segment will be chosen among those available. If SINGLE is chosen, it will also be necessary to declare which instance has to be chosen if several of them have are present. The syntax is the following: SEGMENTS DEFINITION { SEGMENT(segment_name) { INTERVAL = SINGLE://option// } } Where option refers to one of those available: FIRST : the first segment instance in the text. LAST : the last segment instance in the text. LONGEST : the longest segment instance in the text, that is to say the segment containing the highest number of sentences or characters when two or more segments share the same number of sentences. SHORTEST : the shortest segment instance in the text, that is to say the segment containing the lowest number of sentences or characters when two or more segments share the same number of sentences. If a segment interval is not defined in the segments definition, the segment will admit multiple instances by default.","title":"Interval"},{"location":"segments/transforming-boundaries/#segments-priority","text":"Segments priority is an optional block of instructions that allows users to set a scale of priority among two segments and define, manage and solve conflicts among different overlapping segments, so that the segments themselves can be reshaped to avoid any overlap. The syntax of SEGMENT PRIORITY is the following: SEGMENTS PRIORITY { segment_name1 //ALWAYS// segment_name2 } SEGMENT PRIORITY is a syntax keyword and must be written in capitals as shown above. segment_name1 and segment_name2 refer to the segment names the instruction applies to. The names must be spelled in the same exact way as they are when the segments are declared. ALWAYS is a syntax keyword and must be written in capitals as shown above. The operator ALWAYS is used to solve conflicts by identifying the first segment in the expression as always dominant over the second, when the two overlap. In fact, if we set the SEGMENT PRIORITY as follows: SEGMENTS PRIORITY { A ALWAYS B } the segment A will maintain its original extension whereas the segment B will be cropped: its beginning has been pushed forward compared to its original extension so that it now starts where the segment A ends, thus removing the original overlap. If the priority scheme is inverted: SEGMENTS PRIORITY { B ALWAYS A } the end of the segment A will be pushed backwards to avoid any overlap with the segment B, which will maintain its original beginning.","title":"Segments priority"},{"location":"segments/transforming-boundaries/#segment-boundaries-elaboration-process","text":"By definition, all types of segments lie on separate planes and do not influence each other. Therefore, segments are validated separately. In the end, if any segment priority instruction has been defined, the segments affected by these instructions will be reshaped based on the specified criteria. The process that leads to segments creation and validation is composed of four phases.","title":"Segment boundaries elaboration process"},{"location":"segments/transforming-boundaries/#phase-a-boundaries-and-closures-validation","text":"Segments are validated by taking into account BOUNDARY and CLOSURE criteria, thus verifying the following conditions: If the segments definition does not contain instructions for BOUNDARY and CLOSURE , a default BOUNDARY instruction will be applied, that is to say BOUNDARY BEGIN = POSITION FIRST and BOUNDARY END = POSITION LAST . Any orphan boundaries will not be taken into account as no CLOSURE instruction will be applied. If the segments definition does not contain BOUNDARY instructions, but contains CLOSURE instructions, a default BOUNDARY instruction will be applied, that is to say BOUNDARY BEGIN = POSITION FIRST and BOUNDARY END = POSITION LAST . All orphan boundaries will be closed according to the CLOSURE instructions specified: the segments resulting from this process can neither overlap each other, nor can they overlap the segments already defined by the default BOUNDARY instructions. If the segments definition contains BOUNDARY instructions, but does not contain CLOSURE instructions, all BOUNDARY instructions will be applied and orphan boundaries will not be taken into account as no CLOSURE instruction is present. If the segments definition contains both BOUNDARY and CLOSURE instructions, first all BOUNDARY instructions will be applied, then the specified CLOSURE will be applied to the remaining orphan boundaries.","title":"Phase A: boundaries and closures validation"},{"location":"segments/transforming-boundaries/#phase-b-length-filter","text":"LENGTH filters (if any) are applied and all the segments that do not satisfy them are discarded.","title":"Phase B: length filter"},{"location":"segments/transforming-boundaries/#phase-c-interval-control","text":"Segments for which INTERVAL criteria have been specified are reworked if it is necessary to generate a single segment instance.","title":"Phase C: interval control"},{"location":"segments/transforming-boundaries/#phase-d-priority-reshaping","text":"Segments belonging to a group with PRIORITY criteria are reshaped.","title":"Phase D: priority reshaping"},{"location":"tagging/","text":"Tagging Introduction Tagging means manipulating the disambiguation output to enrich the tokens with additional information (\"tags\") that can be referenced in segmentation, categorization and extraction rules. The process takes place after the disambiguation and before the evaluation of categorization and extraction rules. Two non alternative tagging techniques can be adopted: Tagging by rules . Tagging by script . The final result is a combination of the two, because scripts are evaluated after rules and can change the results of rules activation. The tagging rules are the first to be evaluated among all the other rules so that the segmentation, categorization and extraction rules can benefit from the tagging activity. Tagging by rules Tagging by rules is a two-step process. In the first step, possible tags are declared in the config.cr file. The syntax is: TAGS { @tag1_label, @tag2_label:tag2_syncon_id } Tags can be declared in two different ways, based on their purpose: by specifying the label ( @tag1_label ) by specifying the label and the syncon ID ( @tag2_label:tag2_syncon_id ) The tag label, once applied to a token, becomes an additional attribute. If multiple tags are applied to the same token, the respective labels will accumulate. The syncon ID, on the other hand, replaces the original syncon ID of the token, i.e. a token cannot have more than one syncon ID. Therefore, applying a tag with a syncon ID to a token is like changing its meaning. The second step is to define tagging rules. Similar to an extraction rule, a tagging rule is triggered when its condition is met, therefore the portion of text that matches the attribute in square brackets is tagged with the tag label and the syncon ID, if any. The syntax is: SCOPE scope_option { TAGGER(_tagging_level) { @tag_label[attribute] } } The tagging level can either be empty (meaning a level value of 10000) or contain an integer (1, 2, 3... 42, etc.). The higher the value is, the later the TAGGER rule will be applied during the tagging phase. For instance, if we have a rule with a TAGGER() and a rule with TAGGER(1) , the latter will be executed before the rule with a null value. This means that TAG s defined by mean of TAGGER(1) rules can furtherly be used or manipulated by the TAGGER() rules. Using tags Tags can be used either by the TAG attribute or by referencing their syncon ID, if any. The next example illustrates both possibilities. Suppose the goals are to: (Extraction) tag all drug codes present in a text in order to easily extract them. (Categorization) improve the results by manipulating the assigned ID for the lemma drug (e.g. for some projects, polysemy may be not an issue to solve). The first step is declaring the tags: TAGS { @CODE, @MEDICINE:100012140//@SYN: #100012140# [drug] } @CODE will be used as a pure label while @MEDICINE will simultaneously attach the tag label and replace the token syncon ID with 100012140. The second step is defining the tagging rules: SCOPE SENTENCE { TAGGER() { @CODE[PATTERN(\"\\d{8}\")] // Every eight digit number will be tagged as CODE. } TAGGER() { @MEDICINE[LEMMA(\"drug\")] // every occurrence of lemma drug (regardless of its meaning) in a text will be tagged as MEDICINE and will be assigned syncon ID 100012140 } } If the above rules are run against this text: Ibuprofen Hs Code is 29420012. The use of this drug is not recommended in patients with advanced renal disease. the numerical string in the semantic analysis will be tagged with the label CODE . The lemma drug meaning changes from syncon 100008386 ( drug in the sense of narcotic ) to syncon 100012140 ( medicine ). At this point, the categorization and extraction rules can now leverage the information derived from the tagging. The following categorization rule will generate a hit in the sample text thanks to the tagger manipulation of the meaning of drug : SCOPE SENTENCE { DOMAIN(dom1) { SYNCON(100012140)//@SYN: #100012140# [drug] } } The following extraction rule will generate a hit by simply stating the desired TAG : SCOPE SENTENCE { IDENTIFY(TEST) { @DRUG_CODE[TAG(CODE)] } } Tagging by script Tagging by script adds or deletes tags using the onTagger function . For example, the following code will tag all disambiguation tokens with a syncon ID equal to 123. function onTagger () { var count = DIS . getTokensCount (); for ( var i = 0 ; i < count ; i ++ ) { var token = DIS . getToken ( i ); if ( token . synId == 123 ) DIS . tagToken ( i , \"mylabel\" ); } }","title":"Tagging"},{"location":"tagging/#tagging","text":"","title":"Tagging"},{"location":"tagging/#introduction","text":"Tagging means manipulating the disambiguation output to enrich the tokens with additional information (\"tags\") that can be referenced in segmentation, categorization and extraction rules. The process takes place after the disambiguation and before the evaluation of categorization and extraction rules. Two non alternative tagging techniques can be adopted: Tagging by rules . Tagging by script . The final result is a combination of the two, because scripts are evaluated after rules and can change the results of rules activation. The tagging rules are the first to be evaluated among all the other rules so that the segmentation, categorization and extraction rules can benefit from the tagging activity.","title":"Introduction"},{"location":"tagging/#tagging-by-rules","text":"Tagging by rules is a two-step process. In the first step, possible tags are declared in the config.cr file. The syntax is: TAGS { @tag1_label, @tag2_label:tag2_syncon_id } Tags can be declared in two different ways, based on their purpose: by specifying the label ( @tag1_label ) by specifying the label and the syncon ID ( @tag2_label:tag2_syncon_id ) The tag label, once applied to a token, becomes an additional attribute. If multiple tags are applied to the same token, the respective labels will accumulate. The syncon ID, on the other hand, replaces the original syncon ID of the token, i.e. a token cannot have more than one syncon ID. Therefore, applying a tag with a syncon ID to a token is like changing its meaning. The second step is to define tagging rules. Similar to an extraction rule, a tagging rule is triggered when its condition is met, therefore the portion of text that matches the attribute in square brackets is tagged with the tag label and the syncon ID, if any. The syntax is: SCOPE scope_option { TAGGER(_tagging_level) { @tag_label[attribute] } } The tagging level can either be empty (meaning a level value of 10000) or contain an integer (1, 2, 3... 42, etc.). The higher the value is, the later the TAGGER rule will be applied during the tagging phase. For instance, if we have a rule with a TAGGER() and a rule with TAGGER(1) , the latter will be executed before the rule with a null value. This means that TAG s defined by mean of TAGGER(1) rules can furtherly be used or manipulated by the TAGGER() rules.","title":"Tagging by rules"},{"location":"tagging/#using-tags","text":"Tags can be used either by the TAG attribute or by referencing their syncon ID, if any. The next example illustrates both possibilities. Suppose the goals are to: (Extraction) tag all drug codes present in a text in order to easily extract them. (Categorization) improve the results by manipulating the assigned ID for the lemma drug (e.g. for some projects, polysemy may be not an issue to solve). The first step is declaring the tags: TAGS { @CODE, @MEDICINE:100012140//@SYN: #100012140# [drug] } @CODE will be used as a pure label while @MEDICINE will simultaneously attach the tag label and replace the token syncon ID with 100012140. The second step is defining the tagging rules: SCOPE SENTENCE { TAGGER() { @CODE[PATTERN(\"\\d{8}\")] // Every eight digit number will be tagged as CODE. } TAGGER() { @MEDICINE[LEMMA(\"drug\")] // every occurrence of lemma drug (regardless of its meaning) in a text will be tagged as MEDICINE and will be assigned syncon ID 100012140 } } If the above rules are run against this text: Ibuprofen Hs Code is 29420012. The use of this drug is not recommended in patients with advanced renal disease. the numerical string in the semantic analysis will be tagged with the label CODE . The lemma drug meaning changes from syncon 100008386 ( drug in the sense of narcotic ) to syncon 100012140 ( medicine ). At this point, the categorization and extraction rules can now leverage the information derived from the tagging. The following categorization rule will generate a hit in the sample text thanks to the tagger manipulation of the meaning of drug : SCOPE SENTENCE { DOMAIN(dom1) { SYNCON(100012140)//@SYN: #100012140# [drug] } } The following extraction rule will generate a hit by simply stating the desired TAG : SCOPE SENTENCE { IDENTIFY(TEST) { @DRUG_CODE[TAG(CODE)] } }","title":"Using tags"},{"location":"tagging/#tagging-by-script","text":"Tagging by script adds or deletes tags using the onTagger function . For example, the following code will tag all disambiguation tokens with a syncon ID equal to 123. function onTagger () { var count = DIS . getTokensCount (); for ( var i = 0 ; i < count ; i ++ ) { var token = DIS . getToken ( i ); if ( token . synId == 123 ) DIS . tagToken ( i , \"mylabel\" ); } }","title":"Tagging by script"}]}